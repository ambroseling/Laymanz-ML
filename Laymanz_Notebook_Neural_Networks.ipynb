{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laymanz Notebooks: Training your first deep learning model\n","Author: Ali Ahmad & Amrbose Ling\n","\n","**Our goal is to get rid of abstractions and black boxes when learning about ML**\n","\n","**What is this notebook about?**\n","\n","In this notebook, we will go over some of the fundamental ideas behind deep learning, neural networks, model training, inference, backpropagation, loss functions, optimizers.\n","\n","**What do I need to set up my environment?**\n","\n","All of our notebooks will only use numpy, pytorch, matplotlib for visualizations. If you are very eager to learn about what PyTorch is and how it works, check out this super detailed notebook on PyTorch! If you are running this on colab you can just import the packages, if you are running this notebook locally , just remember to `pip install numpy torch matplotlib`. Check [here](https://pytorch.org/get-started/locally/) to see which torch version depending on the hardware you have.\n","\n","**How is this notebook structured?**\n","\n","Each notebook will have\n","\n","[**How to use matplotlib for plotting**](https://colab.research.google.com/github/amanchadha/aman-ai/blob/master/matplotlib.ipynb#scrollTo=1-AcMM6NSmP-)\n","\n","\n","## Breakdown\n","*   What is deep learning?\n","*   What is a neuron? What is an activation?\n","*   What is a neural network?\n","*   What is a multi-layer perceptron?\n","*   What is a computation graph?\n","*   What is autograd?\n","*   What is a forward pass?\n","*   What is a loss function?\n","*   What is an optimizer?\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader,Dataset\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["# What is deep learning?\n","\n","You can find many definitions online, but here is my definition:\n","Deep learning involves extracting meaningful insights from data using deep neural networks.\n","Deep learning is a branch of Machine Learning that specializes in the use of neural networks to make predictions.\n","\n","\n","# What can deep learning be used for?\n","* Classification\n","    - Binary classification\n","    - Multiclass classification\n","    - Multilabel classficiation\n","* Regression\n","    - Linear Regression\n","    - Logistic Regression\n","    - Polynomial Regression\n","* Generative\n","    - Next-token prediction (GPT)\n","    - Image / Video Generation\n","\n","**Ambrose yapping**\n","\n","\n","Deep learning is about feeding a neural network a bunch of data, the neural network produces some \n","output, we see how wrong they were compared to what we want, we tell it \"hey you were wrong you gotta change how you think so you get it right next time!\" then we change it, we repeat this until we iterate through all of our data."]},{"cell_type":"markdown","metadata":{},"source":["# What is a neuron?\n","A neuron are nerve cells in our brains, responsible for transmitting electrical signals from one part of the brain to another. \n","\n","## Properties of neurons:\n","- They receive their signals via their dendrites\n","- They have snynapses that module the electrical signals it receives (between dendrites and axons)\n","- They fire an output signal only when the total strength of the input signal exceed a certain threshold"]},{"cell_type":"markdown","metadata":{},"source":["# What is a perceptron?\n","\n","A perceptron is a mathematical model of a biological neuron. Hence we use mathematical operations to model the properties of a neuron.\n","\n","## Properties of a perecptron?\n","- electrical signals are represented by numerical values (some vector)\n","- modulation is modelled by multiplying some weight value to the input signal / values\n","- we model the total strength of a signal by performing a weighted sum of the inputs \n","- we apply an activation function or a step function to model the firing of the signal upon some threshold\n","- it is believed that neurons remain inactive until the net input to the cell body reaches a certain threshold"]},{"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://miro.medium.com/v2/resize:fit:2902/format:webp/1*hkYlTODpjJgo32DoCOWN5w.png\" height=230 width=500/></center>\n","<center> On the left you have the biological neuron, on the right you have the artificial neuron </center>"]},{"cell_type":"markdown","metadata":{},"source":["### How do we represent a perceptron mathematically?\n","\n","$$\n","y = \\sigma ( \\Sigma w_i x_i + b )\n","$$\n","\n","where : \n","* y: represents the output from the neuron\n","* w represents the weight associated with this neuron\n","* x: represents the input to the neuron\n","* b: represets the bias added to each \n","\n","Ambrose's intuition:\n","Think of each neuron having its own behaviour or its own state, it behaves differently from other neurosn which is why each\n","has a different weight and bias associated with it.\n"]},{"cell_type":"markdown","metadata":{},"source":["### How do we represent all these quantities mathematically?\n","Think of different ways you may want to represent inputs, and I can list some examples for some of the most common machine learning applications, all these are different modalities. Since we mentioned that the key to \n","  \n","\n","Lets try to develop an intuition for how you would quantitatively represent data?\n","\n","1. Usecase: Predicting house prices \n","Scenario: Lets say i want to predict the price of houses in Toronto given some information of a house(expensive af)\n","What does data look like:  For this scenario, I want to probably find some way to represent that information of a house **quantitatively**\n","What does input look like: Some collection of numbers that represent some characteristics of the home\n","What does output look like: A number (float/double) representating the price of the home\n","A house can be represented by:\n","    - int: how many bedrooms are in this house\n","    - int: how many square feet is this house\n","    - int: how many bathrooms it has\n","    - String: where is this house (the location)\n","    - String: the population of the city it is in\n","\n","2.  Usecase: Predicting postiive and negative sentiment from text\n","Scenario: Lets say i want to predict if a tweet contains harmful or positive intent right, you know those goddamn politicans\n","What does data look like? In this scenario, the data would probably be the tweets themselves, which is a series of strings.\n","\n","\n","3. Usecase: Predicting the price of Bitcoin \n","Scenario: Lets say i want to predict if the price of bitcoin. \n","What does input look like: A collection of numbers representing past bitcon pries\n","The price of bitcoin can be represented by:\n","    - list of ints: \n","\n","\n","4. Usecase: "]},{"cell_type":"markdown","metadata":{},"source":["### Model Width VS Model Depth\n","\n","**Larger width** (vertically, more neurons): usually means that the neural network has the capacity to remember more feature or encode more features in the weight connecitons or weight matrix. \n","* With more neurons in each layer, you can capture more delicate details with more neurons. \n","* Think of the more neurons you have in each layer, you have more processing units and internal states corresponding to each input unit. \n","* Think of if you have 1 x 10 input, 15 neurons VS 1 x 10 input, 100 neurons, you have much more overall weight connections to the input, so you can capture each intricate value in the input more precisely with more weights.\n","* Think of F1 cars right when they get to the pit stop, if you have only 5 people changing the tires, wiping the windows, pumping gas VS when you have 20 people , you would be be able to be more precise about what you do to the car, 5 people would probably capture less of the tasks they need to do on the car. People (neurons), the car (input).\n","\n","**Larger depth** (more layers), usually means that the neural network can remember or encode more complex,high dimensional features from the training data.\n","\n","* The way i think could help also understand is that think of width as having more functions to model your data, but more functions do not necessarily mean that the functions are more complex. \n","* Increasing the width is similar to going from 1 function: y = mx+b to 20  functions, 20 y = wx+b's. You may be able to capture specific changes in the input data better with more lines. But they are still linear functions and it would always be linear\n","* But with greater depth, you can chain non-linearities together as you apply activation functions and additional weights. So now think of 20 y = (wa(wa(wx+b)+b)+b), this entire function gets more and more complex and you introduce more non-linearity at each layer.\n","* So going from 20 y=wx+b's to 20 y = (wa(wa(wx+b)+b)+b) is what enables you to capture a much more complicated function that fits your training data."]},{"cell_type":"markdown","metadata":{},"source":["### Rank\n","Rank refers to the number of dimensions a tensor has\n","\n","- Rank 0:  referred as a **scalar**, 1 single value\n","- Rank 1:  referred as a **vector**, 1 list of values\n","- Rank 2:  referred as a **matrix**, 1 2d array of values\n","- Rank 3:  referred as a **3D tensor**, you can think of as a cube of numbers or a stack of a number of 2D matrices\n","\n","\n","### Shape \n","Shape is an array of numbers representing the length of each dimension. \n","You can understand this as the number of elements along each dimension\n","\n","```python\n","x = np.array([[[1],[2]],[[3],[4]]])\n","```\n","\n","**Ambrose yapping**\n","\n","The way I like thinking about shapes is in terms of boxes.\n","Lets say the shape of tensor `x` is `(2,4,5)`. This means that there are 2 big boxes. Within those 2 big boxes, each of the 2 big boxes has 4 boxes in it\n","Then for each of the 4 boxes you then have 5 boxes in each one.\n","\n","One very typical example tensor is with shape `(B,C,H,W)`, for instance `(8,3,100,100)`\n","You have 8 boxes, or that means 8 samples\n","Within each box or sample, you have 3 boxes: 1 box for each channel, then within each channel, you have essentially a grid of dimensions `(H x W)`.\n","\n","\n","**NOTE:**\n","When building your neural network in PyTorch (or any deep learning framework), it is very very useful to check the shapes of your tensors at different operations you execute. One of the most common errors is `Shape Mismatch Error` where you are trying to applying illegal operations on tensors because their shapes don't \n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Hn3xTzct-b84"},"outputs":[],"source":["\n","# 1D: Array\n","x = np.array([0.12,0.23,2.34])\n","\n","# 2D: Matrix\n","x = np.array([[0.1232,0.3445,0.345532],[0.1232,0.3445,0.345532]])\n","\n","# Why do we use matrices?\n","# There are many highly efficient linear algebra libraries (NumPY, PyTorch) that are optimized to perfrom matrix multiplications extremely fast\n","# Using matrices allows us to perform operations in PARALLEL much faster to doing sequential operations\n","\n","\n","# Matrix\n","x = np.array([[0,1,2,3,4],[2,3,5,67,5]])\n","\n","# Tensor\n","x = np.array([[0,1,2,3,4],[2,3,5,67,5],[10,20,40,20,2]])\n","\n","# A tensor is nothing but a bigger matrix, it is an array that carries numerical data or think of a tensor as a multi-dimensional array. \n","# Usually matrices are  m x n, tensors can be m x n x c x p ...\n"]},{"cell_type":"markdown","metadata":{},"source":["### Why does neural network architecture matter?\n","When we talk about the neural network architecture, we are referring to the specific arrangement of neurons, the connections of neurons,layers. The specific architecture dictates the capabilities of the model, the specific data it is best suited for, the features it would excel at capturing, the specific tasks it is suited for."]},{"cell_type":"markdown","metadata":{},"source":["**Ambrose Yapping**\n","\n","**So how do you choose the right neural network architecture?**\n","\n","From what I know, it is almost the best to follow what somebody has done before. Reinventing the wheel is almost never a good idea unless your approach is to purely experiment with the architecture. If you are looking to accomplishing a task in ML, ask yourself if you even really need ML to do it. And \n","\n","From my experience, a lot of things seem to work theoretically, but almost never practically. So choose your experiments wisely. \n","\n","For example, `YoLo` architecutre has been experimentally shown to perform well in object detection. So if you want to train a ML model to detect pedestrains or cars, chances are using that architecture is a good idea VS putting together your own.\n","\n","Another example (or this is based on my observation), in the diffusion model literature. A lot of works are based on Stable Diffusion, a method that uses a convolution-based neural network to generate images. It is actually very rare for people to change the architecture drastically. A lot of later works that aimed to improve the generation quality of diffusion models did not change much of the architecutre. It is almost always a safe bet to build on top of other's successful models or experiments."]},{"cell_type":"markdown","metadata":{},"source":["### What is PyTorch?\n","\n","PyTorch is a neural network library that lets you build and trian neural networks with their very comprehensive collection of APIs.\n","\n","1. **Tensor Operations**: Basic operations for creating, manipulating, and transforming tensors.\n","2. **Mathematical Operations**: Functions for performing arithmetic, linear algebra, and complex mathematical computations.\n","3. **Neural Network Operations**: Layers, activation functions, loss functions, and other building blocks for constructing neural networks.\n","4. **Data Manipulation**: Functions for data loading, preprocessing, and augmentation.\n","5. **Optimization**: Algorithms for updating model parameters, such as SGD, Adam, etc.\n","6. **Autograd**: Operators supporting automatic differentiation.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Tensor Operations"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4])\n","tensor([[0.5387, 0.2830, 0.5071],\n","        [0.9525, 0.6684, 0.2839],\n","        [0.4818, 0.3317, 0.2035]])\n","tensor([[0., 0.],\n","        [0., 0.]])\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n"]}],"source":["## -- Tensor Creation --\n","import torch\n","\n","# Create a tensor from a list\n","a = torch.tensor([1, 2, 3, 4])\n","print(a)\n","\n","# Create a tensor with random values\n","b = torch.rand(3, 3)\n","print(b)\n","\n","# Create a tensor of zeros\n","c = torch.zeros(2, 2)\n","print(c)\n","\n","# Create a tensor of ones\n","d = torch.ones(2, 3)\n","print(d)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5, 7, 9])\n","tensor([-3, -3, -3])\n","tensor([ 4, 10, 18])\n","tensor([0.2500, 0.4000, 0.5000])\n"]}],"source":["## -- Arithmetic Operations --\n","\n","x = torch.tensor([1, 2, 3])\n","y = torch.tensor([4, 5, 6])\n","\n","# Addition\n","z = x + y\n","print(z)  # tensor([5, 7, 9])\n","\n","# Subtraction\n","z = x - y\n","print(z)  # tensor([-3, -3, -3])\n","\n","# Multiplication\n","z = x * y\n","print(z)  # tensor([4, 10, 18])\n","\n","# Division\n","z = x / y\n","print(z)  # tensor([0.2500, 0.4000, 0.5000])\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[19, 22],\n","        [43, 50]])\n"]}],"source":["## -- Matrix Multiplication Operations --\n","\n","a = torch.tensor([[1, 2], [3, 4]])\n","b = torch.tensor([[5, 6], [7, 8]])\n","\n","# Matrix multiplication\n","c = torch.mm(a, b)\n","print(c)  # tensor([[19, 22], [43, 50]])\n","#NOTE: the shape of the resulting tensor is m x n if a is of shape ( m x a ), b is of shape ( a x n) \n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 5, 12],\n","        [21, 32]])\n","tensor([[ 1,  4],\n","        [ 9, 16]])\n"]}],"source":["## -- Element-wise operations --\n","\n","a = torch.tensor([[1, 2], [3, 4]])\n","b = torch.tensor([[5, 6], [7, 8]])\n","\n","# Element-wise multiplication\n","c = a * b\n","print(c)  # tensor([[ 5, 12], [21, 32]])\n","\n","# Element-wise exponentiation\n","d = a ** 2\n","print(d)  # tensor([[ 1,  4], [ 9, 16]])\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(10)\n","tensor(2.5000)\n","tensor(4)\n","tensor(1)\n"]}],"source":["## -- Reduce Operations -- \n","a = torch.tensor([1, 2, 3, 4])\n","\n","# Sum\n","sum_a = torch.sum(a)\n","print(sum_a)  # tensor(10)\n","\n","# Mean\n","mean_a = torch.mean(a.float())\n","print(mean_a)  # tensor(2.5)\n","\n","# Max\n","max_a = torch.max(a)\n","print(max_a)  # tensor(4)\n","\n","# Min\n","min_a = torch.min(a)\n","print(min_a)  # tensor(1)\n","\n","#NOTE: these operations also change the dimension of the tensor, notice we go from [N] to [1]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3])\n","tensor([1, 4, 7])\n","tensor([[2, 3],\n","        [5, 6]])\n"]}],"source":["## -- Indexing and Slicing -- \n","\n","a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","# Select the first row\n","row = a[0, :]\n","print(row)  # tensor([1, 2, 3])\n","\n","# Select the first column\n","col = a[:, 0]\n","print(col)  # tensor([1, 4, 7])\n","\n","# Select a submatrix\n","submatrix = a[0:2, 1:3]\n","print(submatrix)\n","# tensor([[2, 3],\n","#         [5, 6]])\n"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Network operations"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 5.6566,  2.2045, -4.0038, -8.8806, -4.4412], grad_fn=<ViewBackward0>)\n","tensor([[ 0.2247, -0.1937,  0.1102,  0.1122, -0.2476,  0.2613,  0.1783, -0.2574,\n","          0.2661,  0.2910],\n","        [ 0.0687,  0.1771, -0.1070,  0.2942,  0.0142, -0.0786,  0.0120,  0.2393,\n","         -0.0371, -0.0315],\n","        [ 0.2894, -0.3155,  0.2887, -0.1632, -0.1273, -0.2657,  0.0326, -0.1205,\n","          0.1410, -0.1886],\n","        [-0.2306,  0.0667, -0.0109, -0.1910,  0.1297,  0.1759, -0.2175, -0.2968,\n","         -0.3054, -0.3088],\n","        [ 0.1541, -0.2164, -0.2813,  0.2545,  0.0256, -0.2343,  0.0421, -0.0850,\n","          0.0504, -0.2954]])\n","tensor([ 0.2173, -0.0241, -0.2926,  0.0431, -0.1737])\n","tensor([[[-0.0373,  0.9377,  1.4127,  0.6872, -0.6168,  1.1203,  0.5345,\n","          -1.3682],\n","         [ 0.5366, -0.4932,  0.1191,  0.6544,  0.6114, -1.0175, -0.5298,\n","           0.4790],\n","         [-0.5222,  0.8440, -0.4290, -0.1199, -0.8134,  1.6066, -1.0080,\n","          -0.7929],\n","         [ 0.3540, -0.5653, -0.9322, -0.0980,  0.8877, -0.2382, -0.8971,\n","           0.5952],\n","         [ 0.6986,  0.3219, -1.0540, -0.4694, -0.0351, -0.8022, -0.7276,\n","           0.9083],\n","         [-0.6588,  0.5682, -0.3335,  0.2433,  0.8223,  0.7498, -0.5161,\n","           0.5719],\n","         [-1.0938,  0.3233, -0.2717, -1.0217, -1.4300,  0.9526, -0.0473,\n","          -0.5834],\n","         [ 0.9087, -0.5068, -1.2447, -0.3490,  0.5443, -1.3396, -1.5546,\n","           0.1689],\n","         [-0.2156, -0.7542, -1.2744, -0.8321,  0.6107, -1.2578, -0.7274,\n","           0.5523],\n","         [ 0.2085, -0.3213, -0.7392, -0.7942, -0.3986,  0.1610, -1.0976,\n","          -1.1919],\n","         [ 0.2710, -0.0438, -0.7756,  0.1103,  0.0921, -0.2871, -0.6836,\n","           0.9189],\n","         [ 0.1481, -0.1215, -0.7175,  0.0858, -0.0600, -0.3592, -0.1863,\n","          -0.6727],\n","         [ 0.2262, -0.2747,  0.8226,  0.1419, -0.3907,  0.0110,  1.3498,\n","           0.0731],\n","         [ 1.1295, -0.8062, -0.3193,  0.2343,  0.5518, -1.3162, -0.4351,\n","           1.3631],\n","         [ 0.0243, -0.1852,  0.0069,  0.2188, -0.0306, -0.0853,  0.3448,\n","           1.1755],\n","         [ 0.1270, -0.0421, -0.5574,  0.2012, -0.4564, -0.2067, -0.3213,\n","           0.6948],\n","         [ 0.2883, -0.0565, -0.6320, -1.6428, -0.2671, -0.5133, -0.6406,\n","          -0.7551],\n","         [ 0.4655,  0.6242,  0.2831, -1.0491, -0.6620,  0.1663, -0.3120,\n","          -0.6949],\n","         [-0.0108, -0.4952,  0.1106, -0.9159, -1.8279,  0.1128,  0.8297,\n","          -0.9974],\n","         [-0.6392, -0.2619,  0.3376, -0.5676, -0.7024,  0.0362, -0.6235,\n","          -1.8683]]], grad_fn=<ConvolutionBackward0>)\n","torch.Size([1, 20, 8])\n","tensor([[[-0.0877,  0.0491, -0.2935],\n","         [-0.0123,  0.2849,  0.3304],\n","         [ 0.2522,  0.0683, -0.1332]],\n","\n","        [[-0.1314,  0.1131,  0.1976],\n","         [-0.2050,  0.2723,  0.1845],\n","         [-0.1737, -0.2403,  0.1155]],\n","\n","        [[-0.1832,  0.2702, -0.2520],\n","         [-0.0497,  0.3077, -0.1087],\n","         [-0.0343,  0.3264, -0.2374]],\n","\n","        [[ 0.1996,  0.1709,  0.1654],\n","         [ 0.0739, -0.1231, -0.1629],\n","         [-0.2844,  0.0591,  0.2608]],\n","\n","        [[-0.2829, -0.3014,  0.3198],\n","         [-0.2927, -0.0988, -0.0302],\n","         [-0.0982,  0.2110, -0.1462]],\n","\n","        [[-0.0086,  0.2088,  0.2049],\n","         [ 0.2400,  0.2244, -0.1547],\n","         [-0.1091,  0.0950, -0.1730]],\n","\n","        [[ 0.0442,  0.0383, -0.0929],\n","         [ 0.0488, -0.3296, -0.0339],\n","         [ 0.1100,  0.1554, -0.3031]],\n","\n","        [[-0.1537, -0.0745,  0.2322],\n","         [-0.2128,  0.0644, -0.0097],\n","         [-0.1658,  0.1580,  0.2770]],\n","\n","        [[-0.0388, -0.1774,  0.1726],\n","         [ 0.1585, -0.1744, -0.2643],\n","         [-0.0414,  0.0264,  0.2207]],\n","\n","        [[ 0.3127,  0.1722,  0.0423],\n","         [-0.0155, -0.2666,  0.0766],\n","         [-0.0542,  0.3058,  0.2179]],\n","\n","        [[-0.2961, -0.3331, -0.1063],\n","         [ 0.0099, -0.1510, -0.0165],\n","         [-0.2187,  0.0774,  0.0432]],\n","\n","        [[ 0.1771, -0.2019,  0.0369],\n","         [ 0.0340,  0.2223, -0.0434],\n","         [-0.0839,  0.2643, -0.0513]],\n","\n","        [[ 0.1186,  0.0659, -0.1467],\n","         [-0.1573, -0.0372, -0.0695],\n","         [ 0.1084, -0.2558, -0.0123]],\n","\n","        [[-0.1815, -0.1989,  0.0767],\n","         [-0.2183, -0.3067,  0.1184],\n","         [-0.2616, -0.2296,  0.3203]],\n","\n","        [[-0.1364,  0.0412,  0.0053],\n","         [-0.1501, -0.0095, -0.1260],\n","         [-0.1444, -0.2337, -0.1176]],\n","\n","        [[-0.1380, -0.1795,  0.0549],\n","         [-0.1788, -0.0204,  0.0621],\n","         [-0.2569,  0.0218, -0.2177]],\n","\n","        [[ 0.0722,  0.1554,  0.2514],\n","         [-0.1822, -0.2888, -0.1396],\n","         [ 0.2949,  0.2173,  0.1838]],\n","\n","        [[-0.0679,  0.1728,  0.2117],\n","         [-0.3242, -0.2605,  0.1528],\n","         [ 0.3091,  0.1611, -0.0184]],\n","\n","        [[ 0.1845, -0.0136, -0.2989],\n","         [-0.2789, -0.3264, -0.0535],\n","         [ 0.1572, -0.0135, -0.0536]],\n","\n","        [[ 0.0243,  0.0987, -0.2329],\n","         [ 0.1511, -0.0651,  0.2510],\n","         [ 0.2548,  0.0533,  0.2142]]])\n","torch.Size([20, 3, 3])\n","torch.Size([20])\n","tensor([[[[ 0.3740, -0.0735,  0.3342,  ...,  0.2040, -0.7324,  0.1758],\n","          [ 0.2203, -0.2161,  0.8257,  ...,  0.4019,  0.9647, -0.1639],\n","          [-0.4276,  0.6062, -0.1104,  ...,  0.1035,  0.6305, -0.6730],\n","          ...,\n","          [-0.7030,  0.2134,  0.1386,  ..., -0.5519, -0.0226,  0.2795],\n","          [-0.3016, -0.4810,  0.1442,  ..., -0.9983, -0.5837,  0.5001],\n","          [ 0.2040,  0.4060,  0.0318,  ...,  0.0799,  1.0035,  0.1961]],\n","\n","         [[ 0.2647, -0.1879, -0.3020,  ..., -0.1096, -0.1592,  0.1364],\n","          [ 0.4161, -0.0178,  1.3689,  ...,  0.3706,  0.0049, -0.2914],\n","          [-0.8070,  1.4333, -0.0943,  ...,  0.6771, -0.2457, -0.8326],\n","          ...,\n","          [-0.6839,  0.3641, -0.8276,  ..., -0.8452,  0.1380, -0.1842],\n","          [ 0.6026, -0.3926,  0.3288,  ..., -0.2875, -0.0680,  0.6362],\n","          [ 0.3130,  0.1445, -1.8587,  ..., -0.2982, -0.2214, -0.6623]],\n","\n","         [[ 0.4748,  1.0268,  0.2473,  ..., -0.0359,  0.3250, -0.3018],\n","          [-0.3815, -0.3546, -0.7112,  ...,  0.6687,  0.5817, -0.3812],\n","          [-0.0791,  0.2045,  0.7269,  ..., -0.9076,  0.4551,  0.3723],\n","          ...,\n","          [-0.5778,  0.2744, -0.3185,  ..., -0.0617, -0.3295, -1.1828],\n","          [-1.0015, -1.3970,  0.2810,  ...,  0.6274,  0.3672,  0.2757],\n","          [ 0.0871, -0.9929, -0.3539,  ...,  0.0092,  0.0762,  0.9705]],\n","\n","         ...,\n","\n","         [[ 0.6595, -0.1176, -0.9203,  ...,  0.3258, -0.5226, -0.2251],\n","          [ 0.2726, -0.7880,  0.5383,  ...,  0.2501, -0.1523,  0.0283],\n","          [-0.8052,  0.4795, -0.8577,  ..., -0.1812,  0.6330, -1.3531],\n","          ...,\n","          [-0.8640,  0.9500, -0.3678,  ..., -1.0001,  0.0232, -0.5638],\n","          [-0.6685, -0.4045,  0.5970,  ..., -0.0063,  0.2153,  0.8255],\n","          [ 0.1510, -0.8418,  0.0128,  ...,  0.2824, -0.0717,  0.9114]],\n","\n","         [[ 1.3577,  0.1480, -0.0584,  ...,  0.0475, -0.5679, -0.9103],\n","          [-0.7117, -0.0579, -0.4971,  ...,  0.6212, -0.1277, -0.1888],\n","          [-0.4834, -0.2072,  0.1229,  ...,  0.0259,  0.4594,  0.0991],\n","          ...,\n","          [ 0.3337,  0.4716,  0.4658,  ...,  0.1108,  0.6293, -0.6622],\n","          [-0.6854, -0.4178,  0.3424,  ..., -0.3010,  0.0323, -0.9919],\n","          [-0.0270, -0.6551,  1.1434,  ...,  0.4565, -0.1364,  0.5669]],\n","\n","         [[-0.1664, -0.1338,  1.0038,  ...,  0.2252, -0.0446, -0.8736],\n","          [-0.4962,  0.6930, -0.7293,  ..., -0.3751, -0.0723,  0.0806],\n","          [ 0.4531, -1.3905,  0.2379,  ...,  0.8636, -0.7365, -0.1893],\n","          ...,\n","          [ 0.1899, -1.0037,  0.9291,  ..., -0.3473, -0.6488,  0.3141],\n","          [ 0.5175,  0.6636, -1.5841,  ...,  0.0223,  0.2204, -0.6881],\n","          [-0.3636,  0.4639,  0.4747,  ..., -0.4265, -0.0869, -0.6324]]]],\n","       grad_fn=<ConvolutionBackward0>)\n","torch.Size([1, 12, 30, 30])\n","tensor([[[[-0.0792, -0.1262,  0.0767],\n","          [-0.0393, -0.1147,  0.1669],\n","          [ 0.1491,  0.0853,  0.1017]],\n","\n","         [[-0.0493,  0.0905,  0.1710],\n","          [ 0.0713,  0.1859, -0.0430],\n","          [-0.1156,  0.0620, -0.0012]],\n","\n","         [[-0.0334, -0.1902,  0.1850],\n","          [ 0.0660,  0.0853, -0.0475],\n","          [ 0.0255, -0.0261, -0.0457]]],\n","\n","\n","        [[[ 0.0016,  0.1894, -0.1387],\n","          [ 0.1532, -0.0650,  0.1048],\n","          [-0.0155, -0.1004, -0.0604]],\n","\n","         [[-0.0953,  0.0492,  0.0969],\n","          [-0.0107,  0.1698,  0.0866],\n","          [-0.1339, -0.1318, -0.1132]],\n","\n","         [[-0.0588, -0.1028,  0.0838],\n","          [-0.1309, -0.1224,  0.0605],\n","          [ 0.1138, -0.1175, -0.1190]]],\n","\n","\n","        [[[ 0.0486,  0.1923,  0.1312],\n","          [-0.1502, -0.0567,  0.0555],\n","          [-0.0908, -0.1306, -0.1750]],\n","\n","         [[-0.0457,  0.1721, -0.0253],\n","          [-0.0378,  0.0172,  0.1444],\n","          [ 0.0239, -0.1848, -0.1177]],\n","\n","         [[ 0.1595,  0.1300, -0.0912],\n","          [ 0.0977,  0.1754, -0.0078],\n","          [-0.1263, -0.0686,  0.0155]]],\n","\n","\n","        [[[-0.1353,  0.0633, -0.1158],\n","          [-0.0904,  0.0003,  0.1223],\n","          [ 0.1323,  0.1303,  0.1896]],\n","\n","         [[ 0.0655,  0.1119,  0.0798],\n","          [-0.0559, -0.1452,  0.0841],\n","          [-0.1319, -0.0195,  0.1871]],\n","\n","         [[-0.1917,  0.0863, -0.0302],\n","          [-0.0060, -0.0502,  0.1765],\n","          [ 0.0070, -0.0460, -0.1866]]],\n","\n","\n","        [[[-0.1364,  0.0422, -0.0067],\n","          [-0.0120,  0.0251, -0.0652],\n","          [-0.1044, -0.1374, -0.0523]],\n","\n","         [[-0.0648, -0.1727, -0.0370],\n","          [ 0.0068, -0.1329, -0.0799],\n","          [ 0.1759,  0.1476, -0.1689]],\n","\n","         [[-0.0342,  0.1148, -0.0174],\n","          [-0.1311, -0.1499, -0.0462],\n","          [-0.0825,  0.0650, -0.0281]]],\n","\n","\n","        [[[ 0.1596,  0.0644, -0.0806],\n","          [-0.1631,  0.0115, -0.0248],\n","          [ 0.1290, -0.0043,  0.1022]],\n","\n","         [[ 0.0791, -0.0635,  0.0044],\n","          [ 0.1844, -0.0045, -0.1821],\n","          [ 0.1705, -0.0704,  0.1359]],\n","\n","         [[ 0.1654,  0.1290,  0.1472],\n","          [-0.0011, -0.1917,  0.1658],\n","          [ 0.0873, -0.0536,  0.0139]]],\n","\n","\n","        [[[ 0.1675, -0.1360,  0.0491],\n","          [ 0.1287,  0.0901, -0.0264],\n","          [-0.0171, -0.0712, -0.1461]],\n","\n","         [[ 0.0553,  0.0130,  0.0702],\n","          [-0.0629, -0.1728, -0.1508],\n","          [ 0.0855, -0.1445,  0.0305]],\n","\n","         [[ 0.0295, -0.1384, -0.0397],\n","          [ 0.0266, -0.0534,  0.0264],\n","          [-0.0103,  0.1850,  0.0330]]],\n","\n","\n","        [[[ 0.1213,  0.1276,  0.0036],\n","          [ 0.0855, -0.0403, -0.0964],\n","          [ 0.0442,  0.0594, -0.1880]],\n","\n","         [[ 0.0217, -0.0216, -0.1486],\n","          [-0.1180,  0.0967, -0.0954],\n","          [-0.1591,  0.1377,  0.1806]],\n","\n","         [[ 0.1902,  0.0131,  0.0397],\n","          [ 0.1077, -0.0815,  0.0682],\n","          [-0.0815,  0.1084,  0.0179]]],\n","\n","\n","        [[[-0.1367,  0.1645, -0.0106],\n","          [-0.1151, -0.1240, -0.1317],\n","          [ 0.0319,  0.0930,  0.0298]],\n","\n","         [[-0.0295,  0.1064,  0.0745],\n","          [-0.1712, -0.0198,  0.1896],\n","          [-0.1846,  0.1143, -0.0388]],\n","\n","         [[-0.1104, -0.0552, -0.1857],\n","          [-0.0442,  0.0520, -0.1442],\n","          [-0.1454,  0.1438, -0.0245]]],\n","\n","\n","        [[[-0.1168,  0.0324,  0.1358],\n","          [ 0.0225, -0.0888,  0.0420],\n","          [ 0.0414, -0.0509, -0.1800]],\n","\n","         [[-0.1754,  0.1332,  0.0684],\n","          [-0.0183,  0.0303, -0.0550],\n","          [ 0.0859,  0.0260,  0.0110]],\n","\n","         [[ 0.0829, -0.0757,  0.1293],\n","          [-0.0584,  0.1174, -0.0446],\n","          [ 0.1607, -0.0053,  0.0777]]],\n","\n","\n","        [[[-0.1056,  0.0944,  0.0490],\n","          [ 0.0195, -0.0169, -0.1788],\n","          [ 0.1791, -0.1145,  0.0739]],\n","\n","         [[-0.1501, -0.0277,  0.0315],\n","          [-0.0811, -0.1794, -0.0539],\n","          [ 0.0006, -0.1303,  0.1456]],\n","\n","         [[ 0.0088,  0.1839, -0.0081],\n","          [ 0.0021,  0.1361, -0.0863],\n","          [-0.0911, -0.0938,  0.0725]]],\n","\n","\n","        [[[ 0.1116, -0.0185, -0.1121],\n","          [ 0.0409,  0.1671, -0.1433],\n","          [ 0.0939, -0.0646,  0.1488]],\n","\n","         [[ 0.1496,  0.1186,  0.0601],\n","          [ 0.0441, -0.1395,  0.1473],\n","          [-0.0321,  0.0914, -0.0116]],\n","\n","         [[-0.0586,  0.1500, -0.0189],\n","          [-0.1436, -0.0726,  0.0367],\n","          [-0.0284,  0.0677,  0.0868]]]])\n","torch.Size([12, 3, 3, 3])\n","torch.Size([12])\n"]}],"source":["# Layer operations\n","in_features = 10\n","out_features = 5\n","\n","# Linear layers(Fully connected layers)\n","x = torch.tensor([1,2,3,4,5,6,7,8,9,10],dtype=torch.float32)\n","layer = nn.Linear(in_features,out_features)\n","output_tensor = layer(x)\n","print(output_tensor)\n","print(layer.weight.data)\n","print(layer.bias.data)\n","#NOTE: usually the last dimension is the feature dimension\n","\n","\n","# Convolutional layers (Conv1d, Conv2d, Conv3d)\n","# Conv 1d\n","# shape: (batch size, number of channels, sequence length of the input sequence)\n","x = torch.randn(1,3,10) # Create a input signal with sequence length 10 and a channel depth of 3 \n","in_channels = 3\n","out_channels = 20\n","kernel_size = 3\n","stride = 1\n","padding = 0\n","layer = nn.Conv1d(in_channels,out_channels,kernel_size,stride,padding)\n","#See the output tensor and its shape\n","output_tensor = layer(x)\n","print(output_tensor)\n","print(output_tensor.shape)\n","print(layer.weight.data)\n","print(layer.weight.data.shape)\n","print(layer.bias.data.shape)\n","#NOTE: PyTorch convolutions take in input where the channel dimension is in the middle\n","#Check out this notebook for more info on convolutions and how they work\n","\n","#Conv 2d\n","# shape: (batch size, number of channels, height of input signal, width of input signal)\n","x = torch.randn(1,3,32,32) # Create a input signal with sequence length 10 and a channel depth of 3 \n","in_channels = 3\n","out_channels = 12\n","kernel_size = (3,3)\n","stride = 1\n","padding = 0\n","layer = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n","output_tensor = layer(x)\n","print(output_tensor)\n","print(output_tensor.shape)\n","print(layer.weight.data)\n","print(layer.weight.data.shape)\n","print(layer.bias.data.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Given normalized_shape=[16], expected input with shape [*, 16], but got input of size[1, 16, 32, 32]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     13\u001b[0m normalization \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(num_features)\n\u001b[0;32m---> 14\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mnormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# NLP example\u001b[39;00m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m#(batch size = 1, num of channels = 16, sequence length = 100)\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[16], expected input with shape [*, 16], but got input of size[1, 16, 32, 32]"]}],"source":["# Normalization operations\n","\n","# NLP example\n","x = torch.randn(1,16,32,32)  #(batch size = 1, num of channels = 16, height = 32, width = 32)\n","\n","# Batch Normalization:  computes a mean and variance FOR EACH FEATURE (each channel)\n","num_features = 16\n","normalization = nn.BatchNorm2d(num_features)\n","output_tensor = normalization(x)\n","\n","# Layer Normalization: compute a mean and variance FOR EACH SAMPLE ()\n","num_features = 16\n","normalization = nn.LayerNorm(num_features)\n","output_tensor = normalization(x)\n","\n","\n","# NLP example\n","x = torch.randn(1,16,100) #(batch size = 1, num of channels = 16, sequence length = 100)\n","\n","# Batch Normalization:  computes a mean and variance FOR EACH FEATURE (each channel)\n","num_features = 16\n","normalization = nn.BatchNorm1d(num_features)\n","output_tensor = normalization(x)\n","\n","\n","# Layer Normalization: compute a mean and variance FOR EACH SAMPLE\n","num_features = 16\n","normalization = nn.LayerNorm(num_features)\n","output_tensor = normalization(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Activation Functions\n","\n","x = torch.randn(1,10)\n","\n","# ReLU: f(x)  = x if x >0 else 0 \n","activation_function = nn.ReLU()\n","output_tensor = activation_function(x)\n","\n","# Sigmoid: f(x) = 1 / (1 + e^-x)\n","activation_function = nn.Sigmoid()\n","output_tensor = activation_function(x)\n","\n","# Tanh: f(x)  = tanh(x)\n","activation_function = nn.Sigmoid()\n","output_tensor = activation_function(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loss functions\n","# these functions measure how well the output values match the ground truth values\n","\n","y = torch.randn(1,3,10)\n","y_pred = torch.randn(1,3,10)\n","\n","# MSE (Mean Squared Error) Loss: loss = (y - y_pred)^2\n","loss_function = nn.MSELoss()\n","loss = loss_function(y_pred,y)\n","\n","# Cross Entropy Loss\n","loss_function = nn.CrossEntropyLoss()\n","loss = loss_function(y_pred,y)\n","\n","# Binary Cross Entropy Loss\n","loss_function = nn.BCELoss()\n","loss = loss_function(y_pred,y)\n","\n","\n","# KL Divergence Loss\n","loss_function = nn.KLDivLoss()\n","# if not log_target: # default\n","#     loss_pointwise = target * (target.log() - input)\n","# else:\n","#     loss_pointwise = target.exp() * (target - input)\n","\n","loss = loss_function(y_pred,y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Optimizer Algorithms\n","# "]},{"cell_type":"markdown","metadata":{},"source":["##  How do you train a neural network to do something you want?\n","\n","* Data Preparation & Preprocessing\n","    * Step 1: Find a bunch of data for the thing you want to train your model on\n","    * Step 2: Preprocess the data to turn it into the right form\n","\n","* Model training\n","    * Step 1: Pass data to the model (forward pass) to get model output\n","    * Step 2: Compute the loss between data and model output\n","    * Step 3: Perform backpropagation (backward pass)\n","    * Step 4: Perform gradient descent and weight updates\n","\n","* Model Evaluation\n","    * Step 1: Evaluate it to assess its performance\n","    * Step 2: Assess its performance on test data\n","\n","\n","\n","In this notebook we will put more emphasis on model training, \n","The most common training loop you will ever see:\n","```python \n","for epoch in range(num_epochs):\n","    for batch in training_data:\n","        output = model(batch.x) # -(1)\n","        loss = loss_fn(batch.y,output) # -(2)\n","        loss.backward() # -(3)\n","        optimizer.step() # -(4)\n","        optimizer.zero_grad()\n","```"]},{"cell_type":"markdown","metadata":{},"source":["**Ambrose Yapping**\n","\n","\n","`Epoch` refers to how many times you go through your entire training data. Sometimes people use `training_steps` in different settings and depneding on the training task. For example, in diffusion model training, you may not need to go through the entire training dataset multiple times. So I can say `training_steps = 100`, meaning I want my model to perform weight updates 100 times (100 batches of data processed by the model)"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing your data (Data perspective)\n","* Depending on the data you have, you have different ways to **preprocess** your data\n","* Some of the most common modalities out there are text, images, audio."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Preparing your data (PyTorch perspective)\n","* In PyTorch, a `Dataset` object is a class that allows you to access samples of your data. This class also allows you to perform certain data transformations to process the raw samples you grab from your dataset\n","* In case you dont know, a **sample** just means 1 pair of input and output. So for a image classification task, a sample may be (x: image,y: label), in text sentiment analysis, sample may be (x: string of text,y: float probability).\n","\n","* Lets say you have a dataset called `MyDataset`,it inherits the `Dataset` class. It represents a collection of data samples. \n","    ```python\n","    # \n","    class MyDataset(Dataset):\n","        ...\n","        def __getitem__(self,index):\n","            return\n","\n","    ```\n","    - What this class lets you do is that you can specify the logic for accessing 1 sample\n","    - So when you do `dataset[0]` calls `__getitem__()` and a sample of data will be returned\n","    - you can also apply transformations that would be applied to all the samples in your dataset when you fetch them\n","        - inside `__getitem__()`, you can apply transformations to the input and output by doing `x = self.transform(x)` and `y = self.target_transform(y)`.\n","        - **When will I do this?**, some common usecases\n","            - Transforms:\n","                - Images: cropping, resizing, normalization\n","                - Text: tokenization (turning words into numbers), padding and truncation\n","                - Audio: resampling, normalization, augmentation\n","            - Target Transform:\n","                - one hot encoding\n","                - encoding labels\n","                - normalization (coordinates)\n","                - format conversions\n","        - NOTE: the most common application of transformations is on image modalities, for text we have designated components that specifically do preprocessing (tokenizer) \n","* You can also define specific ways you want to grab samples from the dataset through the use of a sampler or `Sampler` class, which intuitively dictates how you sample data from your dataset.\n","    ```python\n","    class MySampler(Sampler):\n","        ...\n","        def __iter__(self):\n","            for i in range(self.N):\n","                yield i \n","    ```\n","    - it is used to specify the dataloading order, and specifies the indexes\n","    - more specifically it is an iterable dataset\n","* As for a `DataLoader`, this object is the intermediate process that prepares the data we take from the data into batches.\n","    ```python\n","    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","    ```\n","    - the DataLoader receives the `dataset` object as input and returns the dataloader \n","### How they all work together:\n","<center>\n","<a href=\"https://ibb.co/ZMgcwc4\"><img src=\"https://i.ibb.co/W2zBdB8/Screenshot-2024-05-30-023025.png\" alt=\"Screenshot-2024-05-30-023025\" border=\"0\"></a></center>\n","\n","* 1. the dataset class allows you to index into different samples in your dataset\n","* 2. your CPU has different workers that grab the samples responsible for constructing a batch\n","* 3. the CPU workers load the queried samples into a queue. \n","* 4. the sampler class also provides which indices do we need for \n","* 4. the dataloader performs the collating procedure, which draws samples in the queue and puts them into a batch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Here is an example of a collating function\n","\n","def collate_fn(samples):\n","    return batch\n","\n","# As input, the collate function takes the raw batch_size number of samples\n","# It then performs some processing or you can define some custom logic\n","# to turn these samples into tensors"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]), tensor([5, 2, 8, 7, 1, 1, 2, 9, 9, 2, 6, 9, 1, 8, 9, 2, 1, 4, 5, 9, 7, 7, 2, 6,\n","        3, 7, 5, 8, 4, 1, 6, 2, 0, 0, 2, 1, 0, 9, 7, 8, 0, 9, 8, 9, 8, 4, 2, 0,\n","        6, 1, 9, 0, 4, 7, 5, 0, 3, 9, 6, 1, 6, 7, 9, 2])]\n","tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n","torch.Size([64, 1, 28, 28])\n","torch.Size([64])\n","<class 'list'>\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader,Dataset\n","import torch.nn.functional as F\n","from torchvision.transforms import v2\n","from torchvision.transforms import functional as TF \n","\n","# Load the MNIST dataset\n","transform = v2.Compose([v2.ToTensor(), v2.Normalize((0.1307,), (0.3081,))])\n","\n","\n","\n","# Create a train dataloader for the dataset\n","train_dataset = MNIST('',train=True,download=True,transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","\n","# Create a test dataloader for the dataset\n","test_dataset = MNIST('',train=True,download=True,transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=True)\n","\n","\n","\n","# What is happening here?\n","# Just like what we described before, the dataloader helps us fetch batches of data to be fed to our model\n","# We create an iterator with the iter() method, that allows us to iterate through the elements of the dataloader\n","# When we do :\n","# for batch in dataloader:\n","#   ...\n","# We create an iterator and executes the next() method to move onto the next batch after each iteration of the loop\n","print(next(iter(train_dataloader))) #each batch is a list\n","print(next(iter(train_dataloader))[0]) # at position 0, we have the x (or images)\n","print(next(iter(train_dataloader))[0].shape) # When you look at the shape, \n","print(next(iter(train_dataloader))[1].shape) # at position 1, we have the y (or labels)\n","print(type(next(iter(train_dataloader))))\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242,  0.1104,  1.0777,  2.3633,\n","           1.6887,  1.0777,  1.0777, -0.3478, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242,  0.8868,  2.6560,  2.8088,  2.8088,\n","           2.8088,  2.8088,  2.8088,  0.4159, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.1951,  2.5287,  2.8088,  2.8088,\n","           2.8088,  2.8088,  2.8088,  2.6051,  0.4668, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242,  0.8995,  2.6815,  2.8088,\n","           2.4269,  2.8088,  2.8088,  2.8088,  1.3705, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.2105,  2.8088,\n","           2.2487,  2.8088,  2.8088,  2.8088,  0.0085, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,  2.1978,\n","           2.8088,  2.8088,  2.8088,  2.8088,  0.0976, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1187,\n","           2.2360,  2.8088,  2.8088,  2.8088,  1.7523, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.7341,  1.3196,\n","           2.7960,  2.8088,  2.8088,  2.8088,  1.0013, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242,  0.0849,  2.2996,  2.7578,  2.8088,\n","           2.8088,  2.8088,  2.8088,  2.8088,  0.0085, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242,  0.6068,  2.7197,  2.8088,  2.8088,\n","           2.8088,  2.8088,  2.8088,  2.8088,  1.4341, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242,  0.9759,  2.7069,  2.8088,\n","           2.8088,  2.8088,  2.8088,  2.8088,  1.7523, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.5049,  0.7086,\n","           0.7086,  0.7595,  2.6306,  2.8088,  1.7523, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4115,  1.2050,  2.8088,  2.3633, -0.0296, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.3351,  2.8088,  2.8088,  2.2996, -0.0678, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242,  1.3323,  2.8088,  2.8088,  1.0523, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,\n","           1.8287,  0.7213, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.1060,  2.6306,  2.8088,  2.8088,  0.0085, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.6632,\n","           2.8088,  2.3124,  0.3777,  0.0340, -0.4242,  0.2249,  0.7086,\n","           2.1978,  2.8088,  2.8088,  1.3450, -0.3224, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3013,\n","           2.0832,  2.8088,  2.8088,  2.4015,  1.8669,  2.6433,  2.8088,\n","           2.8088,  2.7960,  2.2869, -0.2842, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","           1.3577,  0.9123,  2.8088,  2.8088,  2.8088,  2.8088,  2.8088,\n","           2.8088,  2.5797, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.3351,  1.0650,  1.0650,  1.6759,  2.8088,  2.8088,\n","           2.6178,  0.2122, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n","         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n","          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n","3\n"]}],"source":["# Lets see the dataset sample\n","# Each sample is a tuple (remeber it is a pair of x and y)\n","# THis is the x (image x)\n","print(train_dataset[10][0])\n","\n","# This is the label for the corresponding \n","print(train_dataset[10][1])\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 28, 28])\n","3\n"]}],"source":["# Lets see what is the shape of the sample\n","print(train_dataset[10][0].shape)\n","# This is the shape of the image\n","\n","print(train_dataset[10][1])\n","# This is the label"]},{"cell_type":"markdown","metadata":{},"source":["**Ambrose Yapping**\n","- **Parameters VS Hyperparameters**\n","    - **Hyperparameters**: refer to the numbers you can physically change when you train your model, configutations or settings you can explicity set. There are hyperparameters that are specific to the **training process** and some specific to the **deep learning model**. \n","    - Each hyperparameter has its own role in training. Altering either sets of these hyperparameters can greatly change the performance and output of the model\n","    - Here are some examples of training hyperparameters:\n","        - Learning rate\n","        - Batch size\n","        - Gradient accumulation steps\n","        - \n","    - Here are some examples of model hyperparameters:\n","        - Number of neurons\n","        - Number of feature dimensions\n","        - Number of layers\n","        - Number of channels\n","    - **Parameters**: refer to the numbers you cannot physically go change when you train your model. This refers to the weights and biases of each weight matrix inside your model. These are only influenced by the loss value that is computed.\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Train MNIST with our own neural network engine"]},{"cell_type":"markdown","metadata":{},"source":["# Creating our very own neural network engine\n","\n","Somtimes it may be confusing to understand what happens under the hood when you train a neural network in PyTorch. Just like Andrej Karpathy's video on backpropagation, I would like to extend it and give more intuition as to what happens when you train a neural network. \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Creating our own Multi-Layer Perceptron"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n","        super(MLP,self).__init__()\n","        self.linear1 = nn.Linear(28*28, hidden_size_1) \n","        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n","        self.linear3 = nn.Linear(hidden_size_2, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, img):\n","        x = img.view(-1, 28*28)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        x = self.linear3(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train MNIST using PyTorch"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def train(train_loader, net, epochs=5, total_iterations_limit=None):\n","    cross_el = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n","\n","    total_iterations = 0\n","\n","    for epoch in range(epochs):\n","        net.train()\n","\n","        loss_sum = 0\n","        num_iterations = 0\n","\n","        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n","        if total_iterations_limit is not None:\n","            data_iterator.total = total_iterations_limit\n","        for data in data_iterator:\n","            num_iterations += 1\n","            total_iterations += 1\n","            x, y = data\n","            x = x.to(\"cuda\")\n","            y = y.to(\"cuda\")\n","            optimizer.zero_grad()\n","            output = net(x.view(-1, 28*28))\n","            loss = cross_el(output, y)\n","            loss_sum += loss.item()\n","            avg_loss = loss_sum / num_iterations\n","            data_iterator.set_postfix(loss=avg_loss)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n","                return\n","            \n","def print_size_of_model(model):\n","    torch.save(model.state_dict(), \"temp_delme.p\")\n","    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n","    os.remove('temp_delme.p')\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Lets instantiate our model\n","model = MLP().to(\"cuda\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|| 938/938 [00:05<00:00, 164.70it/s, loss=0.279]\n","Epoch 2: 100%|| 938/938 [00:05<00:00, 164.89it/s, loss=0.115]\n","Epoch 3: 100%|| 938/938 [00:05<00:00, 171.96it/s, loss=0.084] \n","Epoch 4: 100%|| 938/938 [00:05<00:00, 172.99it/s, loss=0.0638]\n","Epoch 5: 100%|| 938/938 [00:05<00:00, 169.69it/s, loss=0.0514]\n"]}],"source":["train(train_dataloader,model)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def test(model: nn.Module, total_iterations: int = None):\n","    correct = 0\n","    total = 0\n","\n","    iterations = 0\n","\n","    model.eval()\n","    model.to(\"cuda\")\n","\n","    with torch.no_grad():\n","        for data in tqdm(test_loader, desc='Testing'):\n","            x, y = data\n","            x = x.to(\"cuda\")\n","            y = y.to(\"cuda\")\n","            output = model(x.view(-1, 784))\n","            for idx, i in enumerate(output):\n","                if torch.argmax(i) == y[idx]:\n","                    correct +=1\n","                total +=1\n","            iterations += 1\n","            if total_iterations is not None and iterations >= total_iterations:\n","                break\n","    print(f'Accuracy: {round(correct/total, 3)}')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Testing: 100%|| 6000/6000 [00:12<00:00, 497.02it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.983\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test(model)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualizing the loss landscape"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Extract model weights\n","weights = []\n","# We iterate through all the parameters of the model\n","for param in model.parameters():\n","    weights.append(param.data.view(-1).cpu().numpy())\n","\n","# You get one big array that stores all the parameters of our model\n","weights = np.concatenate(weights)\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["(89610,)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["weights.shape\n","\n","# Do a sanity check for why is it size 89610\n","#           weights    bias   param for that layer\n","# Layer 1: 784 * 100 + 100 =  78500\n","# Layer 2: 100 * 100 + 100 =  10100\n","# Layer 3: 100 * 10 + 10   =  1010\n","\n","# Add them all up:\n","# 78500 + 10100 + 1010 = 89610"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Define grid points for visualization in 3D\n","grid_resolution = 20\n","x_min, x_max = weights.min(), weights.max()\n","y_min, y_max = weights.min(), weights.max()\n","z_min, z_max = weights.min(), weights.max()\n","xx, yy, zz = np.meshgrid(np.linspace(x_min, x_max, grid_resolution),\n","                         np.linspace(y_min, y_max, grid_resolution),\n","                         np.linspace(z_min, z_max, grid_resolution))\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate loss on a batch of data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     25\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py:53\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 53\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:50\u001b[0m, in \u001b[0;36mToTensor._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Union[PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray], params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_F\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import matplotlib.pyplot as plt\n","\n","# Evaluate loss at each grid point\n","# Evaluate loss at each grid point\n","loss_values = np.zeros(xx.shape)\n","model.eval().cpu()\n","\n","with torch.no_grad():\n","    for i in range(grid_resolution):\n","        for j in range(grid_resolution):\n","            for k in range(grid_resolution):\n","                # Set weights to current grid point\n","                idx = 0\n","                for param in model.parameters():\n","                    param_data = param.data.view(-1).cpu().numpy()\n","                    num_params = param_data.size\n","                    \n","                    if idx + num_params <= xx[i, j, k].size:\n","                        param_data[:] = xx[i, j, k][idx:idx+num_params]\n","                    idx += num_params\n","\n","                # Evaluate loss on a batch of data\n","                total_loss = 0\n","                for data, target in train_dataloader:\n","                    output = model(data)\n","                    loss = F.cross_entropy(output, target, reduction='sum').item()\n","                    total_loss += loss\n","                \n","                loss_values[i, j, k] = total_loss / len(train_dataloader.dataset)\n","\n","# Plot the loss landscape in 3D\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Normalize the loss values for better visualization\n","norm_loss_values = (loss_values - np.min(loss_values)) / (np.max(loss_values) - np.min(loss_values))\n","\n","# Plot the surface\n","surf = ax.plot_surface(xx[:, :, 0], yy[:, :, 0], norm_loss_values[:, :, 0], cmap=plt.cm.coolwarm, edgecolor='none')\n","ax.set_title('3D Loss Landscape Visualization')\n","ax.set_xlabel('Weight Component 1')\n","ax.set_ylabel('Weight Component 2')\n","ax.set_zlabel('Normalized Loss')\n","fig.colorbar(surf, shrink=0.5, aspect=5)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the loss landscape in 3D\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Normalize the loss values for better visualization\n","norm_loss_values = (loss_values - np.min(loss_values)) / (np.max(loss_values) - np.min(loss_values))\n","\n","# Plot the surface\n","surf = ax.scatter(xx.flatten(), yy.flatten(), zz.flatten(), c=norm_loss_values.flatten(), cmap=plt.cm.coolwarm)\n","fig.colorbar(surf, label='Normalized Loss')\n","\n","ax.set_title('3D Loss Landscape Visualization')\n","ax.set_xlabel('Weight Component 1')\n","ax.set_ylabel('Weight Component 2')\n","ax.set_zlabel('Bias Component')"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPy/ZaTDRYGonjxONkNzLGo","gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
