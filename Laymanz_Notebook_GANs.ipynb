{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laymanz Notebooks: Generative Adversarial Networks\n",
    "Author: Ambrose Ling\n",
    "\n",
    "**What is this notebook about?**\n",
    "\n",
    "In this notebook, we will go over some of the most fundamental ideas behind General Adversarial Networks, how they work and why they have been a major advancement in the field of computer vision and generative artifical intelligence. We hope that you can walk away capable of building your own GAN framework along with training your own model from scratch and understanding some of the core ideas that are trending in this field of research.\n",
    "\n",
    "**What do I need to set up my environment?**\n",
    "\n",
    "All of our notebooks will only use numpy, pytorch, matplotlib for visualizations. We will not use any other third-party libraries for model development, optimization or anything like that.\n",
    "\n",
    "**How is this notebook structured?**\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "\n",
    "**Covered papers in this notebook**\n",
    "\n",
    "(will do after finishing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Generative Adversarial Network? (https://github.com/soumith/ganhacks?tab=readme-ov-file#authors)\n",
    "\n",
    "A Generative Adversarial Network (GAN), is a generative model. It aims to learn the distribution of data through an adversarial process. Meaning that the model is in adversary with another (in competition with another).\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "In the GAN framework, you have 2 components:\n",
    "\n",
    "1. The Generator\n",
    "- Its goal is to generate realistic/synthetic images similar to the ones in our dataset. This model is trying to fit our real data distribution.\n",
    "We represent this generator as $G(z,\\theta)$, the generator also defines a mapping from input latent noise to data space.\n",
    "- It recevies noise as input and tries to output a result close to data\n",
    "- **Intuition**: Think of the generator as the counterfeits, they are trying to generate fake money (as realistic as possible) to fool the police (the discriminator).\n",
    "\n",
    "2. The Discriminator\n",
    "- Its goal is to determine if its input comes from the training dataset or from the generator. \n",
    "- More specifically it determines whether a sample comes from the data distribution or the generator distribution\n",
    "- **Intuition**: Think of the discriminator as the police, they are trying to determine if the money they see is fake or real.\n",
    "\n",
    "Some math notation:\n",
    "- $p_{data}(x)$: data distribution\n",
    "- $p_{g}(x)$: generator distribution\n",
    "- $D$: discriminator\n",
    "- $G$: generator\n",
    "- $z$: latent noise variable\n",
    "\n",
    "\n",
    "### How do we train a GAN ?\n",
    "\n",
    "The training objective:\n",
    "$$\n",
    "min_G max_D V(D,G) = E_{x \\sim p_{data}(x)}[log(D(x))] + E_{z \\sim p_z(z)}[log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "**What is this telling us?**\n",
    "- We are training the **discriminator D** to maximize the following expression (maximize the probability that D assigns the correct label to the sample)\n",
    "- We are training the **generator G** to minimize the expression (minimize the probability that D assigns the correct label to the sample, the generator wants to fool the discriminiator D)\n",
    "\n",
    "\n",
    "**NOTE**:\n",
    "- Conv tranpoes: $o = (i-1) \\times s + k - 2p$\n",
    "\n",
    "**Some stuff about WGANs**:\n",
    "https://arxiv.org/pdf/1701.07875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we measure the similarity / difference between 2 probability distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence\n",
    "KL divergence measures how one probability distribution $p$ diverges from a second expected probability distribution.\n",
    "\n",
    "$$\n",
    "D_{KL} = \\int_x p(x) log(\\frac{p(x)}{q(x)}) dx\n",
    "$$\n",
    "\n",
    "**Some nice properties of the KL divergence**:\n",
    "- KL divergence abhors regions where $q(x)$ has non-null mass and $p(x)$ has null mass. This is useful when you are trying to approximate a complex (intractable) distribution $q(x)$ with a tractable distribution $p(x)$.\n",
    "- KL divergence is always non-negative $D_{KL}(P||Q) = 0$ iff $p(x) == q(x)$\n",
    "\n",
    "**Challenges with using the KL divergence**:\n",
    "- Dependence on support: (support is the set of points where probabilty is nonzero or $P > 0$, or the subset of the domain where elements are **not** mapped to zero.). In order to have a defined KL divergence, $support(P) \\subset support(Q)$ and it means that $D_{KL}(P||Q)$ is finite.\n",
    "- Asymetry: $D(P||Q) \\neq D(Q||P)$, these are different operations. If $q(x) >>> 0$ and $p(x) ~ 0$, $q$ has a very small effect on the divergence., wont be a good measure when you have 2 equally important distributions\n",
    "\n",
    "**NOTE:**\n",
    "KL Divergence is not a metric proper.\n",
    "https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence \n",
    "\n",
    "### Jensen Shannon Divergence\n",
    "Jensen Shannon Divergence als measures how one probability distribution $p$ diverges from a second expected probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Div Loss: 1.1949207782745361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/4nl6v6yx5xbfhw761z9zmbv40000gn/T/ipykernel_75675/2031231504.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.log_softmax(p)\n",
      "/var/folders/0l/4nl6v6yx5xbfhw761z9zmbv40000gn/T/ipykernel_75675/2031231504.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.log_softmax(q)\n"
     ]
    }
   ],
   "source": [
    "# KL divergence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "p = torch.randn(1,100)\n",
    "q = torch.randn(1,100)\n",
    "\n",
    "# Softmax\n",
    "# exp_sum = p.exp().sum()\n",
    "# p = p.exp() / exp_sum\n",
    "\n",
    "# We get a probabiltiy distribution\n",
    "p = F.log_softmax(p)\n",
    "q = F.log_softmax(q)\n",
    "\n",
    "kl_div = p.exp() * (p - q)\n",
    "\n",
    "#Reduce it to a loss value for backprop\n",
    "loss = kl_div.sum() / p.shape[0]\n",
    "\n",
    "print(f\"KL Div Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "tensor(-0.1883)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "from torchvision.io import read_image\n",
    "root = \"/Users/ambroseling/Desktop/carly-dataset\"\n",
    "def Preprocess(tensor: torch.Tensor):\n",
    "    tensor = v2.Resize(64)(tensor)\n",
    "    tensor = tensor.float()/127.5 - 1.0\n",
    "    tensor = tensor[:3]\n",
    "    return tensor\n",
    "transforms = v2.Compose([Preprocess])\n",
    "carly_dataset = torchvision.datasets.DatasetFolder(root,loader = read_image,transform= transforms,extensions=['png'])\n",
    "dataloader = DataLoader(carly_dataset,batch_size=2)\n",
    "print(carly_dataset[10][0].shape)\n",
    "print(carly_dataset[14][0].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DCGAN(nn.Module):\n",
    "    '''\n",
    "    Input: (100,)\n",
    "    Output: (64,64) \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.conv_up = nn.ConvTranspose2d(100,1024,4)\n",
    "        self.conv1_up = nn.ConvTranspose2d(1024,512,4,stride=2,padding=1) # (4-1)*2 +4 - 2 = 8\n",
    "        self.batch_norm1 = nn.BatchNorm2d(512)\n",
    "        self.conv2_up = nn.ConvTranspose2d(512,256,4,stride=2,padding=1) # (8-1)*2 + 4 - 2= 16\n",
    "        self.batch_norm2 = nn.BatchNorm2d(256)\n",
    "        self.conv3_up = nn.ConvTranspose2d(256,128,4,stride=2,padding=1) # (16 - 1)*2 +4 -2 = 32\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4_up = nn.ConvTranspose2d(128,3,4,stride=2,padding=1) # (32-1) *2+4-2 = 64\n",
    "        self.batch_norm4 = nn.BatchNorm2d(3)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        x = self.conv_up(x)\n",
    "        x = self.conv1_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.conv2_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.conv3_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.conv4_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.h1 = nn.Linear(100,16)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(16)\n",
    "        self.h2 = nn.Linear(16,256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        self.h3 = nn.Linear(256,512)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.h4 = nn.Linear(512,1024)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(1024)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        x = self.h1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.h4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DCGAN()\n",
    "x = torch.randn(1,100,1,1)\n",
    "out =  generator(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an optimizer for the generator\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(),lr=0.002)\n",
    "\n",
    "# Why do we use Adam for the generator?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()# 64,64\n",
    "        self.conv1 = nn.Conv2d(3,64,4,stride=2,padding=1) # 32\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64,128,4,stride=2,padding=1) # 16\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128,256,4,stride=2,padding=1) # 8\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256,512,4,stride=2,padding=1) # 4\n",
    "        self.batch_norm4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512,1,4,stride=1) # floor((64 - 5) /1) +1 = 60\n",
    "        self.leaky_relu = nn.LeakyReLU(0.02)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5207, grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "\n",
    "x = torch.randn(1,3,64,64)\n",
    "out = discriminator(x)\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_d = torch.optim.SGD(discriminator.parameters(),lr=0.002)\n",
    "\n",
    "# Why do we use SGD for the discriminator?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs = 100\n",
    "steps = 1\n",
    "d_loss = []\n",
    "g_loss = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tensorboard to log the loss and val images\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    training_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            for step in range(steps):\n",
    "                x = batch[0]\n",
    "                prob = discriminator(x).squeeze()\n",
    "                truth = (torch.randint(900,1000,(x.shape[0],))/1000).float()\n",
    "                loss_real = loss_fn(prob,truth)\n",
    "                loss_real.backward()\n",
    "\n",
    "                z = torch.randn(x.shape[0],100,1,1)\n",
    "                x_gen = generator(z)\n",
    "                prob = discriminator(x_gen).squeeze()\n",
    "                truth = (torch.randint(0,100,(x.shape[0],))/1000).float()\n",
    "                loss_fake = loss_fn(prob,truth)\n",
    "                loss_fake.backward()\n",
    "\n",
    "                loss = loss_real + loss_fake\n",
    "                d_loss.append(loss)\n",
    "                writer.add_scalar(\"Discriminator Loss/train\", loss, training_step)\n",
    "                optimizer_d.step()\n",
    "                optimizer_d.zero_grad()\n",
    "\n",
    "                if training_step % 5 ==0:\n",
    "                    with torch.no_grad():\n",
    "                        z = torch.randn(1,100,1,1)\n",
    "                        x_gen = generator(z)\n",
    "                        x_rgb = (x_gen*(255/2) +(255/2)).permute(0,2,3,1)[0].round().numpy().astype(np.uint8)\n",
    "                        writer.add_image(\"Generator result\",x_rgb,global_step = training_step,dataformats=\"HWC\")\n",
    "                        # x_rgb.save(\"val.png\")\n",
    "\n",
    "            z = torch.randn(x.shape[0],100,1,1)\n",
    "            x_gen = generator(z)\n",
    "            prob = discriminator(x_gen).squeeze()\n",
    "            truth = (torch.randint(0,100,(x.shape[0],))/1000).float()\n",
    "            loss = loss_fn(prob,truth)\n",
    "            writer.add_scalar(\"Generator Loss/train\", loss, training_step)\n",
    "            g_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer_g.step()\n",
    "            optimizer_g.zero_grad()\n",
    "\n",
    "            print(f\"Epoch {epoch} - D Loss: {d_loss[-1]} G Loss: {g_loss[-1]}\")\n",
    "            training_step +=1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - D Loss: 1.4082250595092773 G Loss: 0.6733392477035522\n",
      "Epoch 0 - D Loss: 1.2986054420471191 G Loss: 0.34673768281936646\n",
      "Epoch 0 - D Loss: 1.1153295040130615 G Loss: 0.34906458854675293\n",
      "Epoch 0 - D Loss: 0.2991675138473511 G Loss: 0.2963500916957855\n",
      "Epoch 0 - D Loss: 0.3781273365020752 G Loss: 0.21757778525352478\n",
      "Epoch 0 - D Loss: 0.5941674113273621 G Loss: 0.2777368426322937\n",
      "Epoch 0 - D Loss: 0.3844674229621887 G Loss: 0.1595897674560547\n",
      "Epoch 0 - D Loss: 0.46290668845176697 G Loss: 0.34625232219696045\n",
      "Epoch 0 - D Loss: 0.4224802255630493 G Loss: 0.10548730194568634\n",
      "Epoch 0 - D Loss: 0.5365190505981445 G Loss: 0.2056998610496521\n",
      "Epoch 0 - D Loss: 0.5386652946472168 G Loss: 0.20017173886299133\n",
      "Epoch 0 - D Loss: 0.3867282271385193 G Loss: 0.19370493292808533\n",
      "Epoch 0 - D Loss: 0.49545741081237793 G Loss: 0.36607617139816284\n",
      "Epoch 0 - D Loss: 0.28459179401397705 G Loss: 0.2518894672393799\n",
      "Epoch 0 - D Loss: 0.7043341398239136 G Loss: 0.2009260356426239\n",
      "Epoch 0 - D Loss: 0.45573222637176514 G Loss: 0.2626751661300659\n",
      "Epoch 0 - D Loss: 0.5250300765037537 G Loss: 0.30529874563217163\n",
      "Epoch 0 - D Loss: 0.256436288356781 G Loss: 0.30626046657562256\n",
      "Epoch 0 - D Loss: 0.5226417779922485 G Loss: 0.1697976291179657\n",
      "Epoch 0 - D Loss: 0.4084266424179077 G Loss: 0.1935712993144989\n",
      "Epoch 0 - D Loss: 0.4828917682170868 G Loss: 0.26097920536994934\n",
      "Epoch 0 - D Loss: 0.5016734600067139 G Loss: 0.14244376122951508\n",
      "Epoch 0 - D Loss: 0.46645575761795044 G Loss: 0.3605690002441406\n",
      "Epoch 0 - D Loss: 0.5237168669700623 G Loss: 0.3366241455078125\n",
      "Epoch 0 - D Loss: 0.5732312202453613 G Loss: 0.29360222816467285\n",
      "Epoch 1 - D Loss: 0.5962961912155151 G Loss: 0.22491486370563507\n",
      "Epoch 1 - D Loss: 0.5813109874725342 G Loss: 0.22053422033786774\n",
      "Epoch 1 - D Loss: 0.532273530960083 G Loss: 0.23414131999015808\n",
      "Epoch 1 - D Loss: 0.42806804180145264 G Loss: 0.2693348526954651\n",
      "Epoch 1 - D Loss: 0.5192022323608398 G Loss: 0.15628814697265625\n",
      "Epoch 1 - D Loss: 0.3285362720489502 G Loss: 0.23445558547973633\n",
      "Epoch 1 - D Loss: 0.5939064025878906 G Loss: 0.27207544445991516\n",
      "Epoch 1 - D Loss: 0.37784022092819214 G Loss: 0.1549917459487915\n",
      "Epoch 1 - D Loss: 0.44161659479141235 G Loss: 0.24736805260181427\n",
      "Epoch 1 - D Loss: 0.4168771803379059 G Loss: 0.2871630787849426\n",
      "Epoch 1 - D Loss: 0.3045702278614044 G Loss: 0.20206810534000397\n",
      "Epoch 1 - D Loss: 0.44162800908088684 G Loss: 0.08822973072528839\n",
      "Epoch 1 - D Loss: 0.44756242632865906 G Loss: 0.2101571261882782\n",
      "Epoch 1 - D Loss: 0.3756357431411743 G Loss: 0.13941428065299988\n",
      "Epoch 1 - D Loss: 0.45008140802383423 G Loss: 0.27408382296562195\n",
      "Epoch 1 - D Loss: 0.47576600313186646 G Loss: 0.2870550751686096\n",
      "Epoch 1 - D Loss: 0.5791801810264587 G Loss: 0.18164417147636414\n",
      "Epoch 1 - D Loss: 0.35205668210983276 G Loss: 0.2720540165901184\n",
      "Epoch 1 - D Loss: 0.5594943165779114 G Loss: 0.32926836609840393\n",
      "Epoch 1 - D Loss: 0.37548714876174927 G Loss: 0.23849384486675262\n",
      "Epoch 1 - D Loss: 0.38535916805267334 G Loss: 0.1737922728061676\n",
      "Epoch 1 - D Loss: 0.47319602966308594 G Loss: 0.33871519565582275\n",
      "Epoch 1 - D Loss: 0.6066093444824219 G Loss: 0.18604803085327148\n",
      "Epoch 1 - D Loss: 0.5305759906768799 G Loss: 0.26933759450912476\n",
      "Epoch 1 - D Loss: 0.30769702792167664 G Loss: 0.1999233067035675\n",
      "Epoch 2 - D Loss: 0.5245063900947571 G Loss: 0.12307114899158478\n",
      "Epoch 2 - D Loss: 0.28261351585388184 G Loss: 0.1627068966627121\n",
      "Epoch 2 - D Loss: 0.4581049084663391 G Loss: 0.2298508584499359\n",
      "Epoch 2 - D Loss: 0.4343887269496918 G Loss: 0.3365444839000702\n",
      "Epoch 2 - D Loss: 0.24237525463104248 G Loss: 0.19617050886154175\n",
      "Epoch 2 - D Loss: 0.30087608098983765 G Loss: 0.12515732645988464\n",
      "Epoch 2 - D Loss: 0.48855698108673096 G Loss: 0.27553969621658325\n",
      "Epoch 2 - D Loss: 0.3331783711910248 G Loss: 0.2546764016151428\n",
      "Epoch 2 - D Loss: 0.4125460088253021 G Loss: 0.12278319895267487\n",
      "Epoch 2 - D Loss: 0.40572190284729004 G Loss: 0.20721754431724548\n",
      "Epoch 2 - D Loss: 0.4493645131587982 G Loss: 0.18182680010795593\n",
      "Epoch 2 - D Loss: 0.564715564250946 G Loss: 0.18760590255260468\n",
      "Epoch 2 - D Loss: 0.4782155454158783 G Loss: 0.19181755185127258\n",
      "Epoch 2 - D Loss: 0.37750181555747986 G Loss: 0.1556575745344162\n",
      "Epoch 2 - D Loss: 0.5048757791519165 G Loss: 0.17033588886260986\n",
      "Epoch 2 - D Loss: 0.5892070531845093 G Loss: 0.2610301077365875\n",
      "Epoch 2 - D Loss: 0.44724273681640625 G Loss: 0.2726823091506958\n",
      "Epoch 2 - D Loss: 0.4235101342201233 G Loss: 0.1695886254310608\n",
      "Epoch 2 - D Loss: 0.4071267247200012 G Loss: 0.23525398969650269\n",
      "Epoch 2 - D Loss: 0.3974933922290802 G Loss: 0.29716920852661133\n",
      "Epoch 2 - D Loss: 0.3292766213417053 G Loss: 0.17232055962085724\n",
      "Epoch 2 - D Loss: 0.35129737854003906 G Loss: 0.26580190658569336\n",
      "Epoch 2 - D Loss: 0.26684945821762085 G Loss: 0.16388294100761414\n",
      "Epoch 2 - D Loss: 0.5352258682250977 G Loss: 0.18422096967697144\n",
      "Epoch 2 - D Loss: 0.30691206455230713 G Loss: 0.2133994996547699\n",
      "Epoch 3 - D Loss: 0.570422351360321 G Loss: 0.18060675263404846\n",
      "Epoch 3 - D Loss: 0.4086763858795166 G Loss: 0.09003293514251709\n",
      "Epoch 3 - D Loss: 0.39489784836769104 G Loss: 0.22787997126579285\n",
      "Epoch 3 - D Loss: 0.2560619115829468 G Loss: 0.1702072024345398\n",
      "Epoch 3 - D Loss: 0.3419331908226013 G Loss: 0.14279010891914368\n",
      "Epoch 3 - D Loss: 0.3833047151565552 G Loss: 0.14861014485359192\n",
      "Epoch 3 - D Loss: 0.41074734926223755 G Loss: 0.3215950131416321\n",
      "Epoch 3 - D Loss: 0.4587293267250061 G Loss: 0.20761826634407043\n",
      "Epoch 3 - D Loss: 0.5811933279037476 G Loss: 0.133880153298378\n",
      "Epoch 3 - D Loss: 0.5047941207885742 G Loss: 0.11690923571586609\n",
      "Epoch 3 - D Loss: 0.37324053049087524 G Loss: 0.17911633849143982\n",
      "Epoch 3 - D Loss: 0.3490760326385498 G Loss: 0.20335547626018524\n",
      "Epoch 3 - D Loss: 0.37813547253608704 G Loss: 0.058112069964408875\n",
      "Epoch 3 - D Loss: 0.3490273952484131 G Loss: 0.2442675232887268\n",
      "Epoch 3 - D Loss: 0.33128127455711365 G Loss: 0.20405274629592896\n",
      "Epoch 3 - D Loss: 0.4257000684738159 G Loss: 0.33704638481140137\n",
      "Epoch 3 - D Loss: 0.44370269775390625 G Loss: 0.1741735339164734\n",
      "Epoch 3 - D Loss: 0.535891592502594 G Loss: 0.3353005647659302\n",
      "Epoch 3 - D Loss: 0.3792685866355896 G Loss: 0.28742778301239014\n",
      "Epoch 3 - D Loss: 0.31782591342926025 G Loss: 0.06058281660079956\n",
      "Epoch 3 - D Loss: 0.4393541216850281 G Loss: 0.15179941058158875\n",
      "Epoch 3 - D Loss: 0.3520388603210449 G Loss: 0.15252366662025452\n",
      "Epoch 3 - D Loss: 0.24192418158054352 G Loss: 0.21303708851337433\n",
      "Epoch 3 - D Loss: 0.4067559242248535 G Loss: 0.07610927522182465\n",
      "Epoch 3 - D Loss: 0.3621019124984741 G Loss: 0.1953813135623932\n",
      "Epoch 4 - D Loss: 0.4347931444644928 G Loss: 0.2426764965057373\n",
      "Epoch 4 - D Loss: 0.4112544059753418 G Loss: 0.23292019963264465\n",
      "Epoch 4 - D Loss: 0.2945098876953125 G Loss: 0.1637900322675705\n",
      "Epoch 4 - D Loss: 0.2751725912094116 G Loss: 0.08054903149604797\n",
      "Epoch 4 - D Loss: 0.507819652557373 G Loss: 0.17882294952869415\n",
      "Epoch 4 - D Loss: 0.3210437297821045 G Loss: 0.23118333518505096\n",
      "Epoch 4 - D Loss: 0.44746071100234985 G Loss: 0.26793015003204346\n",
      "Epoch 4 - D Loss: 0.41138169169425964 G Loss: 0.22895297408103943\n",
      "Epoch 4 - D Loss: 0.2927112579345703 G Loss: 0.15907327830791473\n",
      "Epoch 4 - D Loss: 0.5167893171310425 G Loss: 0.19373613595962524\n",
      "Epoch 4 - D Loss: 0.4153617024421692 G Loss: 0.28295961022377014\n",
      "Epoch 4 - D Loss: 0.4555305540561676 G Loss: 0.28984418511390686\n",
      "Epoch 4 - D Loss: 0.29316645860671997 G Loss: 0.21632617712020874\n",
      "Epoch 4 - D Loss: 0.3859468996524811 G Loss: 0.12801961600780487\n",
      "Epoch 4 - D Loss: 0.49151891469955444 G Loss: 0.21581077575683594\n",
      "Epoch 4 - D Loss: 0.5181099772453308 G Loss: 0.14686688780784607\n",
      "Epoch 4 - D Loss: 0.5209320783615112 G Loss: 0.10654390603303909\n",
      "Epoch 4 - D Loss: 0.36450669169425964 G Loss: 0.0937243327498436\n",
      "Epoch 4 - D Loss: 0.26591524481773376 G Loss: 0.2128746211528778\n",
      "Epoch 4 - D Loss: 0.30072855949401855 G Loss: 0.2608230412006378\n",
      "Epoch 4 - D Loss: 0.3318641781806946 G Loss: 0.11601199209690094\n",
      "Epoch 4 - D Loss: 0.4554887115955353 G Loss: 0.19404512643814087\n",
      "Epoch 4 - D Loss: 0.5358911752700806 G Loss: 0.2048126757144928\n",
      "Epoch 4 - D Loss: 0.30852195620536804 G Loss: 0.10048284381628036\n",
      "Epoch 4 - D Loss: 0.3353005647659302 G Loss: 0.1483081579208374\n",
      "Epoch 5 - D Loss: 0.3747139275074005 G Loss: 0.22026953101158142\n",
      "Epoch 5 - D Loss: 0.2848966121673584 G Loss: 0.12471407651901245\n",
      "Epoch 5 - D Loss: 0.6111598014831543 G Loss: 0.2353791445493698\n",
      "Epoch 5 - D Loss: 0.47115811705589294 G Loss: 0.2269742339849472\n",
      "Epoch 5 - D Loss: 0.38467836380004883 G Loss: 0.14742787182331085\n",
      "Epoch 5 - D Loss: 0.2726615071296692 G Loss: 0.1984795331954956\n",
      "Epoch 5 - D Loss: 0.42873847484588623 G Loss: 0.2000729888677597\n",
      "Epoch 5 - D Loss: 0.4136693775653839 G Loss: 0.17661745846271515\n",
      "Epoch 5 - D Loss: 0.4132782220840454 G Loss: 0.3187965750694275\n",
      "Epoch 5 - D Loss: 0.282645046710968 G Loss: 0.15768592059612274\n",
      "Epoch 5 - D Loss: 0.3685225546360016 G Loss: 0.2820613980293274\n",
      "Epoch 5 - D Loss: 0.34947746992111206 G Loss: 0.1245308667421341\n",
      "Epoch 5 - D Loss: 0.501113772392273 G Loss: 0.34492042660713196\n",
      "Epoch 5 - D Loss: 0.387202650308609 G Loss: 0.23369938135147095\n",
      "Epoch 5 - D Loss: 0.2480805665254593 G Loss: 0.24246013164520264\n",
      "Epoch 5 - D Loss: 0.4074850082397461 G Loss: 0.18243347108364105\n",
      "Epoch 5 - D Loss: 0.3816714584827423 G Loss: 0.2633531987667084\n",
      "Epoch 5 - D Loss: 0.4027574062347412 G Loss: 0.27005693316459656\n",
      "Epoch 5 - D Loss: 0.4553072452545166 G Loss: 0.22043360769748688\n",
      "Epoch 5 - D Loss: 0.2802032232284546 G Loss: 0.1694422960281372\n",
      "Epoch 5 - D Loss: 0.4393967092037201 G Loss: 0.1922186315059662\n",
      "Epoch 5 - D Loss: 0.33007168769836426 G Loss: 0.3178343176841736\n",
      "Epoch 5 - D Loss: 0.33520206809043884 G Loss: 0.15347008407115936\n",
      "Epoch 5 - D Loss: 0.40924930572509766 G Loss: 0.30967438220977783\n",
      "Epoch 5 - D Loss: 0.4565179646015167 G Loss: 0.2053409367799759\n",
      "Epoch 6 - D Loss: 0.3608509302139282 G Loss: 0.18289700150489807\n",
      "Epoch 6 - D Loss: 0.48478055000305176 G Loss: 0.0949804037809372\n",
      "Epoch 6 - D Loss: 0.5078409314155579 G Loss: 0.20767858624458313\n",
      "Epoch 6 - D Loss: 0.43594154715538025 G Loss: 0.20746666193008423\n",
      "Epoch 6 - D Loss: 0.3956664204597473 G Loss: 0.2379000186920166\n",
      "Epoch 6 - D Loss: 0.38867706060409546 G Loss: 0.29531845450401306\n",
      "Epoch 6 - D Loss: 0.4837961196899414 G Loss: 0.2544848918914795\n",
      "Epoch 6 - D Loss: 0.3710804283618927 G Loss: 0.25334298610687256\n",
      "Epoch 6 - D Loss: 0.41514238715171814 G Loss: 0.2863308787345886\n",
      "Epoch 6 - D Loss: 0.399689257144928 G Loss: 0.11802063137292862\n",
      "Epoch 6 - D Loss: 0.3775255084037781 G Loss: 0.10759812593460083\n",
      "Epoch 6 - D Loss: 0.37914562225341797 G Loss: 0.1520119458436966\n",
      "Epoch 6 - D Loss: 0.3207743763923645 G Loss: 0.22805321216583252\n",
      "Epoch 6 - D Loss: 0.3279530107975006 G Loss: 0.18649128079414368\n",
      "Epoch 6 - D Loss: 0.3942510485649109 G Loss: 0.1111762672662735\n",
      "Epoch 6 - D Loss: 0.5400968194007874 G Loss: 0.2442038208246231\n",
      "Epoch 6 - D Loss: 0.4107271730899811 G Loss: 0.3930455446243286\n",
      "Epoch 6 - D Loss: 0.3619098961353302 G Loss: 0.16104266047477722\n",
      "Epoch 6 - D Loss: 0.3256271481513977 G Loss: 0.20217299461364746\n",
      "Epoch 6 - D Loss: 0.4100329279899597 G Loss: 0.21385809779167175\n",
      "Epoch 6 - D Loss: 0.5095113515853882 G Loss: 0.27347052097320557\n",
      "Epoch 6 - D Loss: 0.4719691276550293 G Loss: 0.20009103417396545\n",
      "Epoch 6 - D Loss: 0.505768895149231 G Loss: 0.21001362800598145\n",
      "Epoch 6 - D Loss: 0.2670205533504486 G Loss: 0.27019160985946655\n",
      "Epoch 6 - D Loss: 0.555354654788971 G Loss: 0.20938916504383087\n",
      "Epoch 7 - D Loss: 0.3212694525718689 G Loss: 0.2038314938545227\n",
      "Epoch 7 - D Loss: 0.35212433338165283 G Loss: 0.07260780036449432\n",
      "Epoch 7 - D Loss: 0.47727254033088684 G Loss: 0.24730215966701508\n",
      "Epoch 7 - D Loss: 0.39950454235076904 G Loss: 0.10305922478437424\n",
      "Epoch 7 - D Loss: 0.5156716704368591 G Loss: 0.2852824926376343\n",
      "Epoch 7 - D Loss: 0.4807339310646057 G Loss: 0.21011020243167877\n",
      "Epoch 7 - D Loss: 0.43256938457489014 G Loss: 0.2673238515853882\n",
      "Epoch 7 - D Loss: 0.2510022521018982 G Loss: 0.1016034185886383\n",
      "Epoch 7 - D Loss: 0.42550528049468994 G Loss: 0.19294440746307373\n",
      "Epoch 7 - D Loss: 0.41297394037246704 G Loss: 0.27227309346199036\n",
      "Epoch 7 - D Loss: 0.33184897899627686 G Loss: 0.14953316748142242\n",
      "Epoch 7 - D Loss: 0.31661349534988403 G Loss: 0.23764334619045258\n",
      "Epoch 7 - D Loss: 0.42394140362739563 G Loss: 0.1134035587310791\n",
      "Epoch 7 - D Loss: 0.42948806285858154 G Loss: 0.11016246676445007\n",
      "Epoch 7 - D Loss: 0.2601540684700012 G Loss: 0.20759743452072144\n",
      "Epoch 7 - D Loss: 0.304931104183197 G Loss: 0.23419541120529175\n",
      "Epoch 7 - D Loss: 0.37601011991500854 G Loss: 0.14471405744552612\n",
      "Epoch 7 - D Loss: 0.5581341981887817 G Loss: 0.20599469542503357\n",
      "Epoch 7 - D Loss: 0.44767600297927856 G Loss: 0.20274394750595093\n",
      "Epoch 7 - D Loss: 0.4043511748313904 G Loss: 0.1690261960029602\n",
      "Epoch 7 - D Loss: 0.428679496049881 G Loss: 0.17001204192638397\n",
      "Epoch 7 - D Loss: 0.29817378520965576 G Loss: 0.15323755145072937\n",
      "Epoch 7 - D Loss: 0.3823891580104828 G Loss: 0.2733609676361084\n",
      "Epoch 7 - D Loss: 0.475368857383728 G Loss: 0.26689034700393677\n",
      "Epoch 7 - D Loss: 0.5091781616210938 G Loss: 0.24535509943962097\n",
      "Epoch 8 - D Loss: 0.2738436460494995 G Loss: 0.23469801247119904\n",
      "Epoch 8 - D Loss: 0.39443448185920715 G Loss: 0.20607532560825348\n",
      "Epoch 8 - D Loss: 0.5504190921783447 G Loss: 0.2739214301109314\n",
      "Epoch 8 - D Loss: 0.45721951127052307 G Loss: 0.2887304127216339\n",
      "Epoch 8 - D Loss: 0.4640260934829712 G Loss: 0.26561376452445984\n",
      "Epoch 8 - D Loss: 0.3769865036010742 G Loss: 0.26929134130477905\n",
      "Epoch 8 - D Loss: 0.4225679636001587 G Loss: 0.19621586799621582\n",
      "Epoch 8 - D Loss: 0.4210070073604584 G Loss: 0.246235653758049\n",
      "Epoch 8 - D Loss: 0.6111582517623901 G Loss: 0.29003793001174927\n",
      "Epoch 8 - D Loss: 0.3663567006587982 G Loss: 0.22120827436447144\n",
      "Epoch 8 - D Loss: 0.4579697549343109 G Loss: 0.21948203444480896\n",
      "Epoch 8 - D Loss: 0.2964767813682556 G Loss: 0.22806285321712494\n",
      "Epoch 8 - D Loss: 0.4855520725250244 G Loss: 0.2608241140842438\n",
      "Epoch 8 - D Loss: 0.5557531118392944 G Loss: 0.2506641745567322\n",
      "Epoch 8 - D Loss: 0.36790579557418823 G Loss: 0.277981698513031\n",
      "Epoch 8 - D Loss: 0.48211386799812317 G Loss: 0.21352216601371765\n",
      "Epoch 8 - D Loss: 0.47236427664756775 G Loss: 0.1217874139547348\n",
      "Epoch 8 - D Loss: 0.43297886848449707 G Loss: 0.19432228803634644\n",
      "Epoch 8 - D Loss: 0.47350066900253296 G Loss: 0.13303178548812866\n",
      "Epoch 8 - D Loss: 0.33947402238845825 G Loss: 0.12274767458438873\n",
      "Epoch 8 - D Loss: 0.48515957593917847 G Loss: 0.12005019932985306\n",
      "Epoch 8 - D Loss: 0.30411937832832336 G Loss: 0.26128247380256653\n",
      "Epoch 8 - D Loss: 0.4451896846294403 G Loss: 0.22972877323627472\n",
      "Epoch 8 - D Loss: 0.5004629492759705 G Loss: 0.09795960038900375\n",
      "Epoch 8 - D Loss: 0.40186819434165955 G Loss: 0.16688162088394165\n",
      "Epoch 9 - D Loss: 0.5406221151351929 G Loss: 0.259691059589386\n",
      "Epoch 9 - D Loss: 0.45652860403060913 G Loss: 0.3019883930683136\n",
      "Epoch 9 - D Loss: 0.4018104672431946 G Loss: 0.2657473683357239\n",
      "Epoch 9 - D Loss: 0.3549070358276367 G Loss: 0.27323436737060547\n",
      "Epoch 9 - D Loss: 0.45995062589645386 G Loss: 0.22751590609550476\n",
      "Epoch 9 - D Loss: 0.43548405170440674 G Loss: 0.1990756392478943\n",
      "Epoch 9 - D Loss: 0.2445424199104309 G Loss: 0.13193081319332123\n",
      "Epoch 9 - D Loss: 0.3722415566444397 G Loss: 0.19838939607143402\n",
      "Epoch 9 - D Loss: 0.3236108422279358 G Loss: 0.13897548615932465\n",
      "Epoch 9 - D Loss: 0.39625316858291626 G Loss: 0.2903002202510834\n",
      "Epoch 9 - D Loss: 0.2739778161048889 G Loss: 0.2629958689212799\n",
      "Epoch 9 - D Loss: 0.3233661651611328 G Loss: 0.2041570246219635\n",
      "Epoch 9 - D Loss: 0.5634713172912598 G Loss: 0.09182972460985184\n",
      "Epoch 9 - D Loss: 0.4469631314277649 G Loss: 0.13170745968818665\n",
      "Epoch 9 - D Loss: 0.36077171564102173 G Loss: 0.27855032682418823\n",
      "Epoch 9 - D Loss: 0.5192707777023315 G Loss: 0.1299438178539276\n",
      "Epoch 9 - D Loss: 0.4630662500858307 G Loss: 0.25783461332321167\n",
      "Epoch 9 - D Loss: 0.37594032287597656 G Loss: 0.12307781726121902\n",
      "Epoch 9 - D Loss: 0.3280394375324249 G Loss: 0.2548946142196655\n",
      "Epoch 9 - D Loss: 0.3712974786758423 G Loss: 0.16323792934417725\n",
      "Epoch 9 - D Loss: 0.4237925112247467 G Loss: 0.2198939025402069\n",
      "Epoch 9 - D Loss: 0.4523393213748932 G Loss: 0.14986518025398254\n",
      "Epoch 9 - D Loss: 0.3063748776912689 G Loss: 0.14733776450157166\n",
      "Epoch 9 - D Loss: 0.16046497225761414 G Loss: 0.2616935968399048\n",
      "Epoch 9 - D Loss: 0.2827273905277252 G Loss: 0.3161470293998718\n",
      "Epoch 10 - D Loss: 0.35539335012435913 G Loss: 0.11486198008060455\n",
      "Epoch 10 - D Loss: 0.4263191819190979 G Loss: 0.21403738856315613\n",
      "Epoch 10 - D Loss: 0.5227515697479248 G Loss: 0.2276076078414917\n",
      "Epoch 10 - D Loss: 0.4284549355506897 G Loss: 0.1699886918067932\n",
      "Epoch 10 - D Loss: 0.5184924602508545 G Loss: 0.08353763073682785\n",
      "Epoch 10 - D Loss: 0.26561424136161804 G Loss: 0.09573933482170105\n",
      "Epoch 10 - D Loss: 0.26778021454811096 G Loss: 0.26603347063064575\n",
      "Epoch 10 - D Loss: 0.3696690499782562 G Loss: 0.06722158193588257\n",
      "Epoch 10 - D Loss: 0.3056715726852417 G Loss: 0.163781076669693\n",
      "Epoch 10 - D Loss: 0.2620641589164734 G Loss: 0.16945621371269226\n",
      "Epoch 10 - D Loss: 0.3902354836463928 G Loss: 0.24143226444721222\n",
      "Epoch 10 - D Loss: 0.5016129016876221 G Loss: 0.14021635055541992\n",
      "Epoch 10 - D Loss: 0.3169846534729004 G Loss: 0.30556851625442505\n",
      "Epoch 10 - D Loss: 0.41610807180404663 G Loss: 0.23143088817596436\n",
      "Epoch 10 - D Loss: 0.4190114438533783 G Loss: 0.1597474068403244\n",
      "Epoch 10 - D Loss: 0.4171704053878784 G Loss: 0.14937280118465424\n",
      "Epoch 10 - D Loss: 0.30844563245773315 G Loss: 0.17955058813095093\n",
      "Epoch 10 - D Loss: 0.5007903575897217 G Loss: 0.13233821094036102\n",
      "Epoch 10 - D Loss: 0.43627533316612244 G Loss: 0.16923832893371582\n",
      "Epoch 10 - D Loss: 0.3449634909629822 G Loss: 0.17708732187747955\n",
      "Epoch 10 - D Loss: 0.45720887184143066 G Loss: 0.3477787375450134\n",
      "Epoch 10 - D Loss: 0.4462866187095642 G Loss: 0.1698412001132965\n",
      "Epoch 10 - D Loss: 0.28120383620262146 G Loss: 0.23324322700500488\n",
      "Epoch 10 - D Loss: 0.3234214186668396 G Loss: 0.30296728014945984\n",
      "Epoch 10 - D Loss: 0.5796375274658203 G Loss: 0.19377994537353516\n",
      "Epoch 11 - D Loss: 0.5501066446304321 G Loss: 0.30103737115859985\n",
      "Epoch 11 - D Loss: 0.45110228657722473 G Loss: 0.23694844543933868\n",
      "Epoch 11 - D Loss: 0.32064151763916016 G Loss: 0.21142008900642395\n",
      "Epoch 11 - D Loss: 0.5016219615936279 G Loss: 0.23429907858371735\n",
      "Epoch 11 - D Loss: 0.3454548716545105 G Loss: 0.1957780122756958\n",
      "Epoch 11 - D Loss: 0.37380483746528625 G Loss: 0.32603901624679565\n",
      "Epoch 11 - D Loss: 0.5042587518692017 G Loss: 0.2979907989501953\n",
      "Epoch 11 - D Loss: 0.45842695236206055 G Loss: 0.25745755434036255\n",
      "Epoch 11 - D Loss: 0.41851770877838135 G Loss: 0.08480975776910782\n",
      "Epoch 11 - D Loss: 0.4125852584838867 G Loss: 0.28859516978263855\n",
      "Epoch 11 - D Loss: 0.37993305921554565 G Loss: 0.16590631008148193\n",
      "Epoch 11 - D Loss: 0.3024192750453949 G Loss: 0.31922149658203125\n",
      "Epoch 11 - D Loss: 0.3310951292514801 G Loss: 0.13711580634117126\n",
      "Epoch 11 - D Loss: 0.3037671744823456 G Loss: 0.09282331168651581\n",
      "Epoch 11 - D Loss: 0.33266499638557434 G Loss: 0.11284978687763214\n",
      "Epoch 11 - D Loss: 0.520920991897583 G Loss: 0.2573167681694031\n",
      "Epoch 11 - D Loss: 0.2249092012643814 G Loss: 0.24239733815193176\n",
      "Epoch 11 - D Loss: 0.3560474216938019 G Loss: 0.14233389496803284\n",
      "Epoch 11 - D Loss: 0.4788858890533447 G Loss: 0.17256569862365723\n",
      "Epoch 11 - D Loss: 0.39439624547958374 G Loss: 0.23169571161270142\n",
      "Epoch 11 - D Loss: 0.3429388999938965 G Loss: 0.2658243477344513\n",
      "Epoch 11 - D Loss: 0.2626226544380188 G Loss: 0.209238663315773\n",
      "Epoch 11 - D Loss: 0.4568980634212494 G Loss: 0.25840362906455994\n",
      "Epoch 11 - D Loss: 0.43128031492233276 G Loss: 0.19355785846710205\n",
      "Epoch 11 - D Loss: 0.47111690044403076 G Loss: 0.17352238297462463\n",
      "Epoch 12 - D Loss: 0.3108983039855957 G Loss: 0.12287189066410065\n",
      "Epoch 12 - D Loss: 0.3841252326965332 G Loss: 0.13215939700603485\n",
      "Epoch 12 - D Loss: 0.3995209336280823 G Loss: 0.3114811182022095\n",
      "Epoch 12 - D Loss: 0.45252057909965515 G Loss: 0.17332378029823303\n",
      "Epoch 12 - D Loss: 0.3830827474594116 G Loss: 0.2139703333377838\n",
      "Epoch 12 - D Loss: 0.49700987339019775 G Loss: 0.19424903392791748\n",
      "Epoch 12 - D Loss: 0.3639807105064392 G Loss: 0.1930869221687317\n",
      "Epoch 12 - D Loss: 0.36907050013542175 G Loss: 0.17400385439395905\n",
      "Epoch 12 - D Loss: 0.3771952986717224 G Loss: 0.267361581325531\n",
      "Epoch 12 - D Loss: 0.29898566007614136 G Loss: 0.15522737801074982\n",
      "Epoch 12 - D Loss: 0.27251267433166504 G Loss: 0.12889191508293152\n",
      "Epoch 12 - D Loss: 0.3711210787296295 G Loss: 0.19382503628730774\n",
      "Epoch 12 - D Loss: 0.5065012574195862 G Loss: 0.17271992564201355\n",
      "Epoch 12 - D Loss: 0.41561001539230347 G Loss: 0.19590705633163452\n",
      "Epoch 12 - D Loss: 0.19207584857940674 G Loss: 0.18094947934150696\n",
      "Epoch 12 - D Loss: 0.424990713596344 G Loss: 0.24964267015457153\n",
      "Epoch 12 - D Loss: 0.4747875928878784 G Loss: 0.11490251868963242\n",
      "Epoch 12 - D Loss: 0.5626820921897888 G Loss: 0.3036044239997864\n",
      "Epoch 12 - D Loss: 0.49729156494140625 G Loss: 0.18043619394302368\n",
      "Epoch 12 - D Loss: 0.4462476372718811 G Loss: 0.18309465050697327\n",
      "Epoch 12 - D Loss: 0.3714742064476013 G Loss: 0.2608456611633301\n",
      "Epoch 12 - D Loss: 0.41791754961013794 G Loss: 0.12991303205490112\n",
      "Epoch 12 - D Loss: 0.4250001311302185 G Loss: 0.12843874096870422\n",
      "Epoch 12 - D Loss: 0.3059714436531067 G Loss: 0.035749517381191254\n",
      "Epoch 12 - D Loss: 0.26156342029571533 G Loss: 0.2003009021282196\n",
      "Epoch 13 - D Loss: 0.2241668701171875 G Loss: 0.1774110496044159\n",
      "Epoch 13 - D Loss: 0.5785434246063232 G Loss: 0.15689703822135925\n",
      "Epoch 13 - D Loss: 0.41742146015167236 G Loss: 0.19408540427684784\n",
      "Epoch 13 - D Loss: 0.46738529205322266 G Loss: 0.2332722395658493\n",
      "Epoch 13 - D Loss: 0.37295275926589966 G Loss: 0.24679309129714966\n",
      "Epoch 13 - D Loss: 0.34537506103515625 G Loss: 0.19656789302825928\n",
      "Epoch 13 - D Loss: 0.3586782217025757 G Loss: 0.2529357671737671\n",
      "Epoch 13 - D Loss: 0.4030001163482666 G Loss: 0.21086779236793518\n",
      "Epoch 13 - D Loss: 0.4363129436969757 G Loss: 0.23960328102111816\n",
      "Epoch 13 - D Loss: 0.2854911983013153 G Loss: 0.20485877990722656\n",
      "Epoch 13 - D Loss: 0.3629940152168274 G Loss: 0.20259714126586914\n",
      "Epoch 13 - D Loss: 0.23734205961227417 G Loss: 0.25602975487709045\n",
      "Epoch 13 - D Loss: 0.40801459550857544 G Loss: 0.1635507345199585\n",
      "Epoch 13 - D Loss: 0.28890520334243774 G Loss: 0.16080832481384277\n",
      "Epoch 13 - D Loss: 0.16367121040821075 G Loss: 0.22955940663814545\n",
      "Epoch 13 - D Loss: 0.3710286617279053 G Loss: 0.29219895601272583\n",
      "Epoch 13 - D Loss: 0.503298282623291 G Loss: 0.1912027895450592\n",
      "Epoch 13 - D Loss: 0.4404142200946808 G Loss: 0.1657770872116089\n",
      "Epoch 13 - D Loss: 0.4570128917694092 G Loss: 0.32818809151649475\n",
      "Epoch 13 - D Loss: 0.43618637323379517 G Loss: 0.24038559198379517\n",
      "Epoch 13 - D Loss: 0.44487059116363525 G Loss: 0.2514258325099945\n",
      "Epoch 13 - D Loss: 0.4822876453399658 G Loss: 0.2854812741279602\n",
      "Epoch 13 - D Loss: 0.45351046323776245 G Loss: 0.1423102170228958\n",
      "Epoch 13 - D Loss: 0.3434596657752991 G Loss: 0.20376908779144287\n",
      "Epoch 13 - D Loss: 0.432456910610199 G Loss: 0.15951964259147644\n",
      "Epoch 14 - D Loss: 0.3912736773490906 G Loss: 0.20387428998947144\n",
      "Epoch 14 - D Loss: 0.35347384214401245 G Loss: 0.3321210741996765\n",
      "Epoch 14 - D Loss: 0.3735749423503876 G Loss: 0.26060807704925537\n",
      "Epoch 14 - D Loss: 0.45733028650283813 G Loss: 0.0935148298740387\n",
      "Epoch 14 - D Loss: 0.41218048334121704 G Loss: 0.08986862748861313\n",
      "Epoch 14 - D Loss: 0.35223811864852905 G Loss: 0.20730909705162048\n",
      "Epoch 14 - D Loss: 0.519660234451294 G Loss: 0.1499120146036148\n",
      "Epoch 14 - D Loss: 0.44202011823654175 G Loss: 0.2187243551015854\n",
      "Epoch 14 - D Loss: 0.5885950922966003 G Loss: 0.3116505742073059\n",
      "Epoch 14 - D Loss: 0.5153934955596924 G Loss: 0.05112080276012421\n",
      "Epoch 14 - D Loss: 0.333250492811203 G Loss: 0.2941049337387085\n",
      "Epoch 14 - D Loss: 0.5130730271339417 G Loss: 0.18749728798866272\n",
      "Epoch 14 - D Loss: 0.36212801933288574 G Loss: 0.1432333141565323\n",
      "Epoch 14 - D Loss: 0.5025229454040527 G Loss: 0.3432307243347168\n",
      "Epoch 14 - D Loss: 0.445794939994812 G Loss: 0.1546313464641571\n",
      "Epoch 14 - D Loss: 0.6160170435905457 G Loss: 0.2677001357078552\n",
      "Epoch 14 - D Loss: 0.22696800529956818 G Loss: 0.2738778293132782\n",
      "Epoch 14 - D Loss: 0.4056892991065979 G Loss: 0.21535393595695496\n",
      "Epoch 14 - D Loss: 0.3382152318954468 G Loss: 0.2732534408569336\n",
      "Epoch 14 - D Loss: 0.3058377206325531 G Loss: 0.2743012011051178\n",
      "Epoch 14 - D Loss: 0.4503045380115509 G Loss: 0.2261013686656952\n",
      "Epoch 14 - D Loss: 0.34626877307891846 G Loss: 0.11188295483589172\n",
      "Epoch 14 - D Loss: 0.49188393354415894 G Loss: 0.09237661957740784\n",
      "Epoch 14 - D Loss: 0.4996286928653717 G Loss: 0.15592285990715027\n",
      "Epoch 14 - D Loss: 0.36403489112854004 G Loss: 0.14326098561286926\n",
      "Epoch 15 - D Loss: 0.4195927679538727 G Loss: 0.3048465847969055\n",
      "Epoch 15 - D Loss: 0.5408800840377808 G Loss: 0.24820476770401\n",
      "Epoch 15 - D Loss: 0.35156339406967163 G Loss: 0.3045651912689209\n",
      "Epoch 15 - D Loss: 0.4609296917915344 G Loss: 0.24419432878494263\n",
      "Epoch 15 - D Loss: 0.3240620493888855 G Loss: 0.23336085677146912\n",
      "Epoch 15 - D Loss: 0.4353597164154053 G Loss: 0.1632554680109024\n",
      "Epoch 15 - D Loss: 0.4856409728527069 G Loss: 0.2047407478094101\n",
      "Epoch 15 - D Loss: 0.4788837432861328 G Loss: 0.23061636090278625\n",
      "Epoch 15 - D Loss: 0.2606283724308014 G Loss: 0.26235514879226685\n",
      "Epoch 15 - D Loss: 0.31102585792541504 G Loss: 0.254403680562973\n",
      "Epoch 15 - D Loss: 0.3839484453201294 G Loss: 0.2051393687725067\n",
      "Epoch 15 - D Loss: 0.37056171894073486 G Loss: 0.10714425146579742\n",
      "Epoch 15 - D Loss: 0.24385520815849304 G Loss: 0.16837841272354126\n",
      "Epoch 15 - D Loss: 0.5140684247016907 G Loss: 0.16292902827262878\n",
      "Epoch 15 - D Loss: 0.4896392226219177 G Loss: 0.18474897742271423\n",
      "Epoch 15 - D Loss: 0.4435712397098541 G Loss: 0.18390168249607086\n",
      "Epoch 15 - D Loss: 0.3588445782661438 G Loss: 0.23933328688144684\n",
      "Epoch 15 - D Loss: 0.2692021429538727 G Loss: 0.20368660986423492\n",
      "Epoch 15 - D Loss: 0.245691180229187 G Loss: 0.24243971705436707\n",
      "Epoch 15 - D Loss: 0.34825336933135986 G Loss: 0.19600506126880646\n",
      "Epoch 15 - D Loss: 0.47956961393356323 G Loss: 0.22720906138420105\n",
      "Epoch 15 - D Loss: 0.4517807364463806 G Loss: 0.16138321161270142\n",
      "Epoch 15 - D Loss: 0.472378671169281 G Loss: 0.11004926264286041\n",
      "Epoch 15 - D Loss: 0.41210371255874634 G Loss: 0.2785714864730835\n",
      "Epoch 15 - D Loss: 0.39956358075141907 G Loss: 0.30310821533203125\n",
      "Epoch 16 - D Loss: 0.368068128824234 G Loss: 0.22338417172431946\n",
      "Epoch 16 - D Loss: 0.3389345705509186 G Loss: 0.28266599774360657\n",
      "Epoch 16 - D Loss: 0.38605910539627075 G Loss: 0.24534401297569275\n",
      "Epoch 16 - D Loss: 0.45807191729545593 G Loss: 0.17350545525550842\n",
      "Epoch 16 - D Loss: 0.3129149377346039 G Loss: 0.22209598124027252\n",
      "Epoch 16 - D Loss: 0.2801891565322876 G Loss: 0.0788567066192627\n",
      "Epoch 16 - D Loss: 0.29855144023895264 G Loss: 0.13305805623531342\n",
      "Epoch 16 - D Loss: 0.36628854274749756 G Loss: 0.14919082820415497\n",
      "Epoch 16 - D Loss: 0.3126811981201172 G Loss: 0.2311457246541977\n",
      "Epoch 16 - D Loss: 0.44597262144088745 G Loss: 0.23462346196174622\n",
      "Epoch 16 - D Loss: 0.44842779636383057 G Loss: 0.22951465845108032\n",
      "Epoch 16 - D Loss: 0.24780504405498505 G Loss: 0.19105009734630585\n",
      "Epoch 16 - D Loss: 0.3158320188522339 G Loss: 0.10657277703285217\n",
      "Epoch 16 - D Loss: 0.39319419860839844 G Loss: 0.12956151366233826\n",
      "Epoch 16 - D Loss: 0.3507474958896637 G Loss: 0.23596124351024628\n",
      "Epoch 16 - D Loss: 0.5151327848434448 G Loss: 0.10208740830421448\n",
      "Epoch 16 - D Loss: 0.5311779975891113 G Loss: 0.14910537004470825\n",
      "Epoch 16 - D Loss: 0.4031968414783478 G Loss: 0.07289066165685654\n",
      "Epoch 16 - D Loss: 0.289476603269577 G Loss: 0.18345072865486145\n",
      "Epoch 16 - D Loss: 0.35638919472694397 G Loss: 0.1905614733695984\n",
      "Epoch 16 - D Loss: 0.4461303949356079 G Loss: 0.21185258030891418\n",
      "Epoch 16 - D Loss: 0.35174521803855896 G Loss: 0.16551989316940308\n",
      "Epoch 16 - D Loss: 0.3756076395511627 G Loss: 0.26438412070274353\n",
      "Epoch 16 - D Loss: 0.4376760423183441 G Loss: 0.07470706105232239\n",
      "Epoch 16 - D Loss: 0.3454270660877228 G Loss: 0.207007497549057\n",
      "Epoch 17 - D Loss: 0.5459833741188049 G Loss: 0.21991756558418274\n",
      "Epoch 17 - D Loss: 0.4357016384601593 G Loss: 0.2341325879096985\n",
      "Epoch 17 - D Loss: 0.4713170528411865 G Loss: 0.2884407341480255\n",
      "Epoch 17 - D Loss: 0.48624759912490845 G Loss: 0.16522487998008728\n",
      "Epoch 17 - D Loss: 0.5441411733627319 G Loss: 0.29613929986953735\n",
      "Epoch 17 - D Loss: 0.4264325499534607 G Loss: 0.2544081509113312\n",
      "Epoch 17 - D Loss: 0.510179877281189 G Loss: 0.11971856653690338\n",
      "Epoch 17 - D Loss: 0.27192965149879456 G Loss: 0.31502681970596313\n",
      "Epoch 17 - D Loss: 0.44222143292427063 G Loss: 0.22631466388702393\n",
      "Epoch 17 - D Loss: 0.4198493957519531 G Loss: 0.07840532064437866\n",
      "Epoch 17 - D Loss: 0.30613863468170166 G Loss: 0.32999682426452637\n",
      "Epoch 17 - D Loss: 0.3691609501838684 G Loss: 0.24813157320022583\n",
      "Epoch 17 - D Loss: 0.3660081923007965 G Loss: 0.286053866147995\n",
      "Epoch 17 - D Loss: 0.28874170780181885 G Loss: 0.3168630003929138\n",
      "Epoch 17 - D Loss: 0.5577229857444763 G Loss: 0.13237671554088593\n",
      "Epoch 17 - D Loss: 0.33744895458221436 G Loss: 0.3107128441333771\n",
      "Epoch 17 - D Loss: 0.5653658509254456 G Loss: 0.25130659341812134\n",
      "Epoch 17 - D Loss: 0.4539099335670471 G Loss: 0.23479849100112915\n",
      "Epoch 17 - D Loss: 0.4742775857448578 G Loss: 0.14331257343292236\n",
      "Epoch 17 - D Loss: 0.28472280502319336 G Loss: 0.24818579852581024\n",
      "Epoch 17 - D Loss: 0.42165058851242065 G Loss: 0.17202264070510864\n",
      "Epoch 17 - D Loss: 0.4262174367904663 G Loss: 0.2576044201850891\n",
      "Epoch 17 - D Loss: 0.4173707365989685 G Loss: 0.3086165487766266\n",
      "Epoch 17 - D Loss: 0.4462706446647644 G Loss: 0.21667058765888214\n",
      "Epoch 17 - D Loss: 0.386934757232666 G Loss: 0.2874416708946228\n",
      "Epoch 18 - D Loss: 0.5106975436210632 G Loss: 0.23204512894153595\n",
      "Epoch 18 - D Loss: 0.4385029375553131 G Loss: 0.27422034740448\n",
      "Epoch 18 - D Loss: 0.37766537070274353 G Loss: 0.07442125678062439\n",
      "Epoch 18 - D Loss: 0.4625769853591919 G Loss: 0.10193246603012085\n",
      "Epoch 18 - D Loss: 0.4451366662979126 G Loss: 0.17661744356155396\n",
      "Epoch 18 - D Loss: 0.4021199941635132 G Loss: 0.15151084959506989\n",
      "Epoch 18 - D Loss: 0.4888237714767456 G Loss: 0.2145969718694687\n",
      "Epoch 18 - D Loss: 0.49286437034606934 G Loss: 0.2734004259109497\n",
      "Epoch 18 - D Loss: 0.34489431977272034 G Loss: 0.08706881105899811\n",
      "Epoch 18 - D Loss: 0.22471944987773895 G Loss: 0.17715699970722198\n",
      "Epoch 18 - D Loss: 0.37242990732192993 G Loss: 0.16483324766159058\n",
      "Epoch 18 - D Loss: 0.5755011439323425 G Loss: 0.14544257521629333\n",
      "Epoch 18 - D Loss: 0.42476922273635864 G Loss: 0.18552610278129578\n",
      "Epoch 18 - D Loss: 0.4769034683704376 G Loss: 0.17867064476013184\n",
      "Epoch 18 - D Loss: 0.36587080359458923 G Loss: 0.15973994135856628\n",
      "Epoch 18 - D Loss: 0.2682092487812042 G Loss: 0.14552071690559387\n",
      "Epoch 18 - D Loss: 0.39624398946762085 G Loss: 0.10993857681751251\n",
      "Epoch 18 - D Loss: 0.5703343152999878 G Loss: 0.20595385134220123\n",
      "Epoch 18 - D Loss: 0.6119523048400879 G Loss: 0.3005761206150055\n",
      "Epoch 18 - D Loss: 0.47473305463790894 G Loss: 0.14368200302124023\n",
      "Epoch 18 - D Loss: 0.4097457528114319 G Loss: 0.09398259222507477\n",
      "Epoch 18 - D Loss: 0.44030848145484924 G Loss: 0.24822524189949036\n",
      "Epoch 18 - D Loss: 0.5624369382858276 G Loss: 0.10141320526599884\n",
      "Epoch 18 - D Loss: 0.34653741121292114 G Loss: 0.3355816900730133\n",
      "Epoch 18 - D Loss: 0.1945093721151352 G Loss: 0.17099088430404663\n",
      "Epoch 19 - D Loss: 0.2599751353263855 G Loss: 0.2125396430492401\n",
      "Epoch 19 - D Loss: 0.44075971841812134 G Loss: 0.12835252285003662\n",
      "Epoch 19 - D Loss: 0.29733940958976746 G Loss: 0.2071528434753418\n",
      "Epoch 19 - D Loss: 0.4625205993652344 G Loss: 0.22251704335212708\n",
      "Epoch 19 - D Loss: 0.24574054777622223 G Loss: 0.31565701961517334\n",
      "Epoch 19 - D Loss: 0.28758183121681213 G Loss: 0.24492600560188293\n",
      "Epoch 19 - D Loss: 0.34265634417533875 G Loss: 0.2965705394744873\n",
      "Epoch 19 - D Loss: 0.3452061414718628 G Loss: 0.1355096846818924\n",
      "Epoch 19 - D Loss: 0.45749595761299133 G Loss: 0.17267686128616333\n",
      "Epoch 19 - D Loss: 0.3385775089263916 G Loss: 0.2796691954135895\n",
      "Epoch 19 - D Loss: 0.30880075693130493 G Loss: 0.2742992639541626\n",
      "Epoch 19 - D Loss: 0.4316343069076538 G Loss: 0.23449552059173584\n",
      "Epoch 19 - D Loss: 0.39938879013061523 G Loss: 0.18472163379192352\n",
      "Epoch 19 - D Loss: 0.500145435333252 G Loss: 0.2476407140493393\n",
      "Epoch 19 - D Loss: 0.5783016681671143 G Loss: 0.1802535355091095\n",
      "Epoch 19 - D Loss: 0.36518633365631104 G Loss: 0.21007628738880157\n",
      "Epoch 19 - D Loss: 0.4005569517612457 G Loss: 0.2737160921096802\n",
      "Epoch 19 - D Loss: 0.314876914024353 G Loss: 0.24033582210540771\n",
      "Epoch 19 - D Loss: 0.4409165680408478 G Loss: 0.14636078476905823\n",
      "Epoch 19 - D Loss: 0.5974080562591553 G Loss: 0.25567737221717834\n",
      "Epoch 19 - D Loss: 0.4187825322151184 G Loss: 0.15163710713386536\n",
      "Epoch 19 - D Loss: 0.45586907863616943 G Loss: 0.24997279047966003\n",
      "Epoch 19 - D Loss: 0.3000083863735199 G Loss: 0.26470747590065\n",
      "Epoch 19 - D Loss: 0.3480972647666931 G Loss: 0.2735139727592468\n",
      "Epoch 19 - D Loss: 0.37489041686058044 G Loss: 0.17766501009464264\n",
      "Epoch 20 - D Loss: 0.5521995425224304 G Loss: 0.18481668829917908\n",
      "Epoch 20 - D Loss: 0.3821481168270111 G Loss: 0.21712450683116913\n",
      "Epoch 20 - D Loss: 0.3582731783390045 G Loss: 0.16645321249961853\n",
      "Epoch 20 - D Loss: 0.37178948521614075 G Loss: 0.24855142831802368\n",
      "Epoch 20 - D Loss: 0.3432501554489136 G Loss: 0.21452748775482178\n",
      "Epoch 20 - D Loss: 0.39539793133735657 G Loss: 0.2162909209728241\n",
      "Epoch 20 - D Loss: 0.3793841600418091 G Loss: 0.06793870776891708\n",
      "Epoch 20 - D Loss: 0.4993855655193329 G Loss: 0.10339643061161041\n",
      "Epoch 20 - D Loss: 0.16810984909534454 G Loss: 0.3262121081352234\n",
      "Epoch 20 - D Loss: 0.5569111108779907 G Loss: 0.17334523797035217\n",
      "Epoch 20 - D Loss: 0.36736318469047546 G Loss: 0.17523691058158875\n",
      "Epoch 20 - D Loss: 0.38890108466148376 G Loss: 0.16005338728427887\n",
      "Epoch 20 - D Loss: 0.5128345489501953 G Loss: 0.22302241623401642\n",
      "Epoch 20 - D Loss: 0.36470893025398254 G Loss: 0.1548861265182495\n",
      "Epoch 20 - D Loss: 0.5028167963027954 G Loss: 0.1639566719532013\n",
      "Epoch 20 - D Loss: 0.23617570102214813 G Loss: 0.09755055606365204\n",
      "Epoch 20 - D Loss: 0.40653368830680847 G Loss: 0.12922120094299316\n",
      "Epoch 20 - D Loss: 0.39344000816345215 G Loss: 0.10707591474056244\n",
      "Epoch 20 - D Loss: 0.4341376721858978 G Loss: 0.19808822870254517\n",
      "Epoch 20 - D Loss: 0.4837309420108795 G Loss: 0.12360115349292755\n",
      "Epoch 20 - D Loss: 0.39640629291534424 G Loss: 0.28693336248397827\n",
      "Epoch 20 - D Loss: 0.4280492663383484 G Loss: 0.15607599914073944\n",
      "Epoch 20 - D Loss: 0.5072412490844727 G Loss: 0.2809371054172516\n",
      "Epoch 20 - D Loss: 0.4148294925689697 G Loss: 0.30264270305633545\n",
      "Epoch 20 - D Loss: 0.4875843822956085 G Loss: 0.13302406668663025\n",
      "Epoch 21 - D Loss: 0.29157423973083496 G Loss: 0.29018110036849976\n",
      "Epoch 21 - D Loss: 0.5262060165405273 G Loss: 0.316206693649292\n",
      "Epoch 21 - D Loss: 0.3375507593154907 G Loss: 0.2682358920574188\n",
      "Epoch 21 - D Loss: 0.3696168065071106 G Loss: 0.2112313210964203\n",
      "Epoch 21 - D Loss: 0.35124218463897705 G Loss: 0.20439985394477844\n",
      "Epoch 21 - D Loss: 0.3143637478351593 G Loss: 0.2742289900779724\n",
      "Epoch 21 - D Loss: 0.3760720491409302 G Loss: 0.1789146363735199\n",
      "Epoch 21 - D Loss: 0.5202152729034424 G Loss: 0.2019355297088623\n",
      "Epoch 21 - D Loss: 0.35097581148147583 G Loss: 0.16711872816085815\n",
      "Epoch 21 - D Loss: 0.4766167998313904 G Loss: 0.2775391638278961\n",
      "Epoch 21 - D Loss: 0.17586496472358704 G Loss: 0.15635541081428528\n",
      "Epoch 21 - D Loss: 0.419970840215683 G Loss: 0.1845281720161438\n",
      "Epoch 21 - D Loss: 0.45416146516799927 G Loss: 0.20258894562721252\n",
      "Epoch 21 - D Loss: 0.4804123640060425 G Loss: 0.3068884015083313\n",
      "Epoch 21 - D Loss: 0.4323238730430603 G Loss: 0.22979772090911865\n",
      "Epoch 21 - D Loss: 0.41958898305892944 G Loss: 0.23120874166488647\n",
      "Epoch 21 - D Loss: 0.5145501494407654 G Loss: 0.12665502727031708\n",
      "Epoch 21 - D Loss: 0.5515387654304504 G Loss: 0.22544042766094208\n",
      "Epoch 21 - D Loss: 0.4349265992641449 G Loss: 0.14764830470085144\n",
      "Epoch 21 - D Loss: 0.39184775948524475 G Loss: 0.21485935151576996\n",
      "Epoch 21 - D Loss: 0.31514355540275574 G Loss: 0.0822187140583992\n",
      "Epoch 21 - D Loss: 0.2826533913612366 G Loss: 0.2097652107477188\n",
      "Epoch 21 - D Loss: 0.31796056032180786 G Loss: 0.1614197939634323\n",
      "Epoch 21 - D Loss: 0.31614840030670166 G Loss: 0.3297245502471924\n",
      "Epoch 21 - D Loss: 0.495454877614975 G Loss: 0.1250266432762146\n",
      "Epoch 22 - D Loss: 0.3492785096168518 G Loss: 0.18296965956687927\n",
      "Epoch 22 - D Loss: 0.3255535066127777 G Loss: 0.2802802324295044\n",
      "Epoch 22 - D Loss: 0.4883047342300415 G Loss: 0.24060988426208496\n",
      "Epoch 22 - D Loss: 0.533322811126709 G Loss: 0.16262288391590118\n",
      "Epoch 22 - D Loss: 0.41811636090278625 G Loss: 0.24788369238376617\n",
      "Epoch 22 - D Loss: 0.47063279151916504 G Loss: 0.1984734833240509\n",
      "Epoch 22 - D Loss: 0.34804612398147583 G Loss: 0.18574747443199158\n",
      "Epoch 22 - D Loss: 0.4012081027030945 G Loss: 0.16235414147377014\n",
      "Epoch 22 - D Loss: 0.3668646514415741 G Loss: 0.18660002946853638\n",
      "Epoch 22 - D Loss: 0.4264932870864868 G Loss: 0.19957800209522247\n",
      "Epoch 22 - D Loss: 0.34751713275909424 G Loss: 0.23104870319366455\n",
      "Epoch 22 - D Loss: 0.4897507131099701 G Loss: 0.2961670458316803\n",
      "Epoch 22 - D Loss: 0.34018832445144653 G Loss: 0.10285089910030365\n",
      "Epoch 22 - D Loss: 0.4861501455307007 G Loss: 0.17493219673633575\n",
      "Epoch 22 - D Loss: 0.39350804686546326 G Loss: 0.2774650454521179\n",
      "Epoch 22 - D Loss: 0.4586438536643982 G Loss: 0.24267494678497314\n",
      "Epoch 22 - D Loss: 0.3213273882865906 G Loss: 0.11760148406028748\n",
      "Epoch 22 - D Loss: 0.5089397430419922 G Loss: 0.19112235307693481\n",
      "Epoch 22 - D Loss: 0.49209922552108765 G Loss: 0.2661372423171997\n",
      "Epoch 22 - D Loss: 0.3191852271556854 G Loss: 0.2702653408050537\n",
      "Epoch 22 - D Loss: 0.4495176076889038 G Loss: 0.20306333899497986\n",
      "Epoch 22 - D Loss: 0.4321640431880951 G Loss: 0.1494624763727188\n",
      "Epoch 22 - D Loss: 0.42330658435821533 G Loss: 0.2481520175933838\n",
      "Epoch 22 - D Loss: 0.4182348847389221 G Loss: 0.25755417346954346\n",
      "Epoch 22 - D Loss: 0.4132780134677887 G Loss: 0.22232553362846375\n",
      "Epoch 23 - D Loss: 0.45816296339035034 G Loss: 0.07380098104476929\n",
      "Epoch 23 - D Loss: 0.36599355936050415 G Loss: 0.1923023909330368\n",
      "Epoch 23 - D Loss: 0.4841294288635254 G Loss: 0.23916088044643402\n",
      "Epoch 23 - D Loss: 0.45234981179237366 G Loss: 0.08005961775779724\n",
      "Epoch 23 - D Loss: 0.39918166399002075 G Loss: 0.2034868448972702\n",
      "Epoch 23 - D Loss: 0.424693763256073 G Loss: 0.22843873500823975\n",
      "Epoch 23 - D Loss: 0.3053646981716156 G Loss: 0.17774134874343872\n",
      "Epoch 23 - D Loss: 0.36566805839538574 G Loss: 0.17027154564857483\n",
      "Epoch 23 - D Loss: 0.24829387664794922 G Loss: 0.3234453499317169\n",
      "Epoch 23 - D Loss: 0.3436497449874878 G Loss: 0.17626628279685974\n",
      "Epoch 23 - D Loss: 0.3511655330657959 G Loss: 0.15013061463832855\n",
      "Epoch 23 - D Loss: 0.5294457674026489 G Loss: 0.15804126858711243\n",
      "Epoch 23 - D Loss: 0.4242646396160126 G Loss: 0.2523679733276367\n",
      "Epoch 23 - D Loss: 0.500861406326294 G Loss: 0.22409549355506897\n",
      "Epoch 23 - D Loss: 0.4204466938972473 G Loss: 0.25755465030670166\n",
      "Epoch 23 - D Loss: 0.5846450924873352 G Loss: 0.19878005981445312\n",
      "Epoch 23 - D Loss: 0.5418944954872131 G Loss: 0.24471160769462585\n",
      "Epoch 23 - D Loss: 0.4203855097293854 G Loss: 0.13272258639335632\n",
      "Epoch 23 - D Loss: 0.4786491096019745 G Loss: 0.22224169969558716\n",
      "Epoch 23 - D Loss: 0.4680662155151367 G Loss: 0.09716999530792236\n",
      "Epoch 23 - D Loss: 0.3491251766681671 G Loss: 0.20947760343551636\n",
      "Epoch 23 - D Loss: 0.27806946635246277 G Loss: 0.3126044273376465\n",
      "Epoch 23 - D Loss: 0.4390542507171631 G Loss: 0.28808730840682983\n",
      "Epoch 23 - D Loss: 0.3826518952846527 G Loss: 0.09087121486663818\n",
      "Epoch 23 - D Loss: 0.4698394536972046 G Loss: 0.17881280183792114\n",
      "Epoch 24 - D Loss: 0.477580189704895 G Loss: 0.06275129318237305\n",
      "Epoch 24 - D Loss: 0.45271316170692444 G Loss: 0.2592677175998688\n",
      "Epoch 24 - D Loss: 0.3607695698738098 G Loss: 0.15257318317890167\n",
      "Epoch 24 - D Loss: 0.39432743191719055 G Loss: 0.22861245274543762\n",
      "Epoch 24 - D Loss: 0.448102205991745 G Loss: 0.15199509263038635\n",
      "Epoch 24 - D Loss: 0.4205569922924042 G Loss: 0.2211809903383255\n",
      "Epoch 24 - D Loss: 0.3569347858428955 G Loss: 0.2639504671096802\n",
      "Epoch 24 - D Loss: 0.28637009859085083 G Loss: 0.2461700141429901\n",
      "Epoch 24 - D Loss: 0.38537877798080444 G Loss: 0.21916647255420685\n",
      "Epoch 24 - D Loss: 0.4860283434391022 G Loss: 0.27005478739738464\n",
      "Epoch 24 - D Loss: 0.4162634611129761 G Loss: 0.2466256022453308\n",
      "Epoch 24 - D Loss: 0.2729915976524353 G Loss: 0.17243507504463196\n",
      "Epoch 24 - D Loss: 0.5486205816268921 G Loss: 0.24905307590961456\n",
      "Epoch 24 - D Loss: 0.47207173705101013 G Loss: 0.27461323142051697\n",
      "Epoch 24 - D Loss: 0.3171248435974121 G Loss: 0.16812479496002197\n",
      "Epoch 24 - D Loss: 0.4947514832019806 G Loss: 0.20483455061912537\n",
      "Epoch 24 - D Loss: 0.3262915015220642 G Loss: 0.07232198119163513\n",
      "Epoch 24 - D Loss: 0.3953683376312256 G Loss: 0.21028029918670654\n",
      "Epoch 24 - D Loss: 0.5047275424003601 G Loss: 0.17991189658641815\n",
      "Epoch 24 - D Loss: 0.2492765486240387 G Loss: 0.06860168278217316\n",
      "Epoch 24 - D Loss: 0.4591788351535797 G Loss: 0.24220018088817596\n",
      "Epoch 24 - D Loss: 0.3573366403579712 G Loss: 0.2382061779499054\n",
      "Epoch 24 - D Loss: 0.29147574305534363 G Loss: 0.22170980274677277\n",
      "Epoch 24 - D Loss: 0.3925729990005493 G Loss: 0.21050716936588287\n",
      "Epoch 24 - D Loss: 0.4831146001815796 G Loss: 0.16927972435951233\n",
      "Epoch 25 - D Loss: 0.4558221101760864 G Loss: 0.18689872324466705\n",
      "Epoch 25 - D Loss: 0.518444299697876 G Loss: 0.12834793329238892\n",
      "Epoch 25 - D Loss: 0.2141970843076706 G Loss: 0.1582062989473343\n",
      "Epoch 25 - D Loss: 0.3862300217151642 G Loss: 0.29162561893463135\n",
      "Epoch 25 - D Loss: 0.6302727460861206 G Loss: 0.24456287920475006\n",
      "Epoch 25 - D Loss: 0.40628430247306824 G Loss: 0.13711172342300415\n",
      "Epoch 25 - D Loss: 0.33946096897125244 G Loss: 0.2303677797317505\n",
      "Epoch 25 - D Loss: 0.3479287624359131 G Loss: 0.22687260806560516\n",
      "Epoch 25 - D Loss: 0.2917691469192505 G Loss: 0.1265183687210083\n",
      "Epoch 25 - D Loss: 0.3155858814716339 G Loss: 0.08716649562120438\n",
      "Epoch 25 - D Loss: 0.2974974513053894 G Loss: 0.2044256329536438\n",
      "Epoch 25 - D Loss: 0.5022290945053101 G Loss: 0.22197556495666504\n",
      "Epoch 25 - D Loss: 0.3010687232017517 G Loss: 0.18639826774597168\n",
      "Epoch 25 - D Loss: 0.3380274772644043 G Loss: 0.36084747314453125\n",
      "Epoch 25 - D Loss: 0.3373759984970093 G Loss: 0.2957770824432373\n",
      "Epoch 25 - D Loss: 0.27501246333122253 G Loss: 0.26314496994018555\n",
      "Epoch 25 - D Loss: 0.5264328718185425 G Loss: 0.3033614754676819\n",
      "Epoch 25 - D Loss: 0.19690999388694763 G Loss: 0.24823027849197388\n",
      "Epoch 25 - D Loss: 0.3842942714691162 G Loss: 0.16525129973888397\n",
      "Epoch 25 - D Loss: 0.34441816806793213 G Loss: 0.20110274851322174\n",
      "Epoch 25 - D Loss: 0.3686094284057617 G Loss: 0.1662464439868927\n",
      "Epoch 25 - D Loss: 0.32282036542892456 G Loss: 0.21257486939430237\n",
      "Epoch 25 - D Loss: 0.505706787109375 G Loss: 0.06364239752292633\n",
      "Epoch 25 - D Loss: 0.4015763998031616 G Loss: 0.1985560953617096\n",
      "Epoch 25 - D Loss: 0.31419000029563904 G Loss: 0.1261799931526184\n",
      "Epoch 26 - D Loss: 0.4263603091239929 G Loss: 0.10671865195035934\n",
      "Epoch 26 - D Loss: 0.30871695280075073 G Loss: 0.07749206572771072\n",
      "Epoch 26 - D Loss: 0.2582123875617981 G Loss: 0.3534529209136963\n",
      "Epoch 26 - D Loss: 0.3633817434310913 G Loss: 0.1788279414176941\n",
      "Epoch 26 - D Loss: 0.30567488074302673 G Loss: 0.29939526319503784\n",
      "Epoch 26 - D Loss: 0.3789651393890381 G Loss: 0.2668580710887909\n",
      "Epoch 26 - D Loss: 0.3288499116897583 G Loss: 0.1844845712184906\n",
      "Epoch 26 - D Loss: 0.3488911986351013 G Loss: 0.1644485890865326\n",
      "Epoch 26 - D Loss: 0.4015551209449768 G Loss: 0.22397801280021667\n",
      "Epoch 26 - D Loss: 0.3720358610153198 G Loss: 0.15393657982349396\n",
      "Epoch 26 - D Loss: 0.5249181389808655 G Loss: 0.1965029239654541\n",
      "Epoch 26 - D Loss: 0.30835169553756714 G Loss: 0.13942301273345947\n",
      "Epoch 26 - D Loss: 0.43320339918136597 G Loss: 0.2514902949333191\n",
      "Epoch 26 - D Loss: 0.43571174144744873 G Loss: 0.21427389979362488\n",
      "Epoch 26 - D Loss: 0.4217531681060791 G Loss: 0.13125154376029968\n",
      "Epoch 26 - D Loss: 0.5302218198776245 G Loss: 0.2537955641746521\n",
      "Epoch 26 - D Loss: 0.409743994474411 G Loss: 0.1192069947719574\n",
      "Epoch 26 - D Loss: 0.3823694586753845 G Loss: 0.23680023849010468\n",
      "Epoch 26 - D Loss: 0.3874002695083618 G Loss: 0.28817662596702576\n",
      "Epoch 26 - D Loss: 0.42160564661026 G Loss: 0.22487036883831024\n",
      "Epoch 26 - D Loss: 0.4721403121948242 G Loss: 0.20739717781543732\n",
      "Epoch 26 - D Loss: 0.4031536877155304 G Loss: 0.14954552054405212\n",
      "Epoch 26 - D Loss: 0.31861215829849243 G Loss: 0.29045990109443665\n",
      "Epoch 26 - D Loss: 0.40318673849105835 G Loss: 0.2853601574897766\n",
      "Epoch 26 - D Loss: 0.4082668423652649 G Loss: 0.1581481248140335\n",
      "Epoch 27 - D Loss: 0.24272143840789795 G Loss: 0.3207886219024658\n",
      "Epoch 27 - D Loss: 0.43041834235191345 G Loss: 0.257268488407135\n",
      "Epoch 27 - D Loss: 0.6104562282562256 G Loss: 0.22879865765571594\n",
      "Epoch 27 - D Loss: 0.38338518142700195 G Loss: 0.13696524500846863\n",
      "Epoch 27 - D Loss: 0.4836384654045105 G Loss: 0.14078226685523987\n",
      "Epoch 27 - D Loss: 0.3239634037017822 G Loss: 0.13976651430130005\n",
      "Epoch 27 - D Loss: 0.28903964161872864 G Loss: 0.2846975326538086\n",
      "Epoch 27 - D Loss: 0.46216630935668945 G Loss: 0.21138720214366913\n",
      "Epoch 27 - D Loss: 0.47061434388160706 G Loss: 0.2317032366991043\n",
      "Epoch 27 - D Loss: 0.411873459815979 G Loss: 0.0919073075056076\n",
      "Epoch 27 - D Loss: 0.3101969361305237 G Loss: 0.12953892350196838\n",
      "Epoch 27 - D Loss: 0.3538828492164612 G Loss: 0.07652702182531357\n",
      "Epoch 27 - D Loss: 0.5139549970626831 G Loss: 0.19120925664901733\n",
      "Epoch 27 - D Loss: 0.3417530059814453 G Loss: 0.20231717824935913\n",
      "Epoch 27 - D Loss: 0.327880322933197 G Loss: 0.14615577459335327\n",
      "Epoch 27 - D Loss: 0.30055153369903564 G Loss: 0.11415725946426392\n",
      "Epoch 27 - D Loss: 0.44852215051651 G Loss: 0.18876054883003235\n",
      "Epoch 27 - D Loss: 0.4398673474788666 G Loss: 0.2185927927494049\n",
      "Epoch 27 - D Loss: 0.36085039377212524 G Loss: 0.30449751019477844\n",
      "Epoch 27 - D Loss: 0.325567364692688 G Loss: 0.1928415149450302\n",
      "Epoch 27 - D Loss: 0.4689875841140747 G Loss: 0.22288428246974945\n",
      "Epoch 27 - D Loss: 0.5589936375617981 G Loss: 0.23687130212783813\n",
      "Epoch 27 - D Loss: 0.39976489543914795 G Loss: 0.13215294480323792\n",
      "Epoch 27 - D Loss: 0.47209012508392334 G Loss: 0.20926028490066528\n",
      "Epoch 27 - D Loss: 0.40996283292770386 G Loss: 0.25646066665649414\n",
      "Epoch 28 - D Loss: 0.33488160371780396 G Loss: 0.2740885615348816\n",
      "Epoch 28 - D Loss: 0.43152788281440735 G Loss: 0.22458302974700928\n",
      "Epoch 28 - D Loss: 0.31362178921699524 G Loss: 0.24151670932769775\n",
      "Epoch 28 - D Loss: 0.47928112745285034 G Loss: 0.22252605855464935\n",
      "Epoch 28 - D Loss: 0.32463008165359497 G Loss: 0.09827086329460144\n",
      "Epoch 28 - D Loss: 0.3649507164955139 G Loss: 0.13572128117084503\n",
      "Epoch 28 - D Loss: 0.36821287870407104 G Loss: 0.23882099986076355\n",
      "Epoch 28 - D Loss: 0.3939109444618225 G Loss: 0.2369121015071869\n",
      "Epoch 28 - D Loss: 0.3535662293434143 G Loss: 0.21722370386123657\n",
      "Epoch 28 - D Loss: 0.48699575662612915 G Loss: 0.24333743751049042\n",
      "Epoch 28 - D Loss: 0.3696758449077606 G Loss: 0.20384067296981812\n",
      "Epoch 28 - D Loss: 0.3898904323577881 G Loss: 0.22775137424468994\n",
      "Epoch 28 - D Loss: 0.22153933346271515 G Loss: 0.22905921936035156\n",
      "Epoch 28 - D Loss: 0.3985770046710968 G Loss: 0.089506596326828\n",
      "Epoch 28 - D Loss: 0.3625499904155731 G Loss: 0.16530171036720276\n",
      "Epoch 28 - D Loss: 0.4208621680736542 G Loss: 0.2249595820903778\n",
      "Epoch 28 - D Loss: 0.3835426867008209 G Loss: 0.18980266153812408\n",
      "Epoch 28 - D Loss: 0.4385659098625183 G Loss: 0.20300021767616272\n",
      "Epoch 28 - D Loss: 0.5938819646835327 G Loss: 0.19084197282791138\n",
      "Epoch 28 - D Loss: 0.5366384983062744 G Loss: 0.1457810401916504\n",
      "Epoch 28 - D Loss: 0.4088928997516632 G Loss: 0.16210153698921204\n",
      "Epoch 28 - D Loss: 0.31709301471710205 G Loss: 0.18416544795036316\n",
      "Epoch 28 - D Loss: 0.48240700364112854 G Loss: 0.260028213262558\n",
      "Epoch 28 - D Loss: 0.46311134099960327 G Loss: 0.09708642959594727\n",
      "Epoch 28 - D Loss: 0.5099028944969177 G Loss: 0.2335173338651657\n",
      "Epoch 29 - D Loss: 0.40434688329696655 G Loss: 0.1319124549627304\n",
      "Epoch 29 - D Loss: 0.24727579951286316 G Loss: 0.2206987887620926\n",
      "Epoch 29 - D Loss: 0.5880663394927979 G Loss: 0.27414143085479736\n",
      "Epoch 29 - D Loss: 0.3521287441253662 G Loss: 0.25312185287475586\n",
      "Epoch 29 - D Loss: 0.21285992860794067 G Loss: 0.259469598531723\n",
      "Epoch 29 - D Loss: 0.3663855791091919 G Loss: 0.19730332493782043\n",
      "Epoch 29 - D Loss: 0.35097724199295044 G Loss: 0.24338960647583008\n",
      "Epoch 29 - D Loss: 0.4425417184829712 G Loss: 0.2687821388244629\n",
      "Epoch 29 - D Loss: 0.29108214378356934 G Loss: 0.1591559797525406\n",
      "Epoch 29 - D Loss: 0.38921287655830383 G Loss: 0.13641628623008728\n",
      "Epoch 29 - D Loss: 0.4413415193557739 G Loss: 0.2108357548713684\n",
      "Epoch 29 - D Loss: 0.48565492033958435 G Loss: 0.20600000023841858\n",
      "Epoch 29 - D Loss: 0.36598068475723267 G Loss: 0.17519250512123108\n",
      "Epoch 29 - D Loss: 0.5511355400085449 G Loss: 0.21061284840106964\n",
      "Epoch 29 - D Loss: 0.5043139457702637 G Loss: 0.16932833194732666\n",
      "Epoch 29 - D Loss: 0.3161458373069763 G Loss: 0.19775468111038208\n",
      "Epoch 29 - D Loss: 0.3350633382797241 G Loss: 0.32912731170654297\n",
      "Epoch 29 - D Loss: 0.3554909825325012 G Loss: 0.13390541076660156\n",
      "Epoch 29 - D Loss: 0.5699530243873596 G Loss: 0.294281929731369\n",
      "Epoch 29 - D Loss: 0.4134666323661804 G Loss: 0.16338495910167694\n",
      "Epoch 29 - D Loss: 0.531445324420929 G Loss: 0.2494078278541565\n",
      "Epoch 29 - D Loss: 0.37196189165115356 G Loss: 0.27396970987319946\n",
      "Epoch 29 - D Loss: 0.4479389786720276 G Loss: 0.2563941478729248\n",
      "Epoch 29 - D Loss: 0.5306119918823242 G Loss: 0.09957048296928406\n",
      "Epoch 29 - D Loss: 0.4279274046421051 G Loss: 0.20417988300323486\n",
      "Epoch 30 - D Loss: 0.43308621644973755 G Loss: 0.2402077615261078\n",
      "Epoch 30 - D Loss: 0.3986097276210785 G Loss: 0.08724497258663177\n",
      "Epoch 30 - D Loss: 0.4943317174911499 G Loss: 0.22678382694721222\n",
      "Epoch 30 - D Loss: 0.3453212380409241 G Loss: 0.19520309567451477\n",
      "Epoch 30 - D Loss: 0.44943127036094666 G Loss: 0.11415265500545502\n",
      "Epoch 30 - D Loss: 0.26161032915115356 G Loss: 0.09959904849529266\n",
      "Epoch 30 - D Loss: 0.4486441910266876 G Loss: 0.19722610712051392\n",
      "Epoch 30 - D Loss: 0.37326914072036743 G Loss: 0.26091620326042175\n",
      "Epoch 30 - D Loss: 0.5199944972991943 G Loss: 0.18726322054862976\n",
      "Epoch 30 - D Loss: 0.2673511505126953 G Loss: 0.2772364318370819\n",
      "Epoch 30 - D Loss: 0.39303046464920044 G Loss: 0.18031474947929382\n",
      "Epoch 30 - D Loss: 0.5302360653877258 G Loss: 0.09150472283363342\n",
      "Epoch 30 - D Loss: 0.362107515335083 G Loss: 0.33673226833343506\n",
      "Epoch 30 - D Loss: 0.5308907628059387 G Loss: 0.21595431864261627\n",
      "Epoch 30 - D Loss: 0.49830156564712524 G Loss: 0.21506363153457642\n",
      "Epoch 30 - D Loss: 0.40110450983047485 G Loss: 0.1487520933151245\n",
      "Epoch 30 - D Loss: 0.333374559879303 G Loss: 0.2835109531879425\n",
      "Epoch 30 - D Loss: 0.35159480571746826 G Loss: 0.1992170661687851\n",
      "Epoch 30 - D Loss: 0.5572646856307983 G Loss: 0.1920585334300995\n",
      "Epoch 30 - D Loss: 0.3736459016799927 G Loss: 0.16545866429805756\n",
      "Epoch 30 - D Loss: 0.3780580759048462 G Loss: 0.27113211154937744\n",
      "Epoch 30 - D Loss: 0.4497028887271881 G Loss: 0.25802817940711975\n",
      "Epoch 30 - D Loss: 0.48544836044311523 G Loss: 0.18064644932746887\n",
      "Epoch 30 - D Loss: 0.410658597946167 G Loss: 0.3001864552497864\n",
      "Epoch 30 - D Loss: 0.3376210629940033 G Loss: 0.16319584846496582\n",
      "Epoch 31 - D Loss: 0.41839325428009033 G Loss: 0.2834698557853699\n",
      "Epoch 31 - D Loss: 0.4061882793903351 G Loss: 0.19821478426456451\n",
      "Epoch 31 - D Loss: 0.4263235330581665 G Loss: 0.1259293407201767\n",
      "Epoch 31 - D Loss: 0.29928600788116455 G Loss: 0.21560460329055786\n",
      "Epoch 31 - D Loss: 0.5197361707687378 G Loss: 0.24852871894836426\n",
      "Epoch 31 - D Loss: 0.49794256687164307 G Loss: 0.1212046816945076\n",
      "Epoch 31 - D Loss: 0.4784637689590454 G Loss: 0.23494544625282288\n",
      "Epoch 31 - D Loss: 0.4173988103866577 G Loss: 0.14659051597118378\n",
      "Epoch 31 - D Loss: 0.44898882508277893 G Loss: 0.07591403275728226\n",
      "Epoch 31 - D Loss: 0.44534775614738464 G Loss: 0.1608300805091858\n",
      "Epoch 31 - D Loss: 0.29129499197006226 G Loss: 0.26434943079948425\n",
      "Epoch 31 - D Loss: 0.2638761103153229 G Loss: 0.11803843826055527\n",
      "Epoch 31 - D Loss: 0.37082502245903015 G Loss: 0.19607102870941162\n",
      "Epoch 31 - D Loss: 0.41533148288726807 G Loss: 0.1376391053199768\n",
      "Epoch 31 - D Loss: 0.4429396390914917 G Loss: 0.1340969204902649\n",
      "Epoch 31 - D Loss: 0.42713475227355957 G Loss: 0.11626912653446198\n",
      "Epoch 31 - D Loss: 0.6209649443626404 G Loss: 0.1405194252729416\n",
      "Epoch 31 - D Loss: 0.4030953049659729 G Loss: 0.17545102536678314\n",
      "Epoch 31 - D Loss: 0.4060443043708801 G Loss: 0.2832849323749542\n",
      "Epoch 31 - D Loss: 0.23953577876091003 G Loss: 0.21497558057308197\n",
      "Epoch 31 - D Loss: 0.38338497281074524 G Loss: 0.20966027677059174\n",
      "Epoch 31 - D Loss: 0.43347257375717163 G Loss: 0.14898066222667694\n",
      "Epoch 31 - D Loss: 0.36275365948677063 G Loss: 0.09225301444530487\n",
      "Epoch 31 - D Loss: 0.4566662311553955 G Loss: 0.13513803482055664\n",
      "Epoch 31 - D Loss: 0.35412317514419556 G Loss: 0.1189456433057785\n",
      "Epoch 32 - D Loss: 0.5021578073501587 G Loss: 0.14427387714385986\n",
      "Epoch 32 - D Loss: 0.35660457611083984 G Loss: 0.28961697220802307\n",
      "Epoch 32 - D Loss: 0.3914894461631775 G Loss: 0.09114931523799896\n",
      "Epoch 32 - D Loss: 0.3549383580684662 G Loss: 0.12859536707401276\n",
      "Epoch 32 - D Loss: 0.4384358525276184 G Loss: 0.1375286877155304\n",
      "Epoch 32 - D Loss: 0.37364161014556885 G Loss: 0.2511661648750305\n",
      "Epoch 32 - D Loss: 0.6481296420097351 G Loss: 0.22948215901851654\n",
      "Epoch 32 - D Loss: 0.4472200274467468 G Loss: 0.1709773689508438\n",
      "Epoch 32 - D Loss: 0.3368018567562103 G Loss: 0.07181737571954727\n",
      "Epoch 32 - D Loss: 0.3148968815803528 G Loss: 0.2695111036300659\n",
      "Epoch 32 - D Loss: 0.3608416020870209 G Loss: 0.2023317664861679\n",
      "Epoch 32 - D Loss: 0.34627336263656616 G Loss: 0.16854864358901978\n",
      "Epoch 32 - D Loss: 0.5369184017181396 G Loss: 0.2794758379459381\n",
      "Epoch 32 - D Loss: 0.4557037949562073 G Loss: 0.27001070976257324\n",
      "Epoch 32 - D Loss: 0.20747539401054382 G Loss: 0.24143901467323303\n",
      "Epoch 32 - D Loss: 0.5126523375511169 G Loss: 0.20153769850730896\n",
      "Epoch 32 - D Loss: 0.4900349974632263 G Loss: 0.3313552141189575\n",
      "Epoch 32 - D Loss: 0.5232889652252197 G Loss: 0.17754364013671875\n",
      "Epoch 32 - D Loss: 0.4281906485557556 G Loss: 0.22377732396125793\n",
      "Epoch 32 - D Loss: 0.4424108862876892 G Loss: 0.13420943915843964\n",
      "Epoch 32 - D Loss: 0.4817965030670166 G Loss: 0.08249989151954651\n",
      "Epoch 32 - D Loss: 0.33324187994003296 G Loss: 0.27099013328552246\n",
      "Epoch 32 - D Loss: 0.44311681389808655 G Loss: 0.1458914875984192\n",
      "Epoch 32 - D Loss: 0.4510507583618164 G Loss: 0.3001537322998047\n",
      "Epoch 32 - D Loss: 0.41902080178260803 G Loss: 0.25605589151382446\n",
      "Epoch 33 - D Loss: 0.42204591631889343 G Loss: 0.17905640602111816\n",
      "Epoch 33 - D Loss: 0.3977692425251007 G Loss: 0.2101415991783142\n",
      "Epoch 33 - D Loss: 0.5105470418930054 G Loss: 0.26794302463531494\n",
      "Epoch 33 - D Loss: 0.4323079288005829 G Loss: 0.14791668951511383\n",
      "Epoch 33 - D Loss: 0.4448792338371277 G Loss: 0.27940964698791504\n",
      "Epoch 33 - D Loss: 0.3987375497817993 G Loss: 0.1254279911518097\n",
      "Epoch 33 - D Loss: 0.3853514790534973 G Loss: 0.08196750283241272\n",
      "Epoch 33 - D Loss: 0.21818988025188446 G Loss: 0.3219355344772339\n",
      "Epoch 33 - D Loss: 0.5777435302734375 G Loss: 0.12567180395126343\n",
      "Epoch 33 - D Loss: 0.27108949422836304 G Loss: 0.19650790095329285\n",
      "Epoch 33 - D Loss: 0.4047498404979706 G Loss: 0.2593257427215576\n",
      "Epoch 33 - D Loss: 0.39582982659339905 G Loss: 0.1959165781736374\n",
      "Epoch 33 - D Loss: 0.4090472161769867 G Loss: 0.18045468628406525\n",
      "Epoch 33 - D Loss: 0.3707629144191742 G Loss: 0.19583706557750702\n",
      "Epoch 33 - D Loss: 0.5168996453285217 G Loss: 0.08002267777919769\n",
      "Epoch 33 - D Loss: 0.43552887439727783 G Loss: 0.22154127061367035\n",
      "Epoch 33 - D Loss: 0.4410719871520996 G Loss: 0.2184629589319229\n",
      "Epoch 33 - D Loss: 0.476065069437027 G Loss: 0.1624128669500351\n",
      "Epoch 33 - D Loss: 0.381762832403183 G Loss: 0.23238088190555573\n",
      "Epoch 33 - D Loss: 0.40385594964027405 G Loss: 0.3322387635707855\n",
      "Epoch 33 - D Loss: 0.37349244952201843 G Loss: 0.1675163209438324\n",
      "Epoch 33 - D Loss: 0.35826873779296875 G Loss: 0.19364194571971893\n",
      "Epoch 33 - D Loss: 0.4159625172615051 G Loss: 0.15294933319091797\n",
      "Epoch 33 - D Loss: 0.44925743341445923 G Loss: 0.26332083344459534\n",
      "Epoch 33 - D Loss: 0.3871850073337555 G Loss: 0.28808847069740295\n",
      "Epoch 34 - D Loss: 0.42417800426483154 G Loss: 0.20849007368087769\n",
      "Epoch 34 - D Loss: 0.45548927783966064 G Loss: 0.1943219006061554\n",
      "Epoch 34 - D Loss: 0.3343930244445801 G Loss: 0.30253666639328003\n",
      "Epoch 34 - D Loss: 0.4512908458709717 G Loss: 0.2779580354690552\n",
      "Epoch 34 - D Loss: 0.3071591258049011 G Loss: 0.19644907116889954\n",
      "Epoch 34 - D Loss: 0.48040202260017395 G Loss: 0.2423766702413559\n",
      "Epoch 34 - D Loss: 0.3549162447452545 G Loss: 0.2848820984363556\n",
      "Epoch 34 - D Loss: 0.4135591387748718 G Loss: 0.13220134377479553\n",
      "Epoch 34 - D Loss: 0.3948690593242645 G Loss: 0.17560850083827972\n",
      "Epoch 34 - D Loss: 0.354766845703125 G Loss: 0.11281374096870422\n",
      "Epoch 34 - D Loss: 0.28826940059661865 G Loss: 0.233010932803154\n",
      "Epoch 34 - D Loss: 0.31248149275779724 G Loss: 0.19564388692378998\n",
      "Epoch 34 - D Loss: 0.43262261152267456 G Loss: 0.36639541387557983\n",
      "Epoch 34 - D Loss: 0.3689516484737396 G Loss: 0.08971777558326721\n",
      "Epoch 34 - D Loss: 0.3517782688140869 G Loss: 0.20207522809505463\n",
      "Epoch 34 - D Loss: 0.41165274381637573 G Loss: 0.18874722719192505\n",
      "Epoch 34 - D Loss: 0.28938785195350647 G Loss: 0.22496989369392395\n",
      "Epoch 34 - D Loss: 0.38518139719963074 G Loss: 0.04632534086704254\n",
      "Epoch 34 - D Loss: 0.3009377419948578 G Loss: 0.08170896768569946\n",
      "Epoch 34 - D Loss: 0.31664007902145386 G Loss: 0.1563391089439392\n",
      "Epoch 34 - D Loss: 0.5442433953285217 G Loss: 0.2745192050933838\n",
      "Epoch 34 - D Loss: 0.4731732904911041 G Loss: 0.16764983534812927\n",
      "Epoch 34 - D Loss: 0.29317912459373474 G Loss: 0.2783305048942566\n",
      "Epoch 34 - D Loss: 0.36234045028686523 G Loss: 0.2727712094783783\n",
      "Epoch 34 - D Loss: 0.41334182024002075 G Loss: 0.28431689739227295\n",
      "Epoch 35 - D Loss: 0.2623547911643982 G Loss: 0.1708296239376068\n",
      "Epoch 35 - D Loss: 0.5279563069343567 G Loss: 0.17129600048065186\n",
      "Epoch 35 - D Loss: 0.44440531730651855 G Loss: 0.2083374708890915\n",
      "Epoch 35 - D Loss: 0.4618048071861267 G Loss: 0.3332872986793518\n",
      "Epoch 35 - D Loss: 0.415696918964386 G Loss: 0.12638536095619202\n",
      "Epoch 35 - D Loss: 0.385804682970047 G Loss: 0.16886253654956818\n",
      "Epoch 35 - D Loss: 0.5679876208305359 G Loss: 0.20253917574882507\n",
      "Epoch 35 - D Loss: 0.35895219445228577 G Loss: 0.1867540329694748\n",
      "Epoch 35 - D Loss: 0.5363004207611084 G Loss: 0.1590137779712677\n",
      "Epoch 35 - D Loss: 0.4311610460281372 G Loss: 0.23446770012378693\n",
      "Epoch 35 - D Loss: 0.2769451141357422 G Loss: 0.19698868691921234\n",
      "Epoch 35 - D Loss: 0.15945559740066528 G Loss: 0.17201948165893555\n",
      "Epoch 35 - D Loss: 0.4363034963607788 G Loss: 0.1601664125919342\n",
      "Epoch 35 - D Loss: 0.5094155073165894 G Loss: 0.17546220123767853\n",
      "Epoch 35 - D Loss: 0.380382239818573 G Loss: 0.2848706841468811\n",
      "Epoch 35 - D Loss: 0.34309059381484985 G Loss: 0.21056745946407318\n",
      "Epoch 35 - D Loss: 0.36256006360054016 G Loss: 0.3282841742038727\n",
      "Epoch 35 - D Loss: 0.41231992840766907 G Loss: 0.12925861775875092\n",
      "Epoch 35 - D Loss: 0.31274306774139404 G Loss: 0.1843714416027069\n",
      "Epoch 35 - D Loss: 0.4365934729576111 G Loss: 0.12979328632354736\n",
      "Epoch 35 - D Loss: 0.5550234913825989 G Loss: 0.19134046137332916\n",
      "Epoch 35 - D Loss: 0.5251390933990479 G Loss: 0.23546798527240753\n",
      "Epoch 35 - D Loss: 0.459754079580307 G Loss: 0.31993138790130615\n",
      "Epoch 35 - D Loss: 0.34915879368782043 G Loss: 0.17271016538143158\n",
      "Epoch 35 - D Loss: 0.42076075077056885 G Loss: 0.1117464154958725\n",
      "Epoch 36 - D Loss: 0.4847552180290222 G Loss: 0.16874733567237854\n",
      "Epoch 36 - D Loss: 0.2770722806453705 G Loss: 0.052924931049346924\n",
      "Epoch 36 - D Loss: 0.34454071521759033 G Loss: 0.20929694175720215\n",
      "Epoch 36 - D Loss: 0.3553098738193512 G Loss: 0.2173287570476532\n",
      "Epoch 36 - D Loss: 0.3391769528388977 G Loss: 0.2755225896835327\n",
      "Epoch 36 - D Loss: 0.32617777585983276 G Loss: 0.09069988876581192\n",
      "Epoch 36 - D Loss: 0.574953556060791 G Loss: 0.11426106095314026\n",
      "Epoch 36 - D Loss: 0.5411302447319031 G Loss: 0.31728723645210266\n",
      "Epoch 36 - D Loss: 0.4266783595085144 G Loss: 0.2248711884021759\n",
      "Epoch 36 - D Loss: 0.4744792878627777 G Loss: 0.19341516494750977\n",
      "Epoch 36 - D Loss: 0.46306756138801575 G Loss: 0.18228894472122192\n",
      "Epoch 36 - D Loss: 0.40682512521743774 G Loss: 0.12604829668998718\n",
      "Epoch 36 - D Loss: 0.2970605492591858 G Loss: 0.20116166770458221\n",
      "Epoch 36 - D Loss: 0.34474319219589233 G Loss: 0.25075775384902954\n",
      "Epoch 36 - D Loss: 0.4017794728279114 G Loss: 0.07577498257160187\n",
      "Epoch 36 - D Loss: 0.5572898387908936 G Loss: 0.1338026225566864\n",
      "Epoch 36 - D Loss: 0.42914485931396484 G Loss: 0.2737584114074707\n",
      "Epoch 36 - D Loss: 0.5159662365913391 G Loss: 0.11004939675331116\n",
      "Epoch 36 - D Loss: 0.2739972174167633 G Loss: 0.3228863775730133\n",
      "Epoch 36 - D Loss: 0.5937175154685974 G Loss: 0.12939061224460602\n",
      "Epoch 36 - D Loss: 0.44001802802085876 G Loss: 0.11803551763296127\n",
      "Epoch 36 - D Loss: 0.2663731276988983 G Loss: 0.2502095103263855\n",
      "Epoch 36 - D Loss: 0.33279797434806824 G Loss: 0.1313095986843109\n",
      "Epoch 36 - D Loss: 0.27901601791381836 G Loss: 0.22139345109462738\n",
      "Epoch 36 - D Loss: 0.38421300053596497 G Loss: 0.254189133644104\n",
      "Epoch 37 - D Loss: 0.5017051696777344 G Loss: 0.21150533854961395\n",
      "Epoch 37 - D Loss: 0.31499746441841125 G Loss: 0.0757235437631607\n",
      "Epoch 37 - D Loss: 0.251265287399292 G Loss: 0.17391476035118103\n",
      "Epoch 37 - D Loss: 0.40488412976264954 G Loss: 0.26211830973625183\n",
      "Epoch 37 - D Loss: 0.3036186397075653 G Loss: 0.19000044465065002\n",
      "Epoch 37 - D Loss: 0.5268992185592651 G Loss: 0.21843719482421875\n",
      "Epoch 37 - D Loss: 0.4849637746810913 G Loss: 0.19636398553848267\n",
      "Epoch 37 - D Loss: 0.32165589928627014 G Loss: 0.09582111239433289\n",
      "Epoch 37 - D Loss: 0.47715550661087036 G Loss: 0.31088268756866455\n",
      "Epoch 37 - D Loss: 0.4131232500076294 G Loss: 0.22318574786186218\n",
      "Epoch 37 - D Loss: 0.43978360295295715 G Loss: 0.20285114645957947\n",
      "Epoch 37 - D Loss: 0.32571908831596375 G Loss: 0.28072309494018555\n",
      "Epoch 37 - D Loss: 0.5646998882293701 G Loss: 0.24431604146957397\n",
      "Epoch 37 - D Loss: 0.3816395699977875 G Loss: 0.2694143056869507\n",
      "Epoch 37 - D Loss: 0.3906922936439514 G Loss: 0.24123650789260864\n",
      "Epoch 37 - D Loss: 0.49128520488739014 G Loss: 0.12711305916309357\n",
      "Epoch 37 - D Loss: 0.42096683382987976 G Loss: 0.3123616874217987\n",
      "Epoch 37 - D Loss: 0.30563417077064514 G Loss: 0.173907071352005\n",
      "Epoch 37 - D Loss: 0.4659690260887146 G Loss: 0.2712089419364929\n",
      "Epoch 37 - D Loss: 0.420192152261734 G Loss: 0.2208486646413803\n",
      "Epoch 37 - D Loss: 0.40467509627342224 G Loss: 0.19015663862228394\n",
      "Epoch 37 - D Loss: 0.3753185272216797 G Loss: 0.1612691730260849\n",
      "Epoch 37 - D Loss: 0.49819791316986084 G Loss: 0.10348555445671082\n",
      "Epoch 37 - D Loss: 0.30194205045700073 G Loss: 0.09868393838405609\n",
      "Epoch 37 - D Loss: 0.4647848308086395 G Loss: 0.17489805817604065\n",
      "Epoch 38 - D Loss: 0.4366687536239624 G Loss: 0.19305843114852905\n",
      "Epoch 38 - D Loss: 0.37244072556495667 G Loss: 0.24874567985534668\n",
      "Epoch 38 - D Loss: 0.24952886998653412 G Loss: 0.24801476299762726\n",
      "Epoch 38 - D Loss: 0.537788450717926 G Loss: 0.21438655257225037\n",
      "Epoch 38 - D Loss: 0.38300567865371704 G Loss: 0.20399950444698334\n",
      "Epoch 38 - D Loss: 0.5589156746864319 G Loss: 0.09527108073234558\n",
      "Epoch 38 - D Loss: 0.47350403666496277 G Loss: 0.11594388633966446\n",
      "Epoch 38 - D Loss: 0.44080689549446106 G Loss: 0.32080650329589844\n",
      "Epoch 38 - D Loss: 0.4885517954826355 G Loss: 0.20158515870571136\n",
      "Epoch 38 - D Loss: 0.39252838492393494 G Loss: 0.1550302505493164\n",
      "Epoch 38 - D Loss: 0.43722420930862427 G Loss: 0.34016624093055725\n",
      "Epoch 38 - D Loss: 0.36952275037765503 G Loss: 0.26947277784347534\n",
      "Epoch 38 - D Loss: 0.471514493227005 G Loss: 0.16990342736244202\n",
      "Epoch 38 - D Loss: 0.4126778244972229 G Loss: 0.09933865070343018\n",
      "Epoch 38 - D Loss: 0.37551912665367126 G Loss: 0.1680753529071808\n",
      "Epoch 38 - D Loss: 0.5144656896591187 G Loss: 0.146450012922287\n",
      "Epoch 38 - D Loss: 0.4371179938316345 G Loss: 0.3037881553173065\n",
      "Epoch 38 - D Loss: 0.6262174248695374 G Loss: 0.15693962574005127\n",
      "Epoch 38 - D Loss: 0.3753209412097931 G Loss: 0.2209722399711609\n",
      "Epoch 38 - D Loss: 0.43916213512420654 G Loss: 0.1079731360077858\n",
      "Epoch 38 - D Loss: 0.3594050109386444 G Loss: 0.2604990005493164\n",
      "Epoch 38 - D Loss: 0.49631384015083313 G Loss: 0.12377516180276871\n",
      "Epoch 38 - D Loss: 0.43259739875793457 G Loss: 0.21222108602523804\n",
      "Epoch 38 - D Loss: 0.2992362380027771 G Loss: 0.266348659992218\n",
      "Epoch 38 - D Loss: 0.5186535120010376 G Loss: 0.1725899875164032\n",
      "Epoch 39 - D Loss: 0.4709467887878418 G Loss: 0.09816940128803253\n",
      "Epoch 39 - D Loss: 0.42082181572914124 G Loss: 0.21359947323799133\n",
      "Epoch 39 - D Loss: 0.488392174243927 G Loss: 0.173607736825943\n",
      "Epoch 39 - D Loss: 0.3892500102519989 G Loss: 0.27824950218200684\n",
      "Epoch 39 - D Loss: 0.45504701137542725 G Loss: 0.20815998315811157\n",
      "Epoch 39 - D Loss: 0.4276062846183777 G Loss: 0.13687902688980103\n",
      "Epoch 39 - D Loss: 0.3999146819114685 G Loss: 0.2840980291366577\n",
      "Epoch 39 - D Loss: 0.38125187158584595 G Loss: 0.1819179654121399\n",
      "Epoch 39 - D Loss: 0.446140319108963 G Loss: 0.2589624524116516\n",
      "Epoch 39 - D Loss: 0.3035528361797333 G Loss: 0.19934499263763428\n",
      "Epoch 39 - D Loss: 0.4034469723701477 G Loss: 0.1613527238368988\n",
      "Epoch 39 - D Loss: 0.44599613547325134 G Loss: 0.20264965295791626\n",
      "Epoch 39 - D Loss: 0.32561245560646057 G Loss: 0.08633994311094284\n",
      "Epoch 39 - D Loss: 0.3343992829322815 G Loss: 0.21460692584514618\n",
      "Epoch 39 - D Loss: 0.4072186350822449 G Loss: 0.1600123941898346\n",
      "Epoch 39 - D Loss: 0.41291695833206177 G Loss: 0.260153204202652\n",
      "Epoch 39 - D Loss: 0.4098149538040161 G Loss: 0.23020994663238525\n",
      "Epoch 39 - D Loss: 0.3284306526184082 G Loss: 0.24058164656162262\n",
      "Epoch 39 - D Loss: 0.38040441274642944 G Loss: 0.2500288188457489\n",
      "Epoch 39 - D Loss: 0.35690510272979736 G Loss: 0.11059506237506866\n",
      "Epoch 39 - D Loss: 0.46578139066696167 G Loss: 0.19360071420669556\n",
      "Epoch 39 - D Loss: 0.3908275365829468 G Loss: 0.29370421171188354\n",
      "Epoch 39 - D Loss: 0.4585306644439697 G Loss: 0.26128897070884705\n",
      "Epoch 39 - D Loss: 0.5059269070625305 G Loss: 0.19538196921348572\n",
      "Epoch 39 - D Loss: 0.1735660582780838 G Loss: 0.1803399920463562\n",
      "Epoch 40 - D Loss: 0.48275765776634216 G Loss: 0.22937241196632385\n",
      "Epoch 40 - D Loss: 0.47014421224594116 G Loss: 0.25220611691474915\n",
      "Epoch 40 - D Loss: 0.36402106285095215 G Loss: 0.1973983198404312\n",
      "Epoch 40 - D Loss: 0.45671990513801575 G Loss: 0.19259221851825714\n",
      "Epoch 40 - D Loss: 0.5182385444641113 G Loss: 0.1771966814994812\n",
      "Epoch 40 - D Loss: 0.3290832042694092 G Loss: 0.31033456325531006\n",
      "Epoch 40 - D Loss: 0.3569960594177246 G Loss: 0.16166338324546814\n",
      "Epoch 40 - D Loss: 0.49349862337112427 G Loss: 0.21465212106704712\n",
      "Epoch 40 - D Loss: 0.2443126142024994 G Loss: 0.17935539782047272\n",
      "Epoch 40 - D Loss: 0.47733524441719055 G Loss: 0.15152610838413239\n",
      "Epoch 40 - D Loss: 0.419931024312973 G Loss: 0.24820011854171753\n",
      "Epoch 40 - D Loss: 0.5367017984390259 G Loss: 0.2066282480955124\n",
      "Epoch 40 - D Loss: 0.29062819480895996 G Loss: 0.18859222531318665\n",
      "Epoch 40 - D Loss: 0.3972155451774597 G Loss: 0.21836751699447632\n",
      "Epoch 40 - D Loss: 0.35504433512687683 G Loss: 0.13845151662826538\n",
      "Epoch 40 - D Loss: 0.5086348056793213 G Loss: 0.24791187047958374\n",
      "Epoch 40 - D Loss: 0.3543578088283539 G Loss: 0.07990112900733948\n",
      "Epoch 40 - D Loss: 0.24585062265396118 G Loss: 0.09563405066728592\n",
      "Epoch 40 - D Loss: 0.3142358660697937 G Loss: 0.25522521138191223\n",
      "Epoch 40 - D Loss: 0.538612961769104 G Loss: 0.16061481833457947\n",
      "Epoch 40 - D Loss: 0.3438083529472351 G Loss: 0.22457370162010193\n",
      "Epoch 40 - D Loss: 0.4012819528579712 G Loss: 0.18713632225990295\n",
      "Epoch 40 - D Loss: 0.4070624113082886 G Loss: 0.19728480279445648\n",
      "Epoch 40 - D Loss: 0.3418205976486206 G Loss: 0.27903425693511963\n",
      "Epoch 40 - D Loss: 0.43431949615478516 G Loss: 0.2520107626914978\n",
      "Epoch 41 - D Loss: 0.38848525285720825 G Loss: 0.15158048272132874\n",
      "Epoch 41 - D Loss: 0.49584856629371643 G Loss: 0.2999218702316284\n",
      "Epoch 41 - D Loss: 0.34107691049575806 G Loss: 0.1593516319990158\n",
      "Epoch 41 - D Loss: 0.4406532049179077 G Loss: 0.22794005274772644\n",
      "Epoch 41 - D Loss: 0.48628196120262146 G Loss: 0.1670236736536026\n",
      "Epoch 41 - D Loss: 0.43501347303390503 G Loss: 0.2257472574710846\n",
      "Epoch 41 - D Loss: 0.4215138554573059 G Loss: 0.21370701491832733\n",
      "Epoch 41 - D Loss: 0.46998369693756104 G Loss: 0.11686652898788452\n",
      "Epoch 41 - D Loss: 0.3404942750930786 G Loss: 0.2005797028541565\n",
      "Epoch 41 - D Loss: 0.23834902048110962 G Loss: 0.18877051770687103\n",
      "Epoch 41 - D Loss: 0.32163143157958984 G Loss: 0.19575190544128418\n",
      "Epoch 41 - D Loss: 0.2891188859939575 G Loss: 0.2584356963634491\n",
      "Epoch 41 - D Loss: 0.2803049683570862 G Loss: 0.15322597324848175\n",
      "Epoch 41 - D Loss: 0.37183916568756104 G Loss: 0.25735247135162354\n",
      "Epoch 41 - D Loss: 0.3289026618003845 G Loss: 0.1471097469329834\n",
      "Epoch 41 - D Loss: 0.3627527356147766 G Loss: 0.20812277495861053\n",
      "Epoch 41 - D Loss: 0.3866932988166809 G Loss: 0.2356606423854828\n",
      "Epoch 41 - D Loss: 0.35079050064086914 G Loss: 0.1450456976890564\n",
      "Epoch 41 - D Loss: 0.40026360750198364 G Loss: 0.17322437465190887\n",
      "Epoch 41 - D Loss: 0.38489359617233276 G Loss: 0.23161596059799194\n",
      "Epoch 41 - D Loss: 0.29161393642425537 G Loss: 0.2521860897541046\n",
      "Epoch 41 - D Loss: 0.4408101439476013 G Loss: 0.18418461084365845\n",
      "Epoch 41 - D Loss: 0.2965262830257416 G Loss: 0.2756105363368988\n",
      "Epoch 41 - D Loss: 0.475741982460022 G Loss: 0.20810696482658386\n",
      "Epoch 41 - D Loss: 0.3979954421520233 G Loss: 0.22024065256118774\n",
      "Epoch 42 - D Loss: 0.39678943157196045 G Loss: 0.24344411492347717\n",
      "Epoch 42 - D Loss: 0.47934961318969727 G Loss: 0.19667884707450867\n",
      "Epoch 42 - D Loss: 0.3118714690208435 G Loss: 0.19956669211387634\n",
      "Epoch 42 - D Loss: 0.30837082862854004 G Loss: 0.1640838384628296\n",
      "Epoch 42 - D Loss: 0.37417957186698914 G Loss: 0.15268181264400482\n",
      "Epoch 42 - D Loss: 0.27481356263160706 G Loss: 0.325031042098999\n",
      "Epoch 42 - D Loss: 0.3488374650478363 G Loss: 0.1012679859995842\n",
      "Epoch 42 - D Loss: 0.5075865387916565 G Loss: 0.22544458508491516\n",
      "Epoch 42 - D Loss: 0.38033419847488403 G Loss: 0.09706869721412659\n",
      "Epoch 42 - D Loss: 0.3179609775543213 G Loss: 0.22925108671188354\n",
      "Epoch 42 - D Loss: 0.3767507076263428 G Loss: 0.05329582467675209\n",
      "Epoch 42 - D Loss: 0.21247494220733643 G Loss: 0.18136271834373474\n",
      "Epoch 42 - D Loss: 0.35182440280914307 G Loss: 0.19275707006454468\n",
      "Epoch 42 - D Loss: 0.5002215504646301 G Loss: 0.2586961090564728\n",
      "Epoch 42 - D Loss: 0.4340125024318695 G Loss: 0.2818087339401245\n",
      "Epoch 42 - D Loss: 0.38267940282821655 G Loss: 0.3069698214530945\n",
      "Epoch 42 - D Loss: 0.35563525557518005 G Loss: 0.2832361161708832\n",
      "Epoch 42 - D Loss: 0.41225165128707886 G Loss: 0.09599055349826813\n",
      "Epoch 42 - D Loss: 0.4087778925895691 G Loss: 0.2436787486076355\n",
      "Epoch 42 - D Loss: 0.44638386368751526 G Loss: 0.2353161722421646\n",
      "Epoch 42 - D Loss: 0.2664704918861389 G Loss: 0.21820619702339172\n",
      "Epoch 42 - D Loss: 0.38271188735961914 G Loss: 0.17538604140281677\n",
      "Epoch 42 - D Loss: 0.4525512754917145 G Loss: 0.25013425946235657\n",
      "Epoch 42 - D Loss: 0.44477325677871704 G Loss: 0.19883279502391815\n",
      "Epoch 42 - D Loss: 0.29806262254714966 G Loss: 0.29475337266921997\n",
      "Epoch 43 - D Loss: 0.3726736903190613 G Loss: 0.19279620051383972\n",
      "Epoch 43 - D Loss: 0.5809886455535889 G Loss: 0.2681676149368286\n",
      "Epoch 43 - D Loss: 0.334504634141922 G Loss: 0.2404491901397705\n",
      "Epoch 43 - D Loss: 0.3857988119125366 G Loss: 0.24080657958984375\n",
      "Epoch 43 - D Loss: 0.4164755642414093 G Loss: 0.22860682010650635\n",
      "Epoch 43 - D Loss: 0.510871410369873 G Loss: 0.22145633399486542\n",
      "Epoch 43 - D Loss: 0.35479357838630676 G Loss: 0.17421169579029083\n",
      "Epoch 43 - D Loss: 0.31997808814048767 G Loss: 0.20936334133148193\n",
      "Epoch 43 - D Loss: 0.4378678798675537 G Loss: 0.24999941885471344\n",
      "Epoch 43 - D Loss: 0.33305883407592773 G Loss: 0.10283370316028595\n",
      "Epoch 43 - D Loss: 0.3285321891307831 G Loss: 0.2118339240550995\n",
      "Epoch 43 - D Loss: 0.5323688387870789 G Loss: 0.17610707879066467\n",
      "Epoch 43 - D Loss: 0.47904378175735474 G Loss: 0.171735018491745\n",
      "Epoch 43 - D Loss: 0.3911525011062622 G Loss: 0.2226828783750534\n",
      "Epoch 43 - D Loss: 0.2767075300216675 G Loss: 0.21093851327896118\n",
      "Epoch 43 - D Loss: 0.21867817640304565 G Loss: 0.1402357518672943\n",
      "Epoch 43 - D Loss: 0.3683021068572998 G Loss: 0.2861374318599701\n",
      "Epoch 43 - D Loss: 0.45830273628234863 G Loss: 0.2695598304271698\n",
      "Epoch 43 - D Loss: 0.32191503047943115 G Loss: 0.19515353441238403\n",
      "Epoch 43 - D Loss: 0.5566567182540894 G Loss: 0.2249775230884552\n",
      "Epoch 43 - D Loss: 0.38805848360061646 G Loss: 0.11062406748533249\n",
      "Epoch 43 - D Loss: 0.4545042812824249 G Loss: 0.2646838128566742\n",
      "Epoch 43 - D Loss: 0.48679256439208984 G Loss: 0.29741787910461426\n",
      "Epoch 43 - D Loss: 0.43967369198799133 G Loss: 0.18742075562477112\n",
      "Epoch 43 - D Loss: 0.5102028846740723 G Loss: 0.15988998115062714\n",
      "Epoch 44 - D Loss: 0.4690213203430176 G Loss: 0.1732679307460785\n",
      "Epoch 44 - D Loss: 0.3799320459365845 G Loss: 0.08223183453083038\n",
      "Epoch 44 - D Loss: 0.4439426064491272 G Loss: 0.2741206884384155\n",
      "Epoch 44 - D Loss: 0.4504661560058594 G Loss: 0.11768840998411179\n",
      "Epoch 44 - D Loss: 0.5046393275260925 G Loss: 0.2970486879348755\n",
      "Epoch 44 - D Loss: 0.5320520401000977 G Loss: 0.11187466979026794\n",
      "Epoch 44 - D Loss: 0.6129032969474792 G Loss: 0.22679045796394348\n",
      "Epoch 44 - D Loss: 0.4522337317466736 G Loss: 0.15226846933364868\n",
      "Epoch 44 - D Loss: 0.43318361043930054 G Loss: 0.33226823806762695\n",
      "Epoch 44 - D Loss: 0.3757452368736267 G Loss: 0.3148692548274994\n",
      "Epoch 44 - D Loss: 0.45657026767730713 G Loss: 0.08337458968162537\n",
      "Epoch 44 - D Loss: 0.5034447312355042 G Loss: 0.1662731170654297\n",
      "Epoch 44 - D Loss: 0.40526288747787476 G Loss: 0.09859860688447952\n",
      "Epoch 44 - D Loss: 0.48899972438812256 G Loss: 0.23986995220184326\n",
      "Epoch 44 - D Loss: 0.40177303552627563 G Loss: 0.24785108864307404\n",
      "Epoch 44 - D Loss: 0.5331404209136963 G Loss: 0.2226269543170929\n",
      "Epoch 44 - D Loss: 0.430528849363327 G Loss: 0.1283589005470276\n",
      "Epoch 44 - D Loss: 0.4088757634162903 G Loss: 0.12251411378383636\n",
      "Epoch 44 - D Loss: 0.36390647292137146 G Loss: 0.11470011621713638\n",
      "Epoch 44 - D Loss: 0.40824615955352783 G Loss: 0.06363293528556824\n",
      "Epoch 44 - D Loss: 0.43039584159851074 G Loss: 0.2883855104446411\n",
      "Epoch 44 - D Loss: 0.41256049275398254 G Loss: 0.20644104480743408\n",
      "Epoch 44 - D Loss: 0.35624825954437256 G Loss: 0.23265552520751953\n",
      "Epoch 44 - D Loss: 0.43499433994293213 G Loss: 0.22072532773017883\n",
      "Epoch 44 - D Loss: 0.43825727701187134 G Loss: 0.18449625372886658\n",
      "Epoch 45 - D Loss: 0.4920206665992737 G Loss: 0.14073938131332397\n",
      "Epoch 45 - D Loss: 0.3419387638568878 G Loss: 0.14586979150772095\n",
      "Epoch 45 - D Loss: 0.3928389847278595 G Loss: 0.23121842741966248\n",
      "Epoch 45 - D Loss: 0.47268176078796387 G Loss: 0.17178572714328766\n",
      "Epoch 45 - D Loss: 0.3107932209968567 G Loss: 0.17688621580600739\n",
      "Epoch 45 - D Loss: 0.3297007083892822 G Loss: 0.22379843890666962\n",
      "Epoch 45 - D Loss: 0.4241901636123657 G Loss: 0.10233713686466217\n",
      "Epoch 45 - D Loss: 0.37296193838119507 G Loss: 0.15802477300167084\n",
      "Epoch 45 - D Loss: 0.46364301443099976 G Loss: 0.20856702327728271\n",
      "Epoch 45 - D Loss: 0.3185245990753174 G Loss: 0.3109172582626343\n",
      "Epoch 45 - D Loss: 0.3939637541770935 G Loss: 0.14382478594779968\n",
      "Epoch 45 - D Loss: 0.5079951286315918 G Loss: 0.24271568655967712\n",
      "Epoch 45 - D Loss: 0.36855947971343994 G Loss: 0.2571847140789032\n",
      "Epoch 45 - D Loss: 0.5228945016860962 G Loss: 0.18401376903057098\n",
      "Epoch 45 - D Loss: 0.4984445571899414 G Loss: 0.22072190046310425\n",
      "Epoch 45 - D Loss: 0.3586136996746063 G Loss: 0.14972439408302307\n",
      "Epoch 45 - D Loss: 0.4309898018836975 G Loss: 0.1608002483844757\n",
      "Epoch 45 - D Loss: 0.4756118059158325 G Loss: 0.2799830138683319\n",
      "Epoch 45 - D Loss: 0.35890400409698486 G Loss: 0.16105139255523682\n",
      "Epoch 45 - D Loss: 0.3814099431037903 G Loss: 0.07910371571779251\n",
      "Epoch 45 - D Loss: 0.36965569853782654 G Loss: 0.1799980252981186\n",
      "Epoch 45 - D Loss: 0.3832760751247406 G Loss: 0.15276899933815002\n",
      "Epoch 45 - D Loss: 0.39811307191848755 G Loss: 0.18178193271160126\n",
      "Epoch 45 - D Loss: 0.44597724080085754 G Loss: 0.21388480067253113\n",
      "Epoch 45 - D Loss: 0.4027421474456787 G Loss: 0.1392720639705658\n",
      "Epoch 46 - D Loss: 0.4568489193916321 G Loss: 0.2458561658859253\n",
      "Epoch 46 - D Loss: 0.487449049949646 G Loss: 0.161230206489563\n",
      "Epoch 46 - D Loss: 0.4725615382194519 G Loss: 0.16384799778461456\n",
      "Epoch 46 - D Loss: 0.4577331244945526 G Loss: 0.2564855217933655\n",
      "Epoch 46 - D Loss: 0.34863781929016113 G Loss: 0.14287656545639038\n",
      "Epoch 46 - D Loss: 0.2921958267688751 G Loss: 0.27733370661735535\n",
      "Epoch 46 - D Loss: 0.2981388568878174 G Loss: 0.1776607781648636\n",
      "Epoch 46 - D Loss: 0.4512535035610199 G Loss: 0.27672016620635986\n",
      "Epoch 46 - D Loss: 0.40594983100891113 G Loss: 0.2073807716369629\n",
      "Epoch 46 - D Loss: 0.431475430727005 G Loss: 0.29034650325775146\n",
      "Epoch 46 - D Loss: 0.3992789387702942 G Loss: 0.26796090602874756\n",
      "Epoch 46 - D Loss: 0.45234376192092896 G Loss: 0.20046308636665344\n",
      "Epoch 46 - D Loss: 0.34635406732559204 G Loss: 0.12811696529388428\n",
      "Epoch 46 - D Loss: 0.2307538390159607 G Loss: 0.3095242977142334\n",
      "Epoch 46 - D Loss: 0.4750196635723114 G Loss: 0.2519369125366211\n",
      "Epoch 46 - D Loss: 0.46450939774513245 G Loss: 0.17994627356529236\n",
      "Epoch 46 - D Loss: 0.42564424872398376 G Loss: 0.16372403502464294\n",
      "Epoch 46 - D Loss: 0.24683937430381775 G Loss: 0.19560883939266205\n",
      "Epoch 46 - D Loss: 0.5140723586082458 G Loss: 0.26970767974853516\n",
      "Epoch 46 - D Loss: 0.4388373792171478 G Loss: 0.2654365003108978\n",
      "Epoch 46 - D Loss: 0.3897348642349243 G Loss: 0.19032925367355347\n",
      "Epoch 46 - D Loss: 0.39256444573402405 G Loss: 0.0918019562959671\n",
      "Epoch 46 - D Loss: 0.47463852167129517 G Loss: 0.24642863869667053\n",
      "Epoch 46 - D Loss: 0.3466203212738037 G Loss: 0.2233322262763977\n",
      "Epoch 46 - D Loss: 0.493280291557312 G Loss: 0.33412522077560425\n",
      "Epoch 47 - D Loss: 0.5211783647537231 G Loss: 0.24514254927635193\n",
      "Epoch 47 - D Loss: 0.2351347804069519 G Loss: 0.23637336492538452\n",
      "Epoch 47 - D Loss: 0.29894891381263733 G Loss: 0.18965759873390198\n",
      "Epoch 47 - D Loss: 0.453798770904541 G Loss: 0.27002912759780884\n",
      "Epoch 47 - D Loss: 0.6145789623260498 G Loss: 0.21221168339252472\n",
      "Epoch 47 - D Loss: 0.5025495290756226 G Loss: 0.21707578003406525\n",
      "Epoch 47 - D Loss: 0.25712743401527405 G Loss: 0.23884925246238708\n",
      "Epoch 47 - D Loss: 0.27095890045166016 G Loss: 0.15723183751106262\n",
      "Epoch 47 - D Loss: 0.3482028543949127 G Loss: 0.17017874121665955\n",
      "Epoch 47 - D Loss: 0.4507925510406494 G Loss: 0.12358684092760086\n",
      "Epoch 47 - D Loss: 0.32705754041671753 G Loss: 0.18248531222343445\n",
      "Epoch 47 - D Loss: 0.4657503068447113 G Loss: 0.12910690903663635\n",
      "Epoch 47 - D Loss: 0.3640718460083008 G Loss: 0.13530464470386505\n",
      "Epoch 47 - D Loss: 0.305820107460022 G Loss: 0.31591394543647766\n",
      "Epoch 47 - D Loss: 0.5123885273933411 G Loss: 0.16573619842529297\n",
      "Epoch 47 - D Loss: 0.40409278869628906 G Loss: 0.191285640001297\n",
      "Epoch 47 - D Loss: 0.3104661703109741 G Loss: 0.1556951403617859\n",
      "Epoch 47 - D Loss: 0.40313252806663513 G Loss: 0.2827776074409485\n",
      "Epoch 47 - D Loss: 0.3699699640274048 G Loss: 0.2559056580066681\n",
      "Epoch 47 - D Loss: 0.4560520648956299 G Loss: 0.3193492293357849\n",
      "Epoch 47 - D Loss: 0.21977609395980835 G Loss: 0.13408949971199036\n",
      "Epoch 47 - D Loss: 0.4019240438938141 G Loss: 0.09762928634881973\n",
      "Epoch 47 - D Loss: 0.42330029606819153 G Loss: 0.09143510460853577\n",
      "Epoch 47 - D Loss: 0.4098166823387146 G Loss: 0.23488149046897888\n",
      "Epoch 47 - D Loss: 0.3506559133529663 G Loss: 0.14231912791728973\n",
      "Epoch 48 - D Loss: 0.44385427236557007 G Loss: 0.24037304520606995\n",
      "Epoch 48 - D Loss: 0.30873697996139526 G Loss: 0.17872464656829834\n",
      "Epoch 48 - D Loss: 0.4538455307483673 G Loss: 0.21446636319160461\n",
      "Epoch 48 - D Loss: 0.41741740703582764 G Loss: 0.20536667108535767\n",
      "Epoch 48 - D Loss: 0.49774402379989624 G Loss: 0.23039162158966064\n",
      "Epoch 48 - D Loss: 0.4678736925125122 G Loss: 0.19344714283943176\n",
      "Epoch 48 - D Loss: 0.3530628979206085 G Loss: 0.12180189788341522\n",
      "Epoch 48 - D Loss: 0.3752431571483612 G Loss: 0.1603783667087555\n",
      "Epoch 48 - D Loss: 0.5201598405838013 G Loss: 0.1772923767566681\n",
      "Epoch 48 - D Loss: 0.33432134985923767 G Loss: 0.2335301637649536\n",
      "Epoch 48 - D Loss: 0.5222707986831665 G Loss: 0.09989296644926071\n",
      "Epoch 48 - D Loss: 0.23367571830749512 G Loss: 0.13554593920707703\n",
      "Epoch 48 - D Loss: 0.5829142332077026 G Loss: 0.24210739135742188\n",
      "Epoch 48 - D Loss: 0.32070106267929077 G Loss: 0.13471081852912903\n",
      "Epoch 48 - D Loss: 0.44935107231140137 G Loss: 0.12474991381168365\n",
      "Epoch 48 - D Loss: 0.43166202306747437 G Loss: 0.18903863430023193\n",
      "Epoch 48 - D Loss: 0.37326014041900635 G Loss: 0.2503455877304077\n",
      "Epoch 48 - D Loss: 0.30363017320632935 G Loss: 0.20839467644691467\n",
      "Epoch 48 - D Loss: 0.3094448149204254 G Loss: 0.2514748275279999\n",
      "Epoch 48 - D Loss: 0.3476051092147827 G Loss: 0.21192589402198792\n",
      "Epoch 48 - D Loss: 0.3398972153663635 G Loss: 0.11681829392910004\n",
      "Epoch 48 - D Loss: 0.39235901832580566 G Loss: 0.23844030499458313\n",
      "Epoch 48 - D Loss: 0.46280398964881897 G Loss: 0.22270822525024414\n",
      "Epoch 48 - D Loss: 0.4387776851654053 G Loss: 0.25838562846183777\n",
      "Epoch 48 - D Loss: 0.516182005405426 G Loss: 0.2353949248790741\n",
      "Epoch 49 - D Loss: 0.4192294776439667 G Loss: 0.18586313724517822\n",
      "Epoch 49 - D Loss: 0.3458501994609833 G Loss: 0.2909182906150818\n",
      "Epoch 49 - D Loss: 0.519805908203125 G Loss: 0.10269187390804291\n",
      "Epoch 49 - D Loss: 0.40385693311691284 G Loss: 0.07847818732261658\n",
      "Epoch 49 - D Loss: 0.42611992359161377 G Loss: 0.2581726908683777\n",
      "Epoch 49 - D Loss: 0.4318774938583374 G Loss: 0.18475309014320374\n",
      "Epoch 49 - D Loss: 0.49427342414855957 G Loss: 0.1778930127620697\n",
      "Epoch 49 - D Loss: 0.2930241823196411 G Loss: 0.2700294852256775\n",
      "Epoch 49 - D Loss: 0.3965611755847931 G Loss: 0.09678259491920471\n",
      "Epoch 49 - D Loss: 0.48253074288368225 G Loss: 0.15693017840385437\n",
      "Epoch 49 - D Loss: 0.3887583017349243 G Loss: 0.15957152843475342\n",
      "Epoch 49 - D Loss: 0.31601545214653015 G Loss: 0.3017776608467102\n",
      "Epoch 49 - D Loss: 0.5013530254364014 G Loss: 0.15394213795661926\n",
      "Epoch 49 - D Loss: 0.471203088760376 G Loss: 0.09016335010528564\n",
      "Epoch 49 - D Loss: 0.4492546319961548 G Loss: 0.13478170335292816\n",
      "Epoch 49 - D Loss: 0.2760718762874603 G Loss: 0.24848587810993195\n",
      "Epoch 49 - D Loss: 0.43499547243118286 G Loss: 0.2687767744064331\n",
      "Epoch 49 - D Loss: 0.3878171443939209 G Loss: 0.17222443222999573\n",
      "Epoch 49 - D Loss: 0.5171345472335815 G Loss: 0.12249274551868439\n",
      "Epoch 49 - D Loss: 0.47961845993995667 G Loss: 0.2107873409986496\n",
      "Epoch 49 - D Loss: 0.2712984085083008 G Loss: 0.24106526374816895\n",
      "Epoch 49 - D Loss: 0.33130109310150146 G Loss: 0.10611599683761597\n",
      "Epoch 49 - D Loss: 0.3721158802509308 G Loss: 0.24247229099273682\n",
      "Epoch 49 - D Loss: 0.39484596252441406 G Loss: 0.1461721658706665\n",
      "Epoch 49 - D Loss: 0.29652222990989685 G Loss: 0.11911620199680328\n",
      "Epoch 50 - D Loss: 0.4144657850265503 G Loss: 0.13345953822135925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m      3\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m loss_real\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m x_gen \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m prob \u001b[38;5;241m=\u001b[39m discriminator(x_gen)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     15\u001b[0m truth \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m100\u001b[39m,(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mDCGAN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm1(x)\n\u001b[0;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm2(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# To see tensorboard results runt he following:\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nucleaise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
