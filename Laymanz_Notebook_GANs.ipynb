{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laymanz Notebooks: Generative Adversarial Networks\n",
    "Author: Ambrose Ling\n",
    "\n",
    "**What is this notebook about?**\n",
    "\n",
    "In this notebook, we will go over some of the most fundamental ideas behind General Adversarial Networks, how they work and why they have been a major advancement in the field of computer vision and generative artifical intelligence. We hope that you can walk away capable of building your own GAN framework along with training your own model from scratch and understanding some of the core ideas that are trending in this field of research.\n",
    "\n",
    "**What do I need to set up my environment?**\n",
    "\n",
    "All of our notebooks will only use numpy, pytorch, matplotlib for visualizations. We will not use any other third-party libraries for model development, optimization or anything like that.\n",
    "\n",
    "**How is this notebook structured?**\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "\n",
    "**Covered papers in this notebook**\n",
    "\n",
    "(will do after finishing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Generative Adversarial Network?\n",
    "\n",
    "A Generative Adversarial Network (GAN), is a generative model. It aims to learn the distribution of data through an adversarial process. Meaning that the model is in adversary with another (in competition with another).\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "In the GAN framework, you have 2 components:\n",
    "\n",
    "1. The Generator\n",
    "- Its goal is to generate realistic/synthetic images similar to the ones in our dataset. This model is trying to fit our real data distribution.\n",
    "We represent this generator as $G(z,\\theta)$, the generator also defines a mapping from input latent noise to data space.\n",
    "- It recevies noise as input and tries to output a result close to data\n",
    "- **Intuition**: Think of the generator as the counterfeits, they are trying to generate fake money (as realistic as possible) to fool the police (the discriminator).\n",
    "\n",
    "2. The Discriminator\n",
    "- Its goal is to determine if its input comes from the training dataset or from the generator. \n",
    "- More specifically it determines whether a sample comes from the data distribution or the generator distribution\n",
    "- **Intuition**: Think of the discriminator as the police, they are trying to determine if the money they see is fake or real.\n",
    "\n",
    "Some math notation:\n",
    "- $p_{data}(x)$: data distribution\n",
    "- $p_{g}(x)$: generator distribution\n",
    "- $D$: discriminator\n",
    "- $G$: generator\n",
    "- $z$: latent noise variable\n",
    "\n",
    "\n",
    "### How do we train a GAN ?\n",
    "\n",
    "The training objective:\n",
    "$$\n",
    "min_G max_D V(D,G) = E_{x \\sim p_{data}(x)}[log(D(x))] + E_{z \\sim p_z(z)}[log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "**What is this telling us?**\n",
    "- We are training the **discriminator D** to maximize the following expression (maximize the probability that D assigns the correct label to the sample)\n",
    "- We are training the **generator G** to minimize the expression (minimize the probability that D assigns the correct label to the sample, the generator wants to fool the discriminiator D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we measure the similarity / difference between 2 probability distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence\n",
    "KL divergence measures how one probability distribution $p$ diverges from a second expected probability distribution.\n",
    "\n",
    "$$\n",
    "D_{KL} = \\int_x p(x) log(\\frac{p(x)}{q(x)}) dx\n",
    "$$\n",
    "\n",
    "**Some nice properties of the KL divergence**:\n",
    "- KL divergence abhors regions where $q(x)$ has non-null mass and $p(x)$ has null mass. This is useful when you are trying to approximate a complex (intractable) distribution $q(x)$ with a tractable distribution $p(x)$.\n",
    "- KL divergence is always non-negative $D_{KL}(P||Q) = 0$ iff $p(x) == q(x)$\n",
    "\n",
    "**Challenges with using the KL divergence**:\n",
    "- Dependence on support: (support is the set of points where probabilty is nonzero or $P > 0$, or the subset of the domain where elements are **not** mapped to zero.). In order to have a defined KL divergence, $support(P) \\subset support(Q)$ and it means that $D_{KL}(P||Q)$ is finite.\n",
    "- Asymetry: $D(P||Q) \\neq D(Q||P)$, these are different operations. If $q(x) >>> 0$ and $p(x) ~ 0$, $q$ has a very small effect on the divergence., wont be a good measure when you have 2 equally important distributions\n",
    "\n",
    "**NOTE:**\n",
    "KL Divergence is not a metric proper.\n",
    "https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence \n",
    "\n",
    "### Jensen Shannon Divergence\n",
    "Jensen Shannon Divergence als measures how one probability distribution $p$ diverges from a second expected probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Div Loss: 0.9689055681228638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/4nl6v6yx5xbfhw761z9zmbv40000gn/T/ipykernel_15851/4236478263.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.log_softmax(p)\n",
      "/var/folders/0l/4nl6v6yx5xbfhw761z9zmbv40000gn/T/ipykernel_15851/4236478263.py:10: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.log_softmax(q)\n"
     ]
    }
   ],
   "source": [
    "# KL divergence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "p = torch.randn(1,100)\n",
    "q = torch.randn(1,100)\n",
    "\n",
    "# Softmax\n",
    "# exp_sum = p.exp().sum()\n",
    "# p = p.exp() / exp_sum\n",
    "\n",
    "# We get a probabiltiy distribution\n",
    "p = F.log_softmax(p)\n",
    "q = F.log_softmax(q)\n",
    "\n",
    "kl_div = p.exp() * (p - q)\n",
    "\n",
    "#Reduce it to a loss value for backprop\n",
    "loss = kl_div.sum() / p.shape[0]\n",
    "\n",
    "print(f\"KL Div Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "tensor([[[-0.7412, -0.7882, -0.8039,  ..., -0.2157, -0.2471, -0.3255],\n",
      "         [-0.5922, -0.5216, -0.7176,  ..., -0.3333, -0.3804, -0.3882],\n",
      "         [-0.6784, -0.6157, -0.8588,  ..., -0.3412, -0.3020, -0.2471],\n",
      "         ...,\n",
      "         [ 0.2706,  0.7725,  0.8588,  ..., -0.4353, -0.4039, -0.4039],\n",
      "         [ 0.2314,  0.7490,  0.3490,  ..., -0.4275, -0.4039, -0.4039],\n",
      "         [ 0.7647,  0.8824,  0.2392,  ..., -0.3647, -0.4118, -0.3961]],\n",
      "\n",
      "        [[-0.7098, -0.7569, -0.7725,  ..., -0.1373, -0.1686, -0.2471],\n",
      "         [-0.5608, -0.4902, -0.6863,  ..., -0.2549, -0.3020, -0.3098],\n",
      "         [-0.6471, -0.5843, -0.8275,  ..., -0.2627, -0.2235, -0.1686],\n",
      "         ...,\n",
      "         [-0.3882,  0.1686,  0.6000,  ..., -0.3647, -0.3333, -0.3333],\n",
      "         [-0.2392,  0.3020,  0.2392,  ..., -0.3647, -0.3333, -0.3333],\n",
      "         [ 0.2078,  0.5373,  0.2000,  ..., -0.3176, -0.3569, -0.3412]],\n",
      "\n",
      "        [[-0.8431, -0.8980, -0.9137,  ..., -0.4039, -0.4431, -0.5216],\n",
      "         [-0.6863, -0.6078, -0.7961,  ..., -0.5294, -0.5765, -0.5843],\n",
      "         [-0.7647, -0.6941, -0.9294,  ..., -0.5373, -0.4980, -0.4431],\n",
      "         ...,\n",
      "         [-0.3882, -0.0667,  0.2706,  ..., -0.7333, -0.6941, -0.6784],\n",
      "         [-0.3725,  0.0039,  0.0275,  ..., -0.7098, -0.7020, -0.6941],\n",
      "         [ 0.0039,  0.1686,  0.0196,  ..., -0.5922, -0.6863, -0.6784]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "from torchvision.io import read_image\n",
    "root = \"/Users/ambroseling/Desktop/carly-dataset\"\n",
    "def Preprocess(tensor: torch.Tensor):\n",
    "    tensor = tensor.float()/127.5 - 1.0\n",
    "    tensor = tensor[:3]\n",
    "    return tensor\n",
    "transforms = v2.Compose([Preprocess])\n",
    "carly_dataset = torchvision.datasets.DatasetFolder(root,loader = read_image,transform= transforms,extensions=['png'])\n",
    "print(carly_dataset[10][0].shape)\n",
    "print(carly_dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Input: 256 x 256\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__inti__()\n",
    "        self.conv1_down = nn.Conv2d(3,12,3,stride=1)\n",
    "        self.conv2_down = nn.Conv2d(12,60,3,stride=1)\n",
    "        self.conv3_down = nn.Conv2d(60,120,5,stride= 2)\n",
    "        self.avg_pool_1 = nn.AvgPool2d(3,1)\n",
    "        self.avg_pool_2 = nn.AvgPool2d(3,1)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.conv1_up = nn.ConvTranspose2d()\n",
    "        self.conv2_up = nn.ConvTranspose2d()\n",
    "        self.conv3_up = nn.ConvTranspose2d()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an optimizer for the generator\n",
    "\n",
    "optimizer_g = torch.optim.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d()\n",
    "        self.conv2 = nn.Conv2d()\n",
    "        self.conv3 = nn.Conv2d()\n",
    "        self.max_pool  = nn.MaxPool2d()\n",
    "        self.linear1 = nn.Linear()\n",
    "        self.linear2 = nn.Linear()\n",
    "        self.linear3 = nn.Linear()\n",
    "        self.sigmoid = nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_d = torch.optim.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nucleaise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
