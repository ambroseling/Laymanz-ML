{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laymanz Notebooks: Generative Adversarial Networks\n",
    "Author: Ambrose Ling\n",
    "\n",
    "**What is this notebook about?**\n",
    "\n",
    "In this notebook, we will go over some of the most fundamental ideas behind General Adversarial Networks, how they work and why they have been a major advancement in the field of computer vision and generative artifical intelligence. We hope that you can walk away capable of building your own GAN framework along with training your own model from scratch and understanding some of the core ideas that are trending in this field of research.\n",
    "\n",
    "**What do I need to set up my environment?**\n",
    "\n",
    "All of our notebooks will only use numpy, pytorch, matplotlib for visualizations. We will not use any other third-party libraries for model development, optimization or anything like that.\n",
    "\n",
    "**How is this notebook structured?**\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\n",
    "\n",
    "**Covered papers in this notebook**\n",
    "\n",
    "(will do after finishing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Generative Adversarial Network? (https://github.com/soumith/ganhacks?tab=readme-ov-file#authors)\n",
    "\n",
    "A Generative Adversarial Network (GAN), is a generative model. It aims to learn the distribution of data through an adversarial process. Meaning that the model is in adversary with another (in competition with another).\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "In the GAN framework, you have 2 components:\n",
    "\n",
    "1. The Generator\n",
    "- Its goal is to generate realistic/synthetic images similar to the ones in our dataset. This model is trying to fit our real data distribution.\n",
    "We represent this generator as $G(z,\\theta)$, the generator also defines a mapping from input latent noise to data space.\n",
    "- It recevies noise as input and tries to output a result close to data\n",
    "- **Intuition**: Think of the generator as the counterfeits, they are trying to generate fake money (as realistic as possible) to fool the police (the discriminator).\n",
    "\n",
    "2. The Discriminator\n",
    "- Its goal is to determine if its input comes from the training dataset or from the generator. \n",
    "- More specifically it determines whether a sample comes from the data distribution or the generator distribution\n",
    "- **Intuition**: Think of the discriminator as the police, they are trying to determine if the money they see is fake or real.\n",
    "\n",
    "Some math notation:\n",
    "- $p_{data}(x)$: data distribution\n",
    "- $p_{g}(x)$: generator distribution\n",
    "- $D$: discriminator\n",
    "- $G$: generator\n",
    "- $z$: latent noise variable\n",
    "\n",
    "\n",
    "### How do we train a GAN ?\n",
    "\n",
    "The training objective:\n",
    "$$\n",
    "min_G max_D V(D,G) = E_{x \\sim p_{data}(x)}[log(D(x))] + E_{z \\sim p_z(z)}[log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "**What is this telling us?**\n",
    "- We are training the **discriminator D** to maximize the following expression (maximize the probability that D assigns the correct label to the sample)\n",
    "- We are training the **generator G** to minimize the expression (minimize the probability that D assigns the correct label to the sample, the generator wants to fool the discriminiator D)\n",
    "\n",
    "\n",
    "**NOTE**:\n",
    "- Conv tranpoes: $o = (i-1) \\times s + k - 2p$\n",
    "\n",
    "**Some stuff about WGANs**:\n",
    "https://arxiv.org/pdf/1701.07875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we measure the similarity / difference between 2 probability distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence\n",
    "KL divergence measures how one probability distribution $p$ diverges from a second expected probability distribution.\n",
    "\n",
    "$$\n",
    "D_{KL} = \\int_x p(x) log(\\frac{p(x)}{q(x)}) dx\n",
    "$$\n",
    "\n",
    "**Some nice properties of the KL divergence**:\n",
    "- KL divergence abhors regions where $q(x)$ has non-null mass and $p(x)$ has null mass. This is useful when you are trying to approximate a complex (intractable) distribution $q(x)$ with a tractable distribution $p(x)$.\n",
    "- KL divergence is always non-negative $D_{KL}(P||Q) = 0$ iff $p(x) == q(x)$\n",
    "\n",
    "**Challenges with using the KL divergence**:\n",
    "- Dependence on support: (support is the set of points where probabilty is nonzero or $P > 0$, or the subset of the domain where elements are **not** mapped to zero.). In order to have a defined KL divergence, $support(P) \\subset support(Q)$ and it means that $D_{KL}(P||Q)$ is finite.\n",
    "- Asymetry: $D(P||Q) \\neq D(Q||P)$, these are different operations. If $q(x) >>> 0$ and $p(x) ~ 0$, $q$ has a very small effect on the divergence., wont be a good measure when you have 2 equally important distributions\n",
    "\n",
    "**NOTE:**\n",
    "KL Divergence is not a metric proper.\n",
    "https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence \n",
    "\n",
    "### Jensen Shannon Divergence\n",
    "Jensen Shannon Divergence als measures how one probability distribution $p$ diverges from a second expected probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Div Loss: 0.9990202188491821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/4nl6v6yx5xbfhw761z9zmbv40000gn/T/ipykernel_18814/2031231504.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.log_softmax(p)\n",
      "/var/folders/0l/4nl6v6yx5xbfhw761z9zmbv40000gn/T/ipykernel_18814/2031231504.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.log_softmax(q)\n"
     ]
    }
   ],
   "source": [
    "# KL divergence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "p = torch.randn(1,100)\n",
    "q = torch.randn(1,100)\n",
    "\n",
    "# Softmax\n",
    "# exp_sum = p.exp().sum()\n",
    "# p = p.exp() / exp_sum\n",
    "\n",
    "# We get a probabiltiy distribution\n",
    "p = F.log_softmax(p)\n",
    "q = F.log_softmax(q)\n",
    "\n",
    "kl_div = p.exp() * (p - q)\n",
    "\n",
    "#Reduce it to a loss value for backprop\n",
    "loss = kl_div.sum() / p.shape[0]\n",
    "\n",
    "print(f\"KL Div Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambroseling/miniforge3/envs/nucleaise/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "tensor([[[-0.6784, -0.7098, -0.6000,  ..., -0.4275, -0.4745, -0.3412],\n",
      "         [-0.6706, -0.5686, -0.2784,  ..., -0.7412, -0.3255, -0.6235],\n",
      "         [-0.6863, -0.2314, -0.5922,  ..., -0.2314, -0.4196, -0.4824],\n",
      "         ...,\n",
      "         [ 0.0196, -0.3961,  0.6157,  ..., -0.4118, -0.4039, -0.4118],\n",
      "         [ 0.5529,  0.0510,  0.3647,  ..., -0.4118, -0.3725, -0.4039],\n",
      "         [ 0.6784,  0.7725,  0.2078,  ...,  0.6392,  0.0275, -0.4196]],\n",
      "\n",
      "        [[-0.6471, -0.6941, -0.5529,  ..., -0.3412, -0.3882, -0.2627],\n",
      "         [-0.6392, -0.5294, -0.2471,  ..., -0.6706, -0.2471, -0.5529],\n",
      "         [-0.6549, -0.2235, -0.5451,  ..., -0.1373, -0.3333, -0.4118],\n",
      "         ...,\n",
      "         [-0.1294, -0.6706,  0.1922,  ..., -0.3490, -0.3333, -0.3412],\n",
      "         [-0.2392,  0.0196, -0.0510,  ..., -0.3569, -0.3020, -0.3333],\n",
      "         [ 0.3255,  0.5294, -0.4275,  ...,  0.5137, -0.0431, -0.3490]],\n",
      "\n",
      "        [[-0.7569, -0.7961, -0.6863,  ..., -0.6078, -0.6471, -0.5373],\n",
      "         [-0.8039, -0.7098, -0.4431,  ..., -0.8588, -0.5216, -0.7647],\n",
      "         [-0.7882, -0.3412, -0.7490,  ..., -0.4902, -0.6314, -0.6471],\n",
      "         ...,\n",
      "         [-0.2000, -0.6706, -0.1529,  ..., -0.7020, -0.6863, -0.6941],\n",
      "         [-0.2941, -0.1294, -0.0667,  ..., -0.6941, -0.6549, -0.6863],\n",
      "         [ 0.0588,  0.1922, -0.3490,  ...,  0.3647, -0.2157, -0.7098]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambroseling/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "from torchvision.io import read_image\n",
    "root = \"/Users/ambroseling/Desktop/carly-dataset\"\n",
    "def Preprocess(tensor: torch.Tensor):\n",
    "    tensor = v2.Resize(64)(tensor)\n",
    "    tensor = tensor.float()/127.5 - 1.0\n",
    "    tensor = tensor[:3]\n",
    "    return tensor\n",
    "transforms = v2.Compose([Preprocess])\n",
    "carly_dataset = torchvision.datasets.DatasetFolder(root,loader = read_image,transform= transforms,extensions=['png'])\n",
    "dataloader = DataLoader(carly_dataset,batch_size=2)\n",
    "print(carly_dataset[10][0].shape)\n",
    "print(carly_dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DCGAN(nn.Module):\n",
    "    '''\n",
    "    Input: (100,)\n",
    "    Output: (64,64) \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.conv_up = nn.ConvTranspose2d(100,1024,4)\n",
    "        self.conv1_up = nn.ConvTranspose2d(1024,512,4,stride=2,padding=1) # (4-1)*2 +4 - 2 = 8\n",
    "        self.batch_norm1 = nn.BatchNorm2d(512)\n",
    "        self.conv2_up = nn.ConvTranspose2d(512,256,4,stride=2,padding=1) # (8-1)*2 + 4 - 2= 16\n",
    "        self.batch_norm2 = nn.BatchNorm2d(256)\n",
    "        self.conv3_up = nn.ConvTranspose2d(256,128,4,stride=2,padding=1) # (16 - 1)*2 +4 -2 = 32\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4_up = nn.ConvTranspose2d(128,3,4,stride=2,padding=1) # (32-1) *2+4-2 = 64\n",
    "        self.batch_norm4 = nn.BatchNorm2d(3)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        x = self.conv_up(x)\n",
    "        x = self.conv1_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.conv2_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.conv3_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.conv4_up(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.h1 = nn.Linear(100,16)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(16)\n",
    "        self.h2 = nn.Linear(16,256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        self.h3 = nn.Linear(256,512)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.h4 = nn.Linear(512,1024)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(1024)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        x = self.h1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.h4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DCGAN()\n",
    "x = torch.randn(1,100,1,1)\n",
    "out =  generator(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an optimizer for the generator\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(),lr=0.002)\n",
    "\n",
    "# Why do we use Adam for the generator?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()# 64,64\n",
    "        self.conv1 = nn.Conv2d(3,64,4,stride=2,padding=1) # 32\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64,128,4,stride=2,padding=1) # 16\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128,256,4,stride=2,padding=1) # 8\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256,512,4,stride=2,padding=1) # 4\n",
    "        self.batch_norm4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512,1,4,stride=1) # floor((64 - 5) /1) +1 = 60\n",
    "        self.leaky_relu = nn.LeakyReLU(0.02)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6898, grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "\n",
    "x = torch.randn(1,3,64,64)\n",
    "out = discriminator(x)\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_d = torch.optim.SGD(discriminator.parameters(),lr=0.002)\n",
    "\n",
    "# Why do we use SGD for the discriminator?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs = 100\n",
    "steps = 1\n",
    "d_loss = []\n",
    "g_loss = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tensorboard to log the loss and val images\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    training_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            for step in range(steps):\n",
    "                x = batch[0]\n",
    "                prob = discriminator(x).squeeze()\n",
    "                truth = (torch.randint(900,1000,(x.shape[0],))/1000).float()\n",
    "                loss_real = loss_fn(prob,truth)\n",
    "                loss_real.backward()\n",
    "\n",
    "                z = torch.randn(x.shape[0],100,1,1)\n",
    "                x_gen = generator(z)\n",
    "                prob = discriminator(x_gen).squeeze()\n",
    "                truth = (torch.randint(0,100,(x.shape[0],))/1000).float()\n",
    "                loss_fake = loss_fn(prob,truth)\n",
    "                loss_fake.backward()\n",
    "\n",
    "                loss = loss_real + loss_fake\n",
    "                d_loss.append(loss)\n",
    "                writer.add_scalar(\"Discriminator Loss/train\", loss, training_step)\n",
    "                optimizer_d.step()\n",
    "                optimizer_d.zero_grad()\n",
    "\n",
    "                if training_step % 5 ==0:\n",
    "                    with torch.no_grad():\n",
    "                        z = torch.randn(1,100,1,1)\n",
    "                        x_gen = generator(z)\n",
    "                        x_rgb = (x_gen*(255/2) +(255/2)).permute(0,2,3,1)[0].round().numpy().astype(np.uint8)\n",
    "                        writer.add_image(\"Generator result\",x_rgb,global_step = training_step,dataformats=\"HWC\")\n",
    "                        # x_rgb.save(\"val.png\")\n",
    "\n",
    "            z = torch.randn(x.shape[0],100,1,1)\n",
    "            x_gen = generator(z)\n",
    "            prob = discriminator(x_gen).squeeze()\n",
    "            truth = (torch.randint(0,100,(x.shape[0],))/1000).float()\n",
    "            loss = loss_fn(prob,truth)\n",
    "            writer.add_scalar(\"Generator Loss/train\", loss, training_step)\n",
    "            g_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer_g.step()\n",
    "            optimizer_g.zero_grad()\n",
    "\n",
    "            print(f\"Epoch {epoch} - D Loss: {d_loss[-1]} G Loss: {g_loss[-1]}\")\n",
    "            training_step +=1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - D Loss: 0.3971705734729767 G Loss: 0.26423323154449463\n",
      "Epoch 0 - D Loss: 0.43171441555023193 G Loss: 0.30197861790657043\n",
      "Epoch 0 - D Loss: 0.2593371570110321 G Loss: 0.11955482512712479\n",
      "Epoch 0 - D Loss: 0.4137299656867981 G Loss: 0.168511301279068\n",
      "Epoch 0 - D Loss: 0.47505325078964233 G Loss: 0.3165205419063568\n",
      "Epoch 0 - D Loss: 0.2644991874694824 G Loss: 0.31334739923477173\n",
      "Epoch 0 - D Loss: 0.3902342915534973 G Loss: 0.22432981431484222\n",
      "Epoch 0 - D Loss: 0.32798030972480774 G Loss: 0.17144706845283508\n",
      "Epoch 0 - D Loss: 0.5607513189315796 G Loss: 0.09795008599758148\n",
      "Epoch 0 - D Loss: 0.4063386619091034 G Loss: 0.1572287380695343\n",
      "Epoch 0 - D Loss: 0.3646537661552429 G Loss: 0.22211939096450806\n",
      "Epoch 0 - D Loss: 0.3750917911529541 G Loss: 0.29652512073516846\n",
      "Epoch 0 - D Loss: 0.30438241362571716 G Loss: 0.21281683444976807\n",
      "Epoch 0 - D Loss: 0.4127422273159027 G Loss: 0.2008722573518753\n",
      "Epoch 0 - D Loss: 0.38680100440979004 G Loss: 0.11734530329704285\n",
      "Epoch 0 - D Loss: 0.43143194913864136 G Loss: 0.2807922065258026\n",
      "Epoch 0 - D Loss: 0.2332053929567337 G Loss: 0.21984602510929108\n",
      "Epoch 0 - D Loss: 0.5309067964553833 G Loss: 0.3289519250392914\n",
      "Epoch 0 - D Loss: 0.37738895416259766 G Loss: 0.17044933140277863\n",
      "Epoch 0 - D Loss: 0.5029677152633667 G Loss: 0.1490347981452942\n",
      "Epoch 0 - D Loss: 0.42619621753692627 G Loss: 0.1870204508304596\n",
      "Epoch 0 - D Loss: 0.4516252279281616 G Loss: 0.2645402252674103\n",
      "Epoch 0 - D Loss: 0.5434210896492004 G Loss: 0.2547759413719177\n",
      "Epoch 0 - D Loss: 0.3518814742565155 G Loss: 0.24133196473121643\n",
      "Epoch 0 - D Loss: 0.15095755457878113 G Loss: 0.13155242800712585\n",
      "Epoch 1 - D Loss: 0.5585168600082397 G Loss: 0.20987577736377716\n",
      "Epoch 1 - D Loss: 0.6618466377258301 G Loss: 0.3262042999267578\n",
      "Epoch 1 - D Loss: 0.4004713296890259 G Loss: 0.16318628191947937\n",
      "Epoch 1 - D Loss: 0.4241544008255005 G Loss: 0.22721529006958008\n",
      "Epoch 1 - D Loss: 0.3545069396495819 G Loss: 0.2765374779701233\n",
      "Epoch 1 - D Loss: 0.48085373640060425 G Loss: 0.14220541715621948\n",
      "Epoch 1 - D Loss: 0.298093318939209 G Loss: 0.21849191188812256\n",
      "Epoch 1 - D Loss: 0.36986470222473145 G Loss: 0.19977882504463196\n",
      "Epoch 1 - D Loss: 0.37970253825187683 G Loss: 0.13602931797504425\n",
      "Epoch 1 - D Loss: 0.3832246661186218 G Loss: 0.28068676590919495\n",
      "Epoch 1 - D Loss: 0.20393456518650055 G Loss: 0.1642991602420807\n",
      "Epoch 1 - D Loss: 0.3360282778739929 G Loss: 0.2503090798854828\n",
      "Epoch 1 - D Loss: 0.5553683042526245 G Loss: 0.2553061246871948\n",
      "Epoch 1 - D Loss: 0.4361926317214966 G Loss: 0.20005689561367035\n",
      "Epoch 1 - D Loss: 0.4862636923789978 G Loss: 0.2417861372232437\n",
      "Epoch 1 - D Loss: 0.3689008355140686 G Loss: 0.28725677728652954\n",
      "Epoch 1 - D Loss: 0.38112640380859375 G Loss: 0.3363255262374878\n",
      "Epoch 1 - D Loss: 0.523501455783844 G Loss: 0.13864926993846893\n",
      "Epoch 1 - D Loss: 0.3493976294994354 G Loss: 0.09205399453639984\n",
      "Epoch 1 - D Loss: 0.31767719984054565 G Loss: 0.1826193630695343\n",
      "Epoch 1 - D Loss: 0.3255304992198944 G Loss: 0.13360492885112762\n",
      "Epoch 1 - D Loss: 0.4724319279193878 G Loss: 0.19008873403072357\n",
      "Epoch 1 - D Loss: 0.39516717195510864 G Loss: 0.16581721603870392\n",
      "Epoch 1 - D Loss: 0.30469202995300293 G Loss: 0.14531156420707703\n",
      "Epoch 1 - D Loss: 0.47499486804008484 G Loss: 0.1586551070213318\n",
      "Epoch 2 - D Loss: 0.2775993347167969 G Loss: 0.15931721031665802\n",
      "Epoch 2 - D Loss: 0.3869611620903015 G Loss: 0.20729467272758484\n",
      "Epoch 2 - D Loss: 0.37625372409820557 G Loss: 0.15450775623321533\n",
      "Epoch 2 - D Loss: 0.21142487227916718 G Loss: 0.3050467073917389\n",
      "Epoch 2 - D Loss: 0.211554616689682 G Loss: 0.2971488833427429\n",
      "Epoch 2 - D Loss: 0.5659773349761963 G Loss: 0.220287024974823\n",
      "Epoch 2 - D Loss: 0.28798145055770874 G Loss: 0.33557942509651184\n",
      "Epoch 2 - D Loss: 0.48148852586746216 G Loss: 0.231233149766922\n",
      "Epoch 2 - D Loss: 0.5050724148750305 G Loss: 0.13432855904102325\n",
      "Epoch 2 - D Loss: 0.46081477403640747 G Loss: 0.12676960229873657\n",
      "Epoch 2 - D Loss: 0.5975115299224854 G Loss: 0.2713354825973511\n",
      "Epoch 2 - D Loss: 0.47481006383895874 G Loss: 0.2439284771680832\n",
      "Epoch 2 - D Loss: 0.32231202721595764 G Loss: 0.283400297164917\n",
      "Epoch 2 - D Loss: 0.2544671595096588 G Loss: 0.16829174757003784\n",
      "Epoch 2 - D Loss: 0.4183194041252136 G Loss: 0.33872148394584656\n",
      "Epoch 2 - D Loss: 0.4440874755382538 G Loss: 0.112761951982975\n",
      "Epoch 2 - D Loss: 0.31012094020843506 G Loss: 0.2229946255683899\n",
      "Epoch 2 - D Loss: 0.39576369524002075 G Loss: 0.23900441825389862\n",
      "Epoch 2 - D Loss: 0.20916083455085754 G Loss: 0.09801704436540604\n",
      "Epoch 2 - D Loss: 0.48105382919311523 G Loss: 0.13363687694072723\n",
      "Epoch 2 - D Loss: 0.5249899625778198 G Loss: 0.26572227478027344\n",
      "Epoch 2 - D Loss: 0.5250224471092224 G Loss: 0.297882616519928\n",
      "Epoch 2 - D Loss: 0.5464538931846619 G Loss: 0.3483384847640991\n",
      "Epoch 2 - D Loss: 0.48208484053611755 G Loss: 0.23693042993545532\n",
      "Epoch 2 - D Loss: 0.5513607263565063 G Loss: 0.22981847822666168\n",
      "Epoch 3 - D Loss: 0.3797288239002228 G Loss: 0.16020944714546204\n",
      "Epoch 3 - D Loss: 0.45483025908470154 G Loss: 0.20346316695213318\n",
      "Epoch 3 - D Loss: 0.31872841715812683 G Loss: 0.1640092432498932\n",
      "Epoch 3 - D Loss: 0.40965598821640015 G Loss: 0.22307349741458893\n",
      "Epoch 3 - D Loss: 0.4130266606807709 G Loss: 0.1195659264922142\n",
      "Epoch 3 - D Loss: 0.43000733852386475 G Loss: 0.22003327310085297\n",
      "Epoch 3 - D Loss: 0.3948266804218292 G Loss: 0.22023838758468628\n",
      "Epoch 3 - D Loss: 0.4006304144859314 G Loss: 0.17304068803787231\n",
      "Epoch 3 - D Loss: 0.31273213028907776 G Loss: 0.22546802461147308\n",
      "Epoch 3 - D Loss: 0.39953359961509705 G Loss: 0.14594897627830505\n",
      "Epoch 3 - D Loss: 0.40884917974472046 G Loss: 0.20719385147094727\n",
      "Epoch 3 - D Loss: 0.3800589442253113 G Loss: 0.19008851051330566\n",
      "Epoch 3 - D Loss: 0.46197307109832764 G Loss: 0.07327495515346527\n",
      "Epoch 3 - D Loss: 0.38728487491607666 G Loss: 0.32192084193229675\n",
      "Epoch 3 - D Loss: 0.502953290939331 G Loss: 0.22261394560337067\n",
      "Epoch 3 - D Loss: 0.40216881036758423 G Loss: 0.25584661960601807\n",
      "Epoch 3 - D Loss: 0.43274569511413574 G Loss: 0.24746005237102509\n",
      "Epoch 3 - D Loss: 0.49133482575416565 G Loss: 0.2875702977180481\n",
      "Epoch 3 - D Loss: 0.3424298167228699 G Loss: 0.29833346605300903\n",
      "Epoch 3 - D Loss: 0.24028465151786804 G Loss: 0.21512281894683838\n",
      "Epoch 3 - D Loss: 0.4801686108112335 G Loss: 0.19647255539894104\n",
      "Epoch 3 - D Loss: 0.434789776802063 G Loss: 0.33550381660461426\n",
      "Epoch 3 - D Loss: 0.45868948101997375 G Loss: 0.16094204783439636\n",
      "Epoch 3 - D Loss: 0.5005041360855103 G Loss: 0.22394730150699615\n",
      "Epoch 3 - D Loss: 0.4071948528289795 G Loss: 0.14782112836837769\n",
      "Epoch 4 - D Loss: 0.2754293978214264 G Loss: 0.30213797092437744\n",
      "Epoch 4 - D Loss: 0.3862481713294983 G Loss: 0.20241929590702057\n",
      "Epoch 4 - D Loss: 0.5231123566627502 G Loss: 0.2727680802345276\n",
      "Epoch 4 - D Loss: 0.3096368908882141 G Loss: 0.09796111285686493\n",
      "Epoch 4 - D Loss: 0.3022625148296356 G Loss: 0.10539908707141876\n",
      "Epoch 4 - D Loss: 0.34068799018859863 G Loss: 0.20627737045288086\n",
      "Epoch 4 - D Loss: 0.5340890884399414 G Loss: 0.21569818258285522\n",
      "Epoch 4 - D Loss: 0.3301476836204529 G Loss: 0.19345781207084656\n",
      "Epoch 4 - D Loss: 0.5286052227020264 G Loss: 0.2134854793548584\n",
      "Epoch 4 - D Loss: 0.370482861995697 G Loss: 0.2287634164094925\n",
      "Epoch 4 - D Loss: 0.4393444061279297 G Loss: 0.2442069947719574\n",
      "Epoch 4 - D Loss: 0.3285471796989441 G Loss: 0.2506867051124573\n",
      "Epoch 4 - D Loss: 0.343220591545105 G Loss: 0.19533120095729828\n",
      "Epoch 4 - D Loss: 0.510071873664856 G Loss: 0.16692423820495605\n",
      "Epoch 4 - D Loss: 0.16262029111385345 G Loss: 0.13420167565345764\n",
      "Epoch 4 - D Loss: 0.4295015335083008 G Loss: 0.22985482215881348\n",
      "Epoch 4 - D Loss: 0.28550344705581665 G Loss: 0.22705970704555511\n",
      "Epoch 4 - D Loss: 0.5820358991622925 G Loss: 0.13665100932121277\n",
      "Epoch 4 - D Loss: 0.5829881429672241 G Loss: 0.2064317762851715\n",
      "Epoch 4 - D Loss: 0.28376901149749756 G Loss: 0.10436840355396271\n",
      "Epoch 4 - D Loss: 0.48030656576156616 G Loss: 0.22996459901332855\n",
      "Epoch 4 - D Loss: 0.29268211126327515 G Loss: 0.08235876262187958\n",
      "Epoch 4 - D Loss: 0.42342609167099 G Loss: 0.14750416576862335\n",
      "Epoch 4 - D Loss: 0.3627355396747589 G Loss: 0.2264235019683838\n",
      "Epoch 4 - D Loss: 0.3269471228122711 G Loss: 0.24828745424747467\n",
      "Epoch 5 - D Loss: 0.43629196286201477 G Loss: 0.1887321174144745\n",
      "Epoch 5 - D Loss: 0.3296893835067749 G Loss: 0.12421035766601562\n",
      "Epoch 5 - D Loss: 0.5198550224304199 G Loss: 0.2777390480041504\n",
      "Epoch 5 - D Loss: 0.37881842255592346 G Loss: 0.09377703815698624\n",
      "Epoch 5 - D Loss: 0.48422884941101074 G Loss: 0.3274468779563904\n",
      "Epoch 5 - D Loss: 0.3851946294307709 G Loss: 0.2384261190891266\n",
      "Epoch 5 - D Loss: 0.5229202508926392 G Loss: 0.30076998472213745\n",
      "Epoch 5 - D Loss: 0.5261290073394775 G Loss: 0.28670042753219604\n",
      "Epoch 5 - D Loss: 0.4252976179122925 G Loss: 0.16538946330547333\n",
      "Epoch 5 - D Loss: 0.3920254707336426 G Loss: 0.18749086558818817\n",
      "Epoch 5 - D Loss: 0.6266972422599792 G Loss: 0.2703639566898346\n",
      "Epoch 5 - D Loss: 0.45591455698013306 G Loss: 0.16687357425689697\n",
      "Epoch 5 - D Loss: 0.5212388038635254 G Loss: 0.1450500637292862\n",
      "Epoch 5 - D Loss: 0.46712809801101685 G Loss: 0.18360859155654907\n",
      "Epoch 5 - D Loss: 0.26563525199890137 G Loss: 0.19530746340751648\n",
      "Epoch 5 - D Loss: 0.3472393751144409 G Loss: 0.19408831000328064\n",
      "Epoch 5 - D Loss: 0.5225391387939453 G Loss: 0.24339938163757324\n",
      "Epoch 5 - D Loss: 0.42145711183547974 G Loss: 0.2250162959098816\n",
      "Epoch 5 - D Loss: 0.30756711959838867 G Loss: 0.18989719450473785\n",
      "Epoch 5 - D Loss: 0.5329645872116089 G Loss: 0.3157343864440918\n",
      "Epoch 5 - D Loss: 0.4217018485069275 G Loss: 0.2072698473930359\n",
      "Epoch 5 - D Loss: 0.5972263216972351 G Loss: 0.22957506775856018\n",
      "Epoch 5 - D Loss: 0.4726758599281311 G Loss: 0.16314928233623505\n",
      "Epoch 5 - D Loss: 0.4540024995803833 G Loss: 0.19326914846897125\n",
      "Epoch 5 - D Loss: 0.3638837933540344 G Loss: 0.31596124172210693\n",
      "Epoch 6 - D Loss: 0.48407986760139465 G Loss: 0.17048239707946777\n",
      "Epoch 6 - D Loss: 0.4706669747829437 G Loss: 0.1160862073302269\n",
      "Epoch 6 - D Loss: 0.43291008472442627 G Loss: 0.08719022572040558\n",
      "Epoch 6 - D Loss: 0.37191981077194214 G Loss: 0.20845964550971985\n",
      "Epoch 6 - D Loss: 0.4955110549926758 G Loss: 0.26834118366241455\n",
      "Epoch 6 - D Loss: 0.4512144923210144 G Loss: 0.2621071934700012\n",
      "Epoch 6 - D Loss: 0.48286494612693787 G Loss: 0.19901347160339355\n",
      "Epoch 6 - D Loss: 0.48020005226135254 G Loss: 0.307583212852478\n",
      "Epoch 6 - D Loss: 0.35353022813796997 G Loss: 0.2035973072052002\n",
      "Epoch 6 - D Loss: 0.42945706844329834 G Loss: 0.24559833109378815\n",
      "Epoch 6 - D Loss: 0.5301773548126221 G Loss: 0.1738741546869278\n",
      "Epoch 6 - D Loss: 0.31169435381889343 G Loss: 0.16399496793746948\n",
      "Epoch 6 - D Loss: 0.2654189169406891 G Loss: 0.08188983798027039\n",
      "Epoch 6 - D Loss: 0.26369619369506836 G Loss: 0.2337660938501358\n",
      "Epoch 6 - D Loss: 0.29440706968307495 G Loss: 0.20230883359909058\n",
      "Epoch 6 - D Loss: 0.37162667512893677 G Loss: 0.22016611695289612\n",
      "Epoch 6 - D Loss: 0.4858153760433197 G Loss: 0.2616546154022217\n",
      "Epoch 6 - D Loss: 0.35584133863449097 G Loss: 0.32789576053619385\n",
      "Epoch 6 - D Loss: 0.22485214471817017 G Loss: 0.10869771242141724\n",
      "Epoch 6 - D Loss: 0.4404430091381073 G Loss: 0.12911498546600342\n",
      "Epoch 6 - D Loss: 0.28906911611557007 G Loss: 0.27316856384277344\n",
      "Epoch 6 - D Loss: 0.3964846730232239 G Loss: 0.2235601544380188\n",
      "Epoch 6 - D Loss: 0.4900147318840027 G Loss: 0.151099294424057\n",
      "Epoch 6 - D Loss: 0.377111554145813 G Loss: 0.20846696197986603\n",
      "Epoch 6 - D Loss: 0.41939371824264526 G Loss: 0.27369529008865356\n",
      "Epoch 7 - D Loss: 0.5741188526153564 G Loss: 0.2927362322807312\n",
      "Epoch 7 - D Loss: 0.3543323278427124 G Loss: 0.22226761281490326\n",
      "Epoch 7 - D Loss: 0.3301349878311157 G Loss: 0.17266421020030975\n",
      "Epoch 7 - D Loss: 0.3958934545516968 G Loss: 0.24400058388710022\n",
      "Epoch 7 - D Loss: 0.48863011598587036 G Loss: 0.17776837944984436\n",
      "Epoch 7 - D Loss: 0.3454911708831787 G Loss: 0.07033354789018631\n",
      "Epoch 7 - D Loss: 0.47639894485473633 G Loss: 0.14143145084381104\n",
      "Epoch 7 - D Loss: 0.542746365070343 G Loss: 0.2807391583919525\n",
      "Epoch 7 - D Loss: 0.3749178349971771 G Loss: 0.21344497799873352\n",
      "Epoch 7 - D Loss: 0.5804286599159241 G Loss: 0.22036990523338318\n",
      "Epoch 7 - D Loss: 0.566311240196228 G Loss: 0.17153508961200714\n",
      "Epoch 7 - D Loss: 0.26953235268592834 G Loss: 0.2290223091840744\n",
      "Epoch 7 - D Loss: 0.37619549036026 G Loss: 0.19381654262542725\n",
      "Epoch 7 - D Loss: 0.2646559774875641 G Loss: 0.22696605324745178\n",
      "Epoch 7 - D Loss: 0.411857545375824 G Loss: 0.17474110424518585\n",
      "Epoch 7 - D Loss: 0.33821749687194824 G Loss: 0.10346036404371262\n",
      "Epoch 7 - D Loss: 0.29356151819229126 G Loss: 0.15070298314094543\n",
      "Epoch 7 - D Loss: 0.4656926393508911 G Loss: 0.2778666615486145\n",
      "Epoch 7 - D Loss: 0.38250914216041565 G Loss: 0.1782364845275879\n",
      "Epoch 7 - D Loss: 0.3102228045463562 G Loss: 0.1530715823173523\n",
      "Epoch 7 - D Loss: 0.4394633173942566 G Loss: 0.178665891289711\n",
      "Epoch 7 - D Loss: 0.3020060062408447 G Loss: 0.13316908478736877\n",
      "Epoch 7 - D Loss: 0.4504283368587494 G Loss: 0.15788504481315613\n",
      "Epoch 7 - D Loss: 0.33923980593681335 G Loss: 0.24731546640396118\n",
      "Epoch 7 - D Loss: 0.20482493937015533 G Loss: 0.2832203507423401\n",
      "Epoch 8 - D Loss: 0.3598029911518097 G Loss: 0.18792010843753815\n",
      "Epoch 8 - D Loss: 0.3421759009361267 G Loss: 0.1683731973171234\n",
      "Epoch 8 - D Loss: 0.4874892234802246 G Loss: 0.2058446705341339\n",
      "Epoch 8 - D Loss: 0.2747114896774292 G Loss: 0.19109690189361572\n",
      "Epoch 8 - D Loss: 0.3520994782447815 G Loss: 0.14905968308448792\n",
      "Epoch 8 - D Loss: 0.44805803894996643 G Loss: 0.2930339574813843\n",
      "Epoch 8 - D Loss: 0.24474996328353882 G Loss: 0.3197503089904785\n",
      "Epoch 8 - D Loss: 0.3767475485801697 G Loss: 0.24967579543590546\n",
      "Epoch 8 - D Loss: 0.38957804441452026 G Loss: 0.12813326716423035\n",
      "Epoch 8 - D Loss: 0.4020889699459076 G Loss: 0.2811086177825928\n",
      "Epoch 8 - D Loss: 0.2508738934993744 G Loss: 0.17930951714515686\n",
      "Epoch 8 - D Loss: 0.3931348919868469 G Loss: 0.218433678150177\n",
      "Epoch 8 - D Loss: 0.47538894414901733 G Loss: 0.20069801807403564\n",
      "Epoch 8 - D Loss: 0.49420225620269775 G Loss: 0.11606484651565552\n",
      "Epoch 8 - D Loss: 0.510266900062561 G Loss: 0.09734037518501282\n",
      "Epoch 8 - D Loss: 0.3253438472747803 G Loss: 0.12570586800575256\n",
      "Epoch 8 - D Loss: 0.4145324230194092 G Loss: 0.06065504252910614\n",
      "Epoch 8 - D Loss: 0.4743509292602539 G Loss: 0.2297118902206421\n",
      "Epoch 8 - D Loss: 0.24991479516029358 G Loss: 0.13926959037780762\n",
      "Epoch 8 - D Loss: 0.3127689063549042 G Loss: 0.3224560618400574\n",
      "Epoch 8 - D Loss: 0.3929316997528076 G Loss: 0.23780293762683868\n",
      "Epoch 8 - D Loss: 0.45719489455223083 G Loss: 0.15550467371940613\n",
      "Epoch 8 - D Loss: 0.4580358564853668 G Loss: 0.21453127264976501\n",
      "Epoch 8 - D Loss: 0.39325273036956787 G Loss: 0.15046155452728271\n",
      "Epoch 8 - D Loss: 0.4167415499687195 G Loss: 0.11169552057981491\n",
      "Epoch 9 - D Loss: 0.4799329936504364 G Loss: 0.2560344338417053\n",
      "Epoch 9 - D Loss: 0.35350918769836426 G Loss: 0.171571284532547\n",
      "Epoch 9 - D Loss: 0.4039599895477295 G Loss: 0.11106890439987183\n",
      "Epoch 9 - D Loss: 0.4241722524166107 G Loss: 0.21348139643669128\n",
      "Epoch 9 - D Loss: 0.3187147080898285 G Loss: 0.29946452379226685\n",
      "Epoch 9 - D Loss: 0.47699034214019775 G Loss: 0.17395225167274475\n",
      "Epoch 9 - D Loss: 0.3987816274166107 G Loss: 0.20933547616004944\n",
      "Epoch 9 - D Loss: 0.35548627376556396 G Loss: 0.18587428331375122\n",
      "Epoch 9 - D Loss: 0.4267140030860901 G Loss: 0.20961633324623108\n",
      "Epoch 9 - D Loss: 0.44698822498321533 G Loss: 0.13179020583629608\n",
      "Epoch 9 - D Loss: 0.3452412784099579 G Loss: 0.12640011310577393\n",
      "Epoch 9 - D Loss: 0.45064428448677063 G Loss: 0.23775538802146912\n",
      "Epoch 9 - D Loss: 0.3591274917125702 G Loss: 0.09323742985725403\n",
      "Epoch 9 - D Loss: 0.2936573326587677 G Loss: 0.2335258275270462\n",
      "Epoch 9 - D Loss: 0.44648444652557373 G Loss: 0.1143292561173439\n",
      "Epoch 9 - D Loss: 0.5084617733955383 G Loss: 0.2773193120956421\n",
      "Epoch 9 - D Loss: 0.2522534728050232 G Loss: 0.281059205532074\n",
      "Epoch 9 - D Loss: 0.32646146416664124 G Loss: 0.20932459831237793\n",
      "Epoch 9 - D Loss: 0.5232409238815308 G Loss: 0.1423969566822052\n",
      "Epoch 9 - D Loss: 0.5191441774368286 G Loss: 0.1921972632408142\n",
      "Epoch 9 - D Loss: 0.3712000250816345 G Loss: 0.19492341578006744\n",
      "Epoch 9 - D Loss: 0.45797935128211975 G Loss: 0.177946075797081\n",
      "Epoch 9 - D Loss: 0.37694334983825684 G Loss: 0.3206899166107178\n",
      "Epoch 9 - D Loss: 0.36879295110702515 G Loss: 0.24108797311782837\n",
      "Epoch 9 - D Loss: 0.2968321740627289 G Loss: 0.16717013716697693\n",
      "Epoch 10 - D Loss: 0.35042160749435425 G Loss: 0.18080365657806396\n",
      "Epoch 10 - D Loss: 0.509668231010437 G Loss: 0.26784437894821167\n",
      "Epoch 10 - D Loss: 0.44093677401542664 G Loss: 0.15139919519424438\n",
      "Epoch 10 - D Loss: 0.4075770974159241 G Loss: 0.10058226436376572\n",
      "Epoch 10 - D Loss: 0.46664005517959595 G Loss: 0.22418108582496643\n",
      "Epoch 10 - D Loss: 0.32042157649993896 G Loss: 0.25381380319595337\n",
      "Epoch 10 - D Loss: 0.33247312903404236 G Loss: 0.1945297122001648\n",
      "Epoch 10 - D Loss: 0.35086703300476074 G Loss: 0.11205653846263885\n",
      "Epoch 10 - D Loss: 0.5239907503128052 G Loss: 0.11011318862438202\n",
      "Epoch 10 - D Loss: 0.3048846423625946 G Loss: 0.10739805549383163\n",
      "Epoch 10 - D Loss: 0.5083912014961243 G Loss: 0.19055482745170593\n",
      "Epoch 10 - D Loss: 0.5494091510772705 G Loss: 0.19066360592842102\n",
      "Epoch 10 - D Loss: 0.4157566428184509 G Loss: 0.2380441129207611\n",
      "Epoch 10 - D Loss: 0.39675432443618774 G Loss: 0.15461969375610352\n",
      "Epoch 10 - D Loss: 0.3771744966506958 G Loss: 0.2732980251312256\n",
      "Epoch 10 - D Loss: 0.4485061466693878 G Loss: 0.10397530347108841\n",
      "Epoch 10 - D Loss: 0.47979265451431274 G Loss: 0.2197411209344864\n",
      "Epoch 10 - D Loss: 0.4128649830818176 G Loss: 0.2864358723163605\n",
      "Epoch 10 - D Loss: 0.40661782026290894 G Loss: 0.22555556893348694\n",
      "Epoch 10 - D Loss: 0.4246063828468323 G Loss: 0.1204005628824234\n",
      "Epoch 10 - D Loss: 0.26817968487739563 G Loss: 0.16792558133602142\n",
      "Epoch 10 - D Loss: 0.45970773696899414 G Loss: 0.06435105949640274\n",
      "Epoch 10 - D Loss: 0.3212411403656006 G Loss: 0.18743574619293213\n",
      "Epoch 10 - D Loss: 0.5168681144714355 G Loss: 0.1975233405828476\n",
      "Epoch 10 - D Loss: 0.2967947721481323 G Loss: 0.18473860621452332\n",
      "Epoch 11 - D Loss: 0.4721193313598633 G Loss: 0.1819756031036377\n",
      "Epoch 11 - D Loss: 0.2824191451072693 G Loss: 0.24208438396453857\n",
      "Epoch 11 - D Loss: 0.4612552523612976 G Loss: 0.23017176985740662\n",
      "Epoch 11 - D Loss: 0.33162224292755127 G Loss: 0.25089603662490845\n",
      "Epoch 11 - D Loss: 0.43675827980041504 G Loss: 0.1454343944787979\n",
      "Epoch 11 - D Loss: 0.3700806498527527 G Loss: 0.2167220562696457\n",
      "Epoch 11 - D Loss: 0.3845524191856384 G Loss: 0.10254228860139847\n",
      "Epoch 11 - D Loss: 0.5035445690155029 G Loss: 0.163214772939682\n",
      "Epoch 11 - D Loss: 0.33523911237716675 G Loss: 0.24640099704265594\n",
      "Epoch 11 - D Loss: 0.4259251654148102 G Loss: 0.1028631404042244\n",
      "Epoch 11 - D Loss: 0.4963560700416565 G Loss: 0.14642445743083954\n",
      "Epoch 11 - D Loss: 0.2650061547756195 G Loss: 0.28427448868751526\n",
      "Epoch 11 - D Loss: 0.3237156867980957 G Loss: 0.21944540739059448\n",
      "Epoch 11 - D Loss: 0.17767557501792908 G Loss: 0.19247829914093018\n",
      "Epoch 11 - D Loss: 0.5314962863922119 G Loss: 0.14254023134708405\n",
      "Epoch 11 - D Loss: 0.4741119146347046 G Loss: 0.2806965708732605\n",
      "Epoch 11 - D Loss: 0.4117927849292755 G Loss: 0.19705194234848022\n",
      "Epoch 11 - D Loss: 0.3571128249168396 G Loss: 0.19649624824523926\n",
      "Epoch 11 - D Loss: 0.293612539768219 G Loss: 0.28628242015838623\n",
      "Epoch 11 - D Loss: 0.46286430954933167 G Loss: 0.22746539115905762\n",
      "Epoch 11 - D Loss: 0.31308016180992126 G Loss: 0.12326432764530182\n",
      "Epoch 11 - D Loss: 0.4602234363555908 G Loss: 0.24020344018936157\n",
      "Epoch 11 - D Loss: 0.46966731548309326 G Loss: 0.24110949039459229\n",
      "Epoch 11 - D Loss: 0.4134565591812134 G Loss: 0.1626971960067749\n",
      "Epoch 11 - D Loss: 0.4881683588027954 G Loss: 0.2782382667064667\n",
      "Epoch 12 - D Loss: 0.40098533034324646 G Loss: 0.2388724535703659\n",
      "Epoch 12 - D Loss: 0.4664248824119568 G Loss: 0.24108731746673584\n",
      "Epoch 12 - D Loss: 0.5169491171836853 G Loss: 0.18034730851650238\n",
      "Epoch 12 - D Loss: 0.40546679496765137 G Loss: 0.14691391587257385\n",
      "Epoch 12 - D Loss: 0.4383716881275177 G Loss: 0.1401938647031784\n",
      "Epoch 12 - D Loss: 0.4655645489692688 G Loss: 0.23351292312145233\n",
      "Epoch 12 - D Loss: 0.4866310954093933 G Loss: 0.2312481850385666\n",
      "Epoch 12 - D Loss: 0.4750298261642456 G Loss: 0.1885717511177063\n",
      "Epoch 12 - D Loss: 0.46784496307373047 G Loss: 0.18715903162956238\n",
      "Epoch 12 - D Loss: 0.40930676460266113 G Loss: 0.15532410144805908\n",
      "Epoch 12 - D Loss: 0.4577004313468933 G Loss: 0.19237178564071655\n",
      "Epoch 12 - D Loss: 0.2274550050497055 G Loss: 0.19896720349788666\n",
      "Epoch 12 - D Loss: 0.26568931341171265 G Loss: 0.2330467253923416\n",
      "Epoch 12 - D Loss: 0.429233193397522 G Loss: 0.34268802404403687\n",
      "Epoch 12 - D Loss: 0.4274064004421234 G Loss: 0.19466623663902283\n",
      "Epoch 12 - D Loss: 0.3252490162849426 G Loss: 0.18476945161819458\n",
      "Epoch 12 - D Loss: 0.36813756823539734 G Loss: 0.09916401654481888\n",
      "Epoch 12 - D Loss: 0.4189625680446625 G Loss: 0.27384263277053833\n",
      "Epoch 12 - D Loss: 0.493527889251709 G Loss: 0.21816301345825195\n",
      "Epoch 12 - D Loss: 0.4855552911758423 G Loss: 0.1834810972213745\n",
      "Epoch 12 - D Loss: 0.4295910596847534 G Loss: 0.1018422394990921\n",
      "Epoch 12 - D Loss: 0.2818121314048767 G Loss: 0.17730922996997833\n",
      "Epoch 12 - D Loss: 0.3787991404533386 G Loss: 0.2934364974498749\n",
      "Epoch 12 - D Loss: 0.4325341582298279 G Loss: 0.2128019630908966\n",
      "Epoch 12 - D Loss: 0.414395809173584 G Loss: 0.17902925610542297\n",
      "Epoch 13 - D Loss: 0.43886426091194153 G Loss: 0.2386757731437683\n",
      "Epoch 13 - D Loss: 0.4516947269439697 G Loss: 0.18170087039470673\n",
      "Epoch 13 - D Loss: 0.5794086456298828 G Loss: 0.24753402173519135\n",
      "Epoch 13 - D Loss: 0.3724518418312073 G Loss: 0.28928136825561523\n",
      "Epoch 13 - D Loss: 0.38938459753990173 G Loss: 0.14239519834518433\n",
      "Epoch 13 - D Loss: 0.504038393497467 G Loss: 0.16636252403259277\n",
      "Epoch 13 - D Loss: 0.5408742427825928 G Loss: 0.19999970495700836\n",
      "Epoch 13 - D Loss: 0.5574932098388672 G Loss: 0.18111391365528107\n",
      "Epoch 13 - D Loss: 0.28563106060028076 G Loss: 0.2018393725156784\n",
      "Epoch 13 - D Loss: 0.4306949973106384 G Loss: 0.2442890703678131\n",
      "Epoch 13 - D Loss: 0.46397003531455994 G Loss: 0.21999037265777588\n",
      "Epoch 13 - D Loss: 0.4427374601364136 G Loss: 0.0827827900648117\n",
      "Epoch 13 - D Loss: 0.6110085248947144 G Loss: 0.271114706993103\n",
      "Epoch 13 - D Loss: 0.3751140832901001 G Loss: 0.24621179699897766\n",
      "Epoch 13 - D Loss: 0.4714643955230713 G Loss: 0.23535193502902985\n",
      "Epoch 13 - D Loss: 0.28732573986053467 G Loss: 0.11969038844108582\n",
      "Epoch 13 - D Loss: 0.39580368995666504 G Loss: 0.209797203540802\n",
      "Epoch 13 - D Loss: 0.3779798448085785 G Loss: 0.2439083456993103\n",
      "Epoch 13 - D Loss: 0.5224704146385193 G Loss: 0.25895559787750244\n",
      "Epoch 13 - D Loss: 0.3975481688976288 G Loss: 0.26922863721847534\n",
      "Epoch 13 - D Loss: 0.454262375831604 G Loss: 0.25019949674606323\n",
      "Epoch 13 - D Loss: 0.3285942077636719 G Loss: 0.18606743216514587\n",
      "Epoch 13 - D Loss: 0.41944241523742676 G Loss: 0.27615615725517273\n",
      "Epoch 13 - D Loss: 0.32028526067733765 G Loss: 0.13995769619941711\n",
      "Epoch 13 - D Loss: 0.5036556124687195 G Loss: 0.19905027747154236\n",
      "Epoch 14 - D Loss: 0.4005606174468994 G Loss: 0.26392388343811035\n",
      "Epoch 14 - D Loss: 0.37354692816734314 G Loss: 0.15179148316383362\n",
      "Epoch 14 - D Loss: 0.32874688506126404 G Loss: 0.23150695860385895\n",
      "Epoch 14 - D Loss: 0.4927940368652344 G Loss: 0.25400346517562866\n",
      "Epoch 14 - D Loss: 0.4606987535953522 G Loss: 0.1637943685054779\n",
      "Epoch 14 - D Loss: 0.5110287666320801 G Loss: 0.23014581203460693\n",
      "Epoch 14 - D Loss: 0.4674602746963501 G Loss: 0.2576408386230469\n",
      "Epoch 14 - D Loss: 0.3871942162513733 G Loss: 0.2979111671447754\n",
      "Epoch 14 - D Loss: 0.3961988091468811 G Loss: 0.2047846019268036\n",
      "Epoch 14 - D Loss: 0.21814818680286407 G Loss: 0.16093850135803223\n",
      "Epoch 14 - D Loss: 0.35622477531433105 G Loss: 0.31977254152297974\n",
      "Epoch 14 - D Loss: 0.3835671544075012 G Loss: 0.2499605119228363\n",
      "Epoch 14 - D Loss: 0.2079077959060669 G Loss: 0.16614465415477753\n",
      "Epoch 14 - D Loss: 0.3819025754928589 G Loss: 0.24678313732147217\n",
      "Epoch 14 - D Loss: 0.39836716651916504 G Loss: 0.2077622413635254\n",
      "Epoch 14 - D Loss: 0.5837606191635132 G Loss: 0.12647508084774017\n",
      "Epoch 14 - D Loss: 0.33749085664749146 G Loss: 0.23289720714092255\n",
      "Epoch 14 - D Loss: 0.28973644971847534 G Loss: 0.05912403762340546\n",
      "Epoch 14 - D Loss: 0.44663843512535095 G Loss: 0.2821866571903229\n",
      "Epoch 14 - D Loss: 0.37486615777015686 G Loss: 0.2433743178844452\n",
      "Epoch 14 - D Loss: 0.40226230025291443 G Loss: 0.2751374840736389\n",
      "Epoch 14 - D Loss: 0.41696304082870483 G Loss: 0.13819573819637299\n",
      "Epoch 14 - D Loss: 0.33283984661102295 G Loss: 0.12472565472126007\n",
      "Epoch 14 - D Loss: 0.4562417268753052 G Loss: 0.18172764778137207\n",
      "Epoch 14 - D Loss: 0.4323498606681824 G Loss: 0.20735011994838715\n",
      "Epoch 15 - D Loss: 0.44914373755455017 G Loss: 0.10691002011299133\n",
      "Epoch 15 - D Loss: 0.5025451183319092 G Loss: 0.13756194710731506\n",
      "Epoch 15 - D Loss: 0.3172745704650879 G Loss: 0.1376531720161438\n",
      "Epoch 15 - D Loss: 0.35370302200317383 G Loss: 0.06880553811788559\n",
      "Epoch 15 - D Loss: 0.41677483916282654 G Loss: 0.2545120120048523\n",
      "Epoch 15 - D Loss: 0.412162721157074 G Loss: 0.16230273246765137\n",
      "Epoch 15 - D Loss: 0.35381460189819336 G Loss: 0.32280433177948\n",
      "Epoch 15 - D Loss: 0.4157786965370178 G Loss: 0.22394771873950958\n",
      "Epoch 15 - D Loss: 0.5489445328712463 G Loss: 0.09984435141086578\n",
      "Epoch 15 - D Loss: 0.4486013352870941 G Loss: 0.14868687093257904\n",
      "Epoch 15 - D Loss: 0.33268338441848755 G Loss: 0.1651751697063446\n",
      "Epoch 15 - D Loss: 0.35233062505722046 G Loss: 0.21355557441711426\n",
      "Epoch 15 - D Loss: 0.38395965099334717 G Loss: 0.2550690770149231\n",
      "Epoch 15 - D Loss: 0.36740776896476746 G Loss: 0.23077093064785004\n",
      "Epoch 15 - D Loss: 0.31370311975479126 G Loss: 0.3108093738555908\n",
      "Epoch 15 - D Loss: 0.48783478140830994 G Loss: 0.18100759387016296\n",
      "Epoch 15 - D Loss: 0.41538938879966736 G Loss: 0.20278429985046387\n",
      "Epoch 15 - D Loss: 0.28155240416526794 G Loss: 0.26562851667404175\n",
      "Epoch 15 - D Loss: 0.4147612452507019 G Loss: 0.14722840487957\n",
      "Epoch 15 - D Loss: 0.3472287356853485 G Loss: 0.08859575539827347\n",
      "Epoch 15 - D Loss: 0.4097304344177246 G Loss: 0.33755743503570557\n",
      "Epoch 15 - D Loss: 0.30503734946250916 G Loss: 0.21227428317070007\n",
      "Epoch 15 - D Loss: 0.5671278238296509 G Loss: 0.13745826482772827\n",
      "Epoch 15 - D Loss: 0.4171191155910492 G Loss: 0.20616652071475983\n",
      "Epoch 15 - D Loss: 0.3820095658302307 G Loss: 0.1386220008134842\n",
      "Epoch 16 - D Loss: 0.3983319103717804 G Loss: 0.25373369455337524\n",
      "Epoch 16 - D Loss: 0.48110878467559814 G Loss: 0.2506997585296631\n",
      "Epoch 16 - D Loss: 0.37545353174209595 G Loss: 0.1766856163740158\n",
      "Epoch 16 - D Loss: 0.33332470059394836 G Loss: 0.0938785970211029\n",
      "Epoch 16 - D Loss: 0.36146020889282227 G Loss: 0.16008085012435913\n",
      "Epoch 16 - D Loss: 0.3891448676586151 G Loss: 0.2709152102470398\n",
      "Epoch 16 - D Loss: 0.4591473937034607 G Loss: 0.2153465896844864\n",
      "Epoch 16 - D Loss: 0.38510677218437195 G Loss: 0.12988387048244476\n",
      "Epoch 16 - D Loss: 0.5277445912361145 G Loss: 0.17140284180641174\n",
      "Epoch 16 - D Loss: 0.5812797546386719 G Loss: 0.15521599352359772\n",
      "Epoch 16 - D Loss: 0.4891452193260193 G Loss: 0.2594098448753357\n",
      "Epoch 16 - D Loss: 0.4986323118209839 G Loss: 0.29538866877555847\n",
      "Epoch 16 - D Loss: 0.2405954748392105 G Loss: 0.1720474660396576\n",
      "Epoch 16 - D Loss: 0.46025294065475464 G Loss: 0.2733251452445984\n",
      "Epoch 16 - D Loss: 0.34070926904678345 G Loss: 0.23380741477012634\n",
      "Epoch 16 - D Loss: 0.5285989046096802 G Loss: 0.07266414165496826\n",
      "Epoch 16 - D Loss: 0.3160899877548218 G Loss: 0.2573815584182739\n",
      "Epoch 16 - D Loss: 0.3839540481567383 G Loss: 0.18217220902442932\n",
      "Epoch 16 - D Loss: 0.4455246031284332 G Loss: 0.2463095784187317\n",
      "Epoch 16 - D Loss: 0.25960782170295715 G Loss: 0.27243250608444214\n",
      "Epoch 16 - D Loss: 0.5563318729400635 G Loss: 0.2460625022649765\n",
      "Epoch 16 - D Loss: 0.4088621437549591 G Loss: 0.21204137802124023\n",
      "Epoch 16 - D Loss: 0.5550932288169861 G Loss: 0.31199023127555847\n",
      "Epoch 16 - D Loss: 0.3776020407676697 G Loss: 0.26829805970191956\n",
      "Epoch 16 - D Loss: 0.36183491349220276 G Loss: 0.19405344128608704\n",
      "Epoch 17 - D Loss: 0.3443174958229065 G Loss: 0.17036333680152893\n",
      "Epoch 17 - D Loss: 0.4906887412071228 G Loss: 0.2721565365791321\n",
      "Epoch 17 - D Loss: 0.446073055267334 G Loss: 0.18214920163154602\n",
      "Epoch 17 - D Loss: 0.2576357424259186 G Loss: 0.19408531486988068\n",
      "Epoch 17 - D Loss: 0.44830694794654846 G Loss: 0.1306319385766983\n",
      "Epoch 17 - D Loss: 0.5416847467422485 G Loss: 0.11330747604370117\n",
      "Epoch 17 - D Loss: 0.3507467806339264 G Loss: 0.190664604306221\n",
      "Epoch 17 - D Loss: 0.41555726528167725 G Loss: 0.1947544515132904\n",
      "Epoch 17 - D Loss: 0.5344745516777039 G Loss: 0.06441791355609894\n",
      "Epoch 17 - D Loss: 0.3816505968570709 G Loss: 0.1821628361940384\n",
      "Epoch 17 - D Loss: 0.6058365702629089 G Loss: 0.18177536129951477\n",
      "Epoch 17 - D Loss: 0.4783130884170532 G Loss: 0.2261374443769455\n",
      "Epoch 17 - D Loss: 0.5798963308334351 G Loss: 0.27947142720222473\n",
      "Epoch 17 - D Loss: 0.37375733256340027 G Loss: 0.2252337485551834\n",
      "Epoch 17 - D Loss: 0.45937246084213257 G Loss: 0.15681201219558716\n",
      "Epoch 17 - D Loss: 0.4032774567604065 G Loss: 0.2210233211517334\n",
      "Epoch 17 - D Loss: 0.35171249508857727 G Loss: 0.2496323436498642\n",
      "Epoch 17 - D Loss: 0.3036426901817322 G Loss: 0.07251095026731491\n",
      "Epoch 17 - D Loss: 0.49236607551574707 G Loss: 0.19485622644424438\n",
      "Epoch 17 - D Loss: 0.49180346727371216 G Loss: 0.22893351316452026\n",
      "Epoch 17 - D Loss: 0.36016422510147095 G Loss: 0.27374792098999023\n",
      "Epoch 17 - D Loss: 0.49582889676094055 G Loss: 0.1594918668270111\n",
      "Epoch 17 - D Loss: 0.43878835439682007 G Loss: 0.28336477279663086\n",
      "Epoch 17 - D Loss: 0.5302689075469971 G Loss: 0.21509072184562683\n",
      "Epoch 17 - D Loss: 0.5450496673583984 G Loss: 0.1958642303943634\n",
      "Epoch 18 - D Loss: 0.5386250615119934 G Loss: 0.17038322985172272\n",
      "Epoch 18 - D Loss: 0.3684435486793518 G Loss: 0.2284129559993744\n",
      "Epoch 18 - D Loss: 0.2860462963581085 G Loss: 0.28619810938835144\n",
      "Epoch 18 - D Loss: 0.3817893862724304 G Loss: 0.17297029495239258\n",
      "Epoch 18 - D Loss: 0.5144284963607788 G Loss: 0.1938885599374771\n",
      "Epoch 18 - D Loss: 0.40236586332321167 G Loss: 0.2802629768848419\n",
      "Epoch 18 - D Loss: 0.5937594175338745 G Loss: 0.18385368585586548\n",
      "Epoch 18 - D Loss: 0.34294646978378296 G Loss: 0.1829415112733841\n",
      "Epoch 18 - D Loss: 0.47966814041137695 G Loss: 0.2182224988937378\n",
      "Epoch 18 - D Loss: 0.4462984502315521 G Loss: 0.1486862301826477\n",
      "Epoch 18 - D Loss: 0.5593186616897583 G Loss: 0.18946364521980286\n",
      "Epoch 18 - D Loss: 0.46283864974975586 G Loss: 0.22826442122459412\n",
      "Epoch 18 - D Loss: 0.5553312301635742 G Loss: 0.24597769975662231\n",
      "Epoch 18 - D Loss: 0.3726274073123932 G Loss: 0.24257102608680725\n",
      "Epoch 18 - D Loss: 0.44695937633514404 G Loss: 0.251220703125\n",
      "Epoch 18 - D Loss: 0.26317912340164185 G Loss: 0.10556623339653015\n",
      "Epoch 18 - D Loss: 0.46150681376457214 G Loss: 0.15730620920658112\n",
      "Epoch 18 - D Loss: 0.36433303356170654 G Loss: 0.2889026701450348\n",
      "Epoch 18 - D Loss: 0.4422183036804199 G Loss: 0.22349026799201965\n",
      "Epoch 18 - D Loss: 0.28677454590797424 G Loss: 0.13380128145217896\n",
      "Epoch 18 - D Loss: 0.38418474793434143 G Loss: 0.22968584299087524\n",
      "Epoch 18 - D Loss: 0.409681499004364 G Loss: 0.16918405890464783\n",
      "Epoch 18 - D Loss: 0.28494346141815186 G Loss: 0.20165807008743286\n",
      "Epoch 18 - D Loss: 0.42479532957077026 G Loss: 0.16654686629772186\n",
      "Epoch 18 - D Loss: 0.3623458743095398 G Loss: 0.2099286913871765\n",
      "Epoch 19 - D Loss: 0.3324151039123535 G Loss: 0.21095284819602966\n",
      "Epoch 19 - D Loss: 0.5926257967948914 G Loss: 0.18395639955997467\n",
      "Epoch 19 - D Loss: 0.45740827918052673 G Loss: 0.21455031633377075\n",
      "Epoch 19 - D Loss: 0.4637135863304138 G Loss: 0.3034028708934784\n",
      "Epoch 19 - D Loss: 0.5074151754379272 G Loss: 0.20389577746391296\n",
      "Epoch 19 - D Loss: 0.34540706872940063 G Loss: 0.30108678340911865\n",
      "Epoch 19 - D Loss: 0.32196149230003357 G Loss: 0.1963471621274948\n",
      "Epoch 19 - D Loss: 0.3559643030166626 G Loss: 0.21614405512809753\n",
      "Epoch 19 - D Loss: 0.44600340723991394 G Loss: 0.22791530191898346\n",
      "Epoch 19 - D Loss: 0.33954107761383057 G Loss: 0.20497727394104004\n",
      "Epoch 19 - D Loss: 0.5524771213531494 G Loss: 0.23060090839862823\n",
      "Epoch 19 - D Loss: 0.6558118462562561 G Loss: 0.2170259952545166\n",
      "Epoch 19 - D Loss: 0.23509186506271362 G Loss: 0.26446759700775146\n",
      "Epoch 19 - D Loss: 0.37489861249923706 G Loss: 0.1445864737033844\n",
      "Epoch 19 - D Loss: 0.4602959156036377 G Loss: 0.24157050251960754\n",
      "Epoch 19 - D Loss: 0.42464542388916016 G Loss: 0.18511845171451569\n",
      "Epoch 19 - D Loss: 0.3392179608345032 G Loss: 0.17630967497825623\n",
      "Epoch 19 - D Loss: 0.42818209528923035 G Loss: 0.13563865423202515\n",
      "Epoch 19 - D Loss: 0.38202065229415894 G Loss: 0.1893429458141327\n",
      "Epoch 19 - D Loss: 0.38546818494796753 G Loss: 0.25028735399246216\n",
      "Epoch 19 - D Loss: 0.32940351963043213 G Loss: 0.33557385206222534\n",
      "Epoch 19 - D Loss: 0.5693907141685486 G Loss: 0.19824548065662384\n",
      "Epoch 19 - D Loss: 0.46454355120658875 G Loss: 0.21652349829673767\n",
      "Epoch 19 - D Loss: 0.3220258951187134 G Loss: 0.23392897844314575\n",
      "Epoch 19 - D Loss: 0.2336636483669281 G Loss: 0.25773516297340393\n",
      "Epoch 20 - D Loss: 0.45589280128479004 G Loss: 0.20832154154777527\n",
      "Epoch 20 - D Loss: 0.41956502199172974 G Loss: 0.2722240686416626\n",
      "Epoch 20 - D Loss: 0.2717047929763794 G Loss: 0.24586087465286255\n",
      "Epoch 20 - D Loss: 0.3272683620452881 G Loss: 0.19618484377861023\n",
      "Epoch 20 - D Loss: 0.49581754207611084 G Loss: 0.1473008096218109\n",
      "Epoch 20 - D Loss: 0.2940182089805603 G Loss: 0.2094106525182724\n",
      "Epoch 20 - D Loss: 0.47771987318992615 G Loss: 0.16251009702682495\n",
      "Epoch 20 - D Loss: 0.43926382064819336 G Loss: 0.12068003416061401\n",
      "Epoch 20 - D Loss: 0.5158892869949341 G Loss: 0.20882949233055115\n",
      "Epoch 20 - D Loss: 0.3262515068054199 G Loss: 0.21732719242572784\n",
      "Epoch 20 - D Loss: 0.28363174200057983 G Loss: 0.1397969126701355\n",
      "Epoch 20 - D Loss: 0.37525278329849243 G Loss: 0.25584810972213745\n",
      "Epoch 20 - D Loss: 0.5003389716148376 G Loss: 0.19877037405967712\n",
      "Epoch 20 - D Loss: 0.3721044659614563 G Loss: 0.1463126391172409\n",
      "Epoch 20 - D Loss: 0.365603506565094 G Loss: 0.23894847929477692\n",
      "Epoch 20 - D Loss: 0.2722998857498169 G Loss: 0.09166425466537476\n",
      "Epoch 20 - D Loss: 0.505776584148407 G Loss: 0.1640857309103012\n",
      "Epoch 20 - D Loss: 0.440456360578537 G Loss: 0.20849259197711945\n",
      "Epoch 20 - D Loss: 0.33559611439704895 G Loss: 0.30224788188934326\n",
      "Epoch 20 - D Loss: 0.4570918679237366 G Loss: 0.2806636691093445\n",
      "Epoch 20 - D Loss: 0.4764362573623657 G Loss: 0.3030318319797516\n",
      "Epoch 20 - D Loss: 0.36119914054870605 G Loss: 0.0953582376241684\n",
      "Epoch 20 - D Loss: 0.25914186239242554 G Loss: 0.2299807071685791\n",
      "Epoch 20 - D Loss: 0.4901660978794098 G Loss: 0.12969213724136353\n",
      "Epoch 20 - D Loss: 0.38659852743148804 G Loss: 0.21996170282363892\n",
      "Epoch 21 - D Loss: 0.4385022222995758 G Loss: 0.10931816697120667\n",
      "Epoch 21 - D Loss: 0.42559361457824707 G Loss: 0.13817596435546875\n",
      "Epoch 21 - D Loss: 0.4158872365951538 G Loss: 0.2503237724304199\n",
      "Epoch 21 - D Loss: 0.5433739423751831 G Loss: 0.26770907640457153\n",
      "Epoch 21 - D Loss: 0.37376904487609863 G Loss: 0.1275312900543213\n",
      "Epoch 21 - D Loss: 0.33765268325805664 G Loss: 0.14865738153457642\n",
      "Epoch 21 - D Loss: 0.5157837271690369 G Loss: 0.18895995616912842\n",
      "Epoch 21 - D Loss: 0.3751950263977051 G Loss: 0.22651147842407227\n",
      "Epoch 21 - D Loss: 0.3623959422111511 G Loss: 0.21137940883636475\n",
      "Epoch 21 - D Loss: 0.46977880597114563 G Loss: 0.22986803948879242\n",
      "Epoch 21 - D Loss: 0.308830589056015 G Loss: 0.24518708884716034\n",
      "Epoch 21 - D Loss: 0.30127477645874023 G Loss: 0.3030356764793396\n",
      "Epoch 21 - D Loss: 0.601495623588562 G Loss: 0.19380071759223938\n",
      "Epoch 21 - D Loss: 0.2408204972743988 G Loss: 0.3077138364315033\n",
      "Epoch 21 - D Loss: 0.44597935676574707 G Loss: 0.2671240568161011\n",
      "Epoch 21 - D Loss: 0.30102241039276123 G Loss: 0.17962364852428436\n",
      "Epoch 21 - D Loss: 0.49238693714141846 G Loss: 0.15911835432052612\n",
      "Epoch 21 - D Loss: 0.34652581810951233 G Loss: 0.24055196344852448\n",
      "Epoch 21 - D Loss: 0.303865909576416 G Loss: 0.15393111109733582\n",
      "Epoch 21 - D Loss: 0.4492855668067932 G Loss: 0.18088987469673157\n",
      "Epoch 21 - D Loss: 0.39602187275886536 G Loss: 0.24088937044143677\n",
      "Epoch 21 - D Loss: 0.4341026246547699 G Loss: 0.17793557047843933\n",
      "Epoch 21 - D Loss: 0.2054961621761322 G Loss: 0.30977416038513184\n",
      "Epoch 21 - D Loss: 0.403012216091156 G Loss: 0.10087177902460098\n",
      "Epoch 21 - D Loss: 0.30412721633911133 G Loss: 0.13311603665351868\n",
      "Epoch 22 - D Loss: 0.3018477261066437 G Loss: 0.10206859558820724\n",
      "Epoch 22 - D Loss: 0.36544495820999146 G Loss: 0.16211120784282684\n",
      "Epoch 22 - D Loss: 0.5013193488121033 G Loss: 0.18795588612556458\n",
      "Epoch 22 - D Loss: 0.40134790539741516 G Loss: 0.22883275151252747\n",
      "Epoch 22 - D Loss: 0.4412032961845398 G Loss: 0.21637211740016937\n",
      "Epoch 22 - D Loss: 0.2766251862049103 G Loss: 0.24087631702423096\n",
      "Epoch 22 - D Loss: 0.3844345510005951 G Loss: 0.214656800031662\n",
      "Epoch 22 - D Loss: 0.4067499041557312 G Loss: 0.2544998526573181\n",
      "Epoch 22 - D Loss: 0.2992071807384491 G Loss: 0.13236881792545319\n",
      "Epoch 22 - D Loss: 0.43328332901000977 G Loss: 0.13353760540485382\n",
      "Epoch 22 - D Loss: 0.22894787788391113 G Loss: 0.264376699924469\n",
      "Epoch 22 - D Loss: 0.33805060386657715 G Loss: 0.1822701096534729\n",
      "Epoch 22 - D Loss: 0.3391028642654419 G Loss: 0.2549667954444885\n",
      "Epoch 22 - D Loss: 0.4054686427116394 G Loss: 0.17417019605636597\n",
      "Epoch 22 - D Loss: 0.3947615623474121 G Loss: 0.3175812363624573\n",
      "Epoch 22 - D Loss: 0.5984399318695068 G Loss: 0.17793932557106018\n",
      "Epoch 22 - D Loss: 0.5018061995506287 G Loss: 0.20047232508659363\n",
      "Epoch 22 - D Loss: 0.33838361501693726 G Loss: 0.30650460720062256\n",
      "Epoch 22 - D Loss: 0.44779330492019653 G Loss: 0.09469429403543472\n",
      "Epoch 22 - D Loss: 0.481501042842865 G Loss: 0.20376111567020416\n",
      "Epoch 22 - D Loss: 0.24551934003829956 G Loss: 0.22369985282421112\n",
      "Epoch 22 - D Loss: 0.3410807251930237 G Loss: 0.2972252368927002\n",
      "Epoch 22 - D Loss: 0.41582316160202026 G Loss: 0.19185304641723633\n",
      "Epoch 22 - D Loss: 0.30947452783584595 G Loss: 0.13665130734443665\n",
      "Epoch 22 - D Loss: 0.4811808168888092 G Loss: 0.1889272928237915\n",
      "Epoch 23 - D Loss: 0.3522632122039795 G Loss: 0.23709186911582947\n",
      "Epoch 23 - D Loss: 0.2353963851928711 G Loss: 0.12159032374620438\n",
      "Epoch 23 - D Loss: 0.38343387842178345 G Loss: 0.17738699913024902\n",
      "Epoch 23 - D Loss: 0.3165140151977539 G Loss: 0.13855589926242828\n",
      "Epoch 23 - D Loss: 0.430838406085968 G Loss: 0.31341689825057983\n",
      "Epoch 23 - D Loss: 0.4844426214694977 G Loss: 0.234686940908432\n",
      "Epoch 23 - D Loss: 0.5392400622367859 G Loss: 0.11888094991445541\n",
      "Epoch 23 - D Loss: 0.42617788910865784 G Loss: 0.193503737449646\n",
      "Epoch 23 - D Loss: 0.2895410656929016 G Loss: 0.2594364881515503\n",
      "Epoch 23 - D Loss: 0.43800610303878784 G Loss: 0.15973539650440216\n",
      "Epoch 23 - D Loss: 0.31733542680740356 G Loss: 0.313809335231781\n",
      "Epoch 23 - D Loss: 0.3617270290851593 G Loss: 0.2172626107931137\n",
      "Epoch 23 - D Loss: 0.416989803314209 G Loss: 0.24145567417144775\n",
      "Epoch 23 - D Loss: 0.34147995710372925 G Loss: 0.24394194781780243\n",
      "Epoch 23 - D Loss: 0.31984326243400574 G Loss: 0.1560591757297516\n",
      "Epoch 23 - D Loss: 0.44108134508132935 G Loss: 0.16290636360645294\n",
      "Epoch 23 - D Loss: 0.3365461826324463 G Loss: 0.28461378812789917\n",
      "Epoch 23 - D Loss: 0.42902684211730957 G Loss: 0.2658017873764038\n",
      "Epoch 23 - D Loss: 0.5160519480705261 G Loss: 0.2234305441379547\n",
      "Epoch 23 - D Loss: 0.3867643475532532 G Loss: 0.14085295796394348\n",
      "Epoch 23 - D Loss: 0.3115884065628052 G Loss: 0.24726265668869019\n",
      "Epoch 23 - D Loss: 0.4742605686187744 G Loss: 0.23755788803100586\n",
      "Epoch 23 - D Loss: 0.4602809250354767 G Loss: 0.21363264322280884\n",
      "Epoch 23 - D Loss: 0.34666919708251953 G Loss: 0.2562219798564911\n",
      "Epoch 23 - D Loss: 0.33327412605285645 G Loss: 0.13268445432186127\n",
      "Epoch 24 - D Loss: 0.3297279179096222 G Loss: 0.14499130845069885\n",
      "Epoch 24 - D Loss: 0.3373975157737732 G Loss: 0.18175795674324036\n",
      "Epoch 24 - D Loss: 0.5180686712265015 G Loss: 0.1549023687839508\n",
      "Epoch 24 - D Loss: 0.4374995529651642 G Loss: 0.06923814862966537\n",
      "Epoch 24 - D Loss: 0.5802631974220276 G Loss: 0.31305810809135437\n",
      "Epoch 24 - D Loss: 0.41646015644073486 G Loss: 0.18923217058181763\n",
      "Epoch 24 - D Loss: 0.48950082063674927 G Loss: 0.23795220255851746\n",
      "Epoch 24 - D Loss: 0.3323037326335907 G Loss: 0.1793108880519867\n",
      "Epoch 24 - D Loss: 0.3465843200683594 G Loss: 0.2665894329547882\n",
      "Epoch 24 - D Loss: 0.5201815366744995 G Loss: 0.12606048583984375\n",
      "Epoch 24 - D Loss: 0.3982652723789215 G Loss: 0.3630785346031189\n",
      "Epoch 24 - D Loss: 0.4406628906726837 G Loss: 0.24194559454917908\n",
      "Epoch 24 - D Loss: 0.613322913646698 G Loss: 0.21419186890125275\n",
      "Epoch 24 - D Loss: 0.5465406179428101 G Loss: 0.21728633344173431\n",
      "Epoch 24 - D Loss: 0.41807037591934204 G Loss: 0.3197384178638458\n",
      "Epoch 24 - D Loss: 0.46096736192703247 G Loss: 0.20320118963718414\n",
      "Epoch 24 - D Loss: 0.34852275252342224 G Loss: 0.2535610795021057\n",
      "Epoch 24 - D Loss: 0.4775415062904358 G Loss: 0.09129033237695694\n",
      "Epoch 24 - D Loss: 0.41895782947540283 G Loss: 0.158293679356575\n",
      "Epoch 24 - D Loss: 0.34412717819213867 G Loss: 0.04234009236097336\n",
      "Epoch 24 - D Loss: 0.5673484206199646 G Loss: 0.2558797001838684\n",
      "Epoch 24 - D Loss: 0.39136940240859985 G Loss: 0.10807886719703674\n",
      "Epoch 24 - D Loss: 0.23148329555988312 G Loss: 0.26103973388671875\n",
      "Epoch 24 - D Loss: 0.3825851082801819 G Loss: 0.0959375873208046\n",
      "Epoch 24 - D Loss: 0.35868915915489197 G Loss: 0.2831672728061676\n",
      "Epoch 25 - D Loss: 0.42276209592819214 G Loss: 0.20237025618553162\n",
      "Epoch 25 - D Loss: 0.41427934169769287 G Loss: 0.2892906963825226\n",
      "Epoch 25 - D Loss: 0.4466768503189087 G Loss: 0.23548313975334167\n",
      "Epoch 25 - D Loss: 0.35593369603157043 G Loss: 0.15841513872146606\n",
      "Epoch 25 - D Loss: 0.2945665121078491 G Loss: 0.13767009973526\n",
      "Epoch 25 - D Loss: 0.2849038541316986 G Loss: 0.17654728889465332\n",
      "Epoch 25 - D Loss: 0.41100847721099854 G Loss: 0.157362163066864\n",
      "Epoch 25 - D Loss: 0.5157338380813599 G Loss: 0.11719346046447754\n",
      "Epoch 25 - D Loss: 0.37957608699798584 G Loss: 0.19431465864181519\n",
      "Epoch 25 - D Loss: 0.39396411180496216 G Loss: 0.14299610257148743\n",
      "Epoch 25 - D Loss: 0.41619864106178284 G Loss: 0.302038311958313\n",
      "Epoch 25 - D Loss: 0.2894502282142639 G Loss: 0.22430366277694702\n",
      "Epoch 25 - D Loss: 0.47522562742233276 G Loss: 0.20833775401115417\n",
      "Epoch 25 - D Loss: 0.48326271772384644 G Loss: 0.08024605363607407\n",
      "Epoch 25 - D Loss: 0.5414754152297974 G Loss: 0.3145637512207031\n",
      "Epoch 25 - D Loss: 0.34943848848342896 G Loss: 0.0986977070569992\n",
      "Epoch 25 - D Loss: 0.27450668811798096 G Loss: 0.11135673522949219\n",
      "Epoch 25 - D Loss: 0.449616938829422 G Loss: 0.29127299785614014\n",
      "Epoch 25 - D Loss: 0.435028612613678 G Loss: 0.2075500190258026\n",
      "Epoch 25 - D Loss: 0.413322776556015 G Loss: 0.23988160490989685\n",
      "Epoch 25 - D Loss: 0.39782485365867615 G Loss: 0.15936818718910217\n",
      "Epoch 25 - D Loss: 0.2935660779476166 G Loss: 0.276671826839447\n",
      "Epoch 25 - D Loss: 0.5043613910675049 G Loss: 0.31511563062667847\n",
      "Epoch 25 - D Loss: 0.4787181317806244 G Loss: 0.17275604605674744\n",
      "Epoch 25 - D Loss: 0.3346596658229828 G Loss: 0.17726151645183563\n",
      "Epoch 26 - D Loss: 0.2602095305919647 G Loss: 0.29658758640289307\n",
      "Epoch 26 - D Loss: 0.20680475234985352 G Loss: 0.17185336351394653\n",
      "Epoch 26 - D Loss: 0.3260514736175537 G Loss: 0.08063454926013947\n",
      "Epoch 26 - D Loss: 0.5071613788604736 G Loss: 0.2981780767440796\n",
      "Epoch 26 - D Loss: 0.34152865409851074 G Loss: 0.155818909406662\n",
      "Epoch 26 - D Loss: 0.2650607228279114 G Loss: 0.14978553354740143\n",
      "Epoch 26 - D Loss: 0.5008065700531006 G Loss: 0.05433967709541321\n",
      "Epoch 26 - D Loss: 0.4017369747161865 G Loss: 0.19674915075302124\n",
      "Epoch 26 - D Loss: 0.46411609649658203 G Loss: 0.24525083601474762\n",
      "Epoch 26 - D Loss: 0.3600630462169647 G Loss: 0.22254323959350586\n",
      "Epoch 26 - D Loss: 0.3992968797683716 G Loss: 0.14122508466243744\n",
      "Epoch 26 - D Loss: 0.3921915292739868 G Loss: 0.20819376409053802\n",
      "Epoch 26 - D Loss: 0.41400375962257385 G Loss: 0.32367873191833496\n",
      "Epoch 26 - D Loss: 0.2256658673286438 G Loss: 0.22341641783714294\n",
      "Epoch 26 - D Loss: 0.48844292759895325 G Loss: 0.22838515043258667\n",
      "Epoch 26 - D Loss: 0.3515799641609192 G Loss: 0.2506867051124573\n",
      "Epoch 26 - D Loss: 0.42984431982040405 G Loss: 0.2167491912841797\n",
      "Epoch 26 - D Loss: 0.41092151403427124 G Loss: 0.2652270197868347\n",
      "Epoch 26 - D Loss: 0.33057302236557007 G Loss: 0.2950727641582489\n",
      "Epoch 26 - D Loss: 0.3910786211490631 G Loss: 0.23854947090148926\n",
      "Epoch 26 - D Loss: 0.5353410840034485 G Loss: 0.22448141872882843\n",
      "Epoch 26 - D Loss: 0.26602494716644287 G Loss: 0.15560398995876312\n",
      "Epoch 26 - D Loss: 0.385378897190094 G Loss: 0.0966411828994751\n",
      "Epoch 26 - D Loss: 0.4135638475418091 G Loss: 0.2145022749900818\n",
      "Epoch 26 - D Loss: 0.41783660650253296 G Loss: 0.19454801082611084\n",
      "Epoch 27 - D Loss: 0.3305469751358032 G Loss: 0.07821978628635406\n",
      "Epoch 27 - D Loss: 0.277115136384964 G Loss: 0.14646883308887482\n",
      "Epoch 27 - D Loss: 0.378800630569458 G Loss: 0.20224349200725555\n",
      "Epoch 27 - D Loss: 0.2932143211364746 G Loss: 0.1756400465965271\n",
      "Epoch 27 - D Loss: 0.495390921831131 G Loss: 0.12025853991508484\n",
      "Epoch 27 - D Loss: 0.2653636038303375 G Loss: 0.2317441999912262\n",
      "Epoch 27 - D Loss: 0.3968947231769562 G Loss: 0.1591651737689972\n",
      "Epoch 27 - D Loss: 0.350249707698822 G Loss: 0.13435989618301392\n",
      "Epoch 27 - D Loss: 0.3551291823387146 G Loss: 0.2377881556749344\n",
      "Epoch 27 - D Loss: 0.2475402057170868 G Loss: 0.1532118320465088\n",
      "Epoch 27 - D Loss: 0.5025160908699036 G Loss: 0.13703350722789764\n",
      "Epoch 27 - D Loss: 0.4816170334815979 G Loss: 0.29953980445861816\n",
      "Epoch 27 - D Loss: 0.274834007024765 G Loss: 0.19757616519927979\n",
      "Epoch 27 - D Loss: 0.4200873374938965 G Loss: 0.19106793403625488\n",
      "Epoch 27 - D Loss: 0.4467363655567169 G Loss: 0.27563971281051636\n",
      "Epoch 27 - D Loss: 0.38984882831573486 G Loss: 0.09866993129253387\n",
      "Epoch 27 - D Loss: 0.3901880979537964 G Loss: 0.21173201501369476\n",
      "Epoch 27 - D Loss: 0.41025078296661377 G Loss: 0.24758583307266235\n",
      "Epoch 27 - D Loss: 0.46063733100891113 G Loss: 0.12364517897367477\n",
      "Epoch 27 - D Loss: 0.3779410719871521 G Loss: 0.2909421920776367\n",
      "Epoch 27 - D Loss: 0.26182636618614197 G Loss: 0.13855984807014465\n",
      "Epoch 27 - D Loss: 0.21348735690116882 G Loss: 0.23613718152046204\n",
      "Epoch 27 - D Loss: 0.5529786348342896 G Loss: 0.2945610284805298\n",
      "Epoch 27 - D Loss: 0.3784111738204956 G Loss: 0.19371344149112701\n",
      "Epoch 27 - D Loss: 0.4135633111000061 G Loss: 0.24042615294456482\n",
      "Epoch 28 - D Loss: 0.4342957139015198 G Loss: 0.21367348730564117\n",
      "Epoch 28 - D Loss: 0.4224783480167389 G Loss: 0.23806847631931305\n",
      "Epoch 28 - D Loss: 0.32014352083206177 G Loss: 0.16695132851600647\n",
      "Epoch 28 - D Loss: 0.5308924913406372 G Loss: 0.2544592022895813\n",
      "Epoch 28 - D Loss: 0.5740630030632019 G Loss: 0.21304401755332947\n",
      "Epoch 28 - D Loss: 0.4094793200492859 G Loss: 0.12716515362262726\n",
      "Epoch 28 - D Loss: 0.41959118843078613 G Loss: 0.21759435534477234\n",
      "Epoch 28 - D Loss: 0.373676598072052 G Loss: 0.18130643665790558\n",
      "Epoch 28 - D Loss: 0.46408611536026 G Loss: 0.23029223084449768\n",
      "Epoch 28 - D Loss: 0.5665508508682251 G Loss: 0.2954931855201721\n",
      "Epoch 28 - D Loss: 0.4959872364997864 G Loss: 0.19516710937023163\n",
      "Epoch 28 - D Loss: 0.4094938039779663 G Loss: 0.1529812514781952\n",
      "Epoch 28 - D Loss: 0.4201039671897888 G Loss: 0.15986967086791992\n",
      "Epoch 28 - D Loss: 0.5314927101135254 G Loss: 0.18653078377246857\n",
      "Epoch 28 - D Loss: 0.44456151127815247 G Loss: 0.1387040913105011\n",
      "Epoch 28 - D Loss: 0.4918466806411743 G Loss: 0.26291024684906006\n",
      "Epoch 28 - D Loss: 0.4550795555114746 G Loss: 0.18366476893424988\n",
      "Epoch 28 - D Loss: 0.3872309923171997 G Loss: 0.1565999686717987\n",
      "Epoch 28 - D Loss: 0.23904263973236084 G Loss: 0.20534466207027435\n",
      "Epoch 28 - D Loss: 0.293342649936676 G Loss: 0.2306266874074936\n",
      "Epoch 28 - D Loss: 0.4366702437400818 G Loss: 0.1936827152967453\n",
      "Epoch 28 - D Loss: 0.5433577299118042 G Loss: 0.16968216001987457\n",
      "Epoch 28 - D Loss: 0.432027131319046 G Loss: 0.15026353299617767\n",
      "Epoch 28 - D Loss: 0.38165393471717834 G Loss: 0.20831599831581116\n",
      "Epoch 28 - D Loss: 0.30612921714782715 G Loss: 0.1836780160665512\n",
      "Epoch 29 - D Loss: 0.417696088552475 G Loss: 0.1054491400718689\n",
      "Epoch 29 - D Loss: 0.48289918899536133 G Loss: 0.24537387490272522\n",
      "Epoch 29 - D Loss: 0.3770313560962677 G Loss: 0.11086507886648178\n",
      "Epoch 29 - D Loss: 0.4405328631401062 G Loss: 0.09577268362045288\n",
      "Epoch 29 - D Loss: 0.5348111987113953 G Loss: 0.232759028673172\n",
      "Epoch 29 - D Loss: 0.4071357846260071 G Loss: 0.20666159689426422\n",
      "Epoch 29 - D Loss: 0.343980610370636 G Loss: 0.28507858514785767\n",
      "Epoch 29 - D Loss: 0.2707894444465637 G Loss: 0.21857944130897522\n",
      "Epoch 29 - D Loss: 0.48710277676582336 G Loss: 0.18226096034049988\n",
      "Epoch 29 - D Loss: 0.4440854787826538 G Loss: 0.2701435685157776\n",
      "Epoch 29 - D Loss: 0.5346725583076477 G Loss: 0.20504817366600037\n",
      "Epoch 29 - D Loss: 0.3789725601673126 G Loss: 0.18159720301628113\n",
      "Epoch 29 - D Loss: 0.300456166267395 G Loss: 0.32589370012283325\n",
      "Epoch 29 - D Loss: 0.278207391500473 G Loss: 0.2088850885629654\n",
      "Epoch 29 - D Loss: 0.4215681552886963 G Loss: 0.23839400708675385\n",
      "Epoch 29 - D Loss: 0.1874520182609558 G Loss: 0.24853859841823578\n",
      "Epoch 29 - D Loss: 0.4566178321838379 G Loss: 0.14999422430992126\n",
      "Epoch 29 - D Loss: 0.4096616208553314 G Loss: 0.21965524554252625\n",
      "Epoch 29 - D Loss: 0.3393062651157379 G Loss: 0.3164733052253723\n",
      "Epoch 29 - D Loss: 0.30010178685188293 G Loss: 0.17369265854358673\n",
      "Epoch 29 - D Loss: 0.4607291519641876 G Loss: 0.15932098031044006\n",
      "Epoch 29 - D Loss: 0.39995598793029785 G Loss: 0.21720916032791138\n",
      "Epoch 29 - D Loss: 0.4656984806060791 G Loss: 0.21337994933128357\n",
      "Epoch 29 - D Loss: 0.434573233127594 G Loss: 0.22740213572978973\n",
      "Epoch 29 - D Loss: 0.45872193574905396 G Loss: 0.21683552861213684\n",
      "Epoch 30 - D Loss: 0.3680184781551361 G Loss: 0.1622197926044464\n",
      "Epoch 30 - D Loss: 0.4429686665534973 G Loss: 0.2345389425754547\n",
      "Epoch 30 - D Loss: 0.34428671002388 G Loss: 0.18342705070972443\n",
      "Epoch 30 - D Loss: 0.5362222790718079 G Loss: 0.14543859660625458\n",
      "Epoch 30 - D Loss: 0.27295657992362976 G Loss: 0.061616092920303345\n",
      "Epoch 30 - D Loss: 0.5335243344306946 G Loss: 0.2664472460746765\n",
      "Epoch 30 - D Loss: 0.4445940852165222 G Loss: 0.10958714783191681\n",
      "Epoch 30 - D Loss: 0.37404733896255493 G Loss: 0.1572188287973404\n",
      "Epoch 30 - D Loss: 0.2479672133922577 G Loss: 0.1948261559009552\n",
      "Epoch 30 - D Loss: 0.34362637996673584 G Loss: 0.29503923654556274\n",
      "Epoch 30 - D Loss: 0.5055722594261169 G Loss: 0.24202196300029755\n",
      "Epoch 30 - D Loss: 0.3628532588481903 G Loss: 0.2960289418697357\n",
      "Epoch 30 - D Loss: 0.3449363708496094 G Loss: 0.21894653141498566\n",
      "Epoch 30 - D Loss: 0.325995534658432 G Loss: 0.16285587847232819\n",
      "Epoch 30 - D Loss: 0.49199631810188293 G Loss: 0.1411413848400116\n",
      "Epoch 30 - D Loss: 0.3655436635017395 G Loss: 0.24971145391464233\n",
      "Epoch 30 - D Loss: 0.4559105932712555 G Loss: 0.23299726843833923\n",
      "Epoch 30 - D Loss: 0.4418151080608368 G Loss: 0.2492356300354004\n",
      "Epoch 30 - D Loss: 0.37114453315734863 G Loss: 0.11558809876441956\n",
      "Epoch 30 - D Loss: 0.5149412751197815 G Loss: 0.1515769064426422\n",
      "Epoch 30 - D Loss: 0.2912440896034241 G Loss: 0.1745012104511261\n",
      "Epoch 30 - D Loss: 0.6098081469535828 G Loss: 0.20649540424346924\n",
      "Epoch 30 - D Loss: 0.4561954140663147 G Loss: 0.22279168665409088\n",
      "Epoch 30 - D Loss: 0.3634280562400818 G Loss: 0.19464820623397827\n",
      "Epoch 30 - D Loss: 0.3818364143371582 G Loss: 0.19796475768089294\n",
      "Epoch 31 - D Loss: 0.3544599115848541 G Loss: 0.23308643698692322\n",
      "Epoch 31 - D Loss: 0.32859402894973755 G Loss: 0.14802789688110352\n",
      "Epoch 31 - D Loss: 0.4772365391254425 G Loss: 0.12008748948574066\n",
      "Epoch 31 - D Loss: 0.3714476227760315 G Loss: 0.15734586119651794\n",
      "Epoch 31 - D Loss: 0.6125576496124268 G Loss: 0.19732852280139923\n",
      "Epoch 31 - D Loss: 0.17521898448467255 G Loss: 0.07266436517238617\n",
      "Epoch 31 - D Loss: 0.37713319063186646 G Loss: 0.16466175019741058\n",
      "Epoch 31 - D Loss: 0.42868953943252563 G Loss: 0.2344273179769516\n",
      "Epoch 31 - D Loss: 0.3282637298107147 G Loss: 0.2301390916109085\n",
      "Epoch 31 - D Loss: 0.33251821994781494 G Loss: 0.2820972800254822\n",
      "Epoch 31 - D Loss: 0.37546855211257935 G Loss: 0.2763223648071289\n",
      "Epoch 31 - D Loss: 0.3394138813018799 G Loss: 0.10970424115657806\n",
      "Epoch 31 - D Loss: 0.3142508864402771 G Loss: 0.26246705651283264\n",
      "Epoch 31 - D Loss: 0.2980644106864929 G Loss: 0.15106070041656494\n",
      "Epoch 31 - D Loss: 0.5179160237312317 G Loss: 0.18860863149166107\n",
      "Epoch 31 - D Loss: 0.48299792408943176 G Loss: 0.2861137390136719\n",
      "Epoch 31 - D Loss: 0.24583669006824493 G Loss: 0.27715522050857544\n",
      "Epoch 31 - D Loss: 0.2632341682910919 G Loss: 0.15241755545139313\n",
      "Epoch 31 - D Loss: 0.3488782048225403 G Loss: 0.1297089159488678\n",
      "Epoch 31 - D Loss: 0.3641653060913086 G Loss: 0.2310398370027542\n",
      "Epoch 31 - D Loss: 0.4774493873119354 G Loss: 0.23575589060783386\n",
      "Epoch 31 - D Loss: 0.3155895471572876 G Loss: 0.22612060606479645\n",
      "Epoch 31 - D Loss: 0.3172624707221985 G Loss: 0.35237032175064087\n",
      "Epoch 31 - D Loss: 0.2816866636276245 G Loss: 0.30713045597076416\n",
      "Epoch 31 - D Loss: 0.3321996033191681 G Loss: 0.1760745644569397\n",
      "Epoch 32 - D Loss: 0.3321452736854553 G Loss: 0.2549554109573364\n",
      "Epoch 32 - D Loss: 0.3838752508163452 G Loss: 0.14227402210235596\n",
      "Epoch 32 - D Loss: 0.5811086893081665 G Loss: 0.1520806849002838\n",
      "Epoch 32 - D Loss: 0.5240766406059265 G Loss: 0.2361830770969391\n",
      "Epoch 32 - D Loss: 0.4046262204647064 G Loss: 0.19171157479286194\n",
      "Epoch 32 - D Loss: 0.38794469833374023 G Loss: 0.1670522391796112\n",
      "Epoch 32 - D Loss: 0.36902764439582825 G Loss: 0.25482845306396484\n",
      "Epoch 32 - D Loss: 0.2874006927013397 G Loss: 0.2219551056623459\n",
      "Epoch 32 - D Loss: 0.45862701535224915 G Loss: 0.1658879518508911\n",
      "Epoch 32 - D Loss: 0.35278448462486267 G Loss: 0.22778809070587158\n",
      "Epoch 32 - D Loss: 0.34840673208236694 G Loss: 0.1239425539970398\n",
      "Epoch 32 - D Loss: 0.41732659935951233 G Loss: 0.2813057601451874\n",
      "Epoch 32 - D Loss: 0.3203197717666626 G Loss: 0.13663692772388458\n",
      "Epoch 32 - D Loss: 0.20940753817558289 G Loss: 0.12975531816482544\n",
      "Epoch 32 - D Loss: 0.2798267900943756 G Loss: 0.2340385615825653\n",
      "Epoch 32 - D Loss: 0.5247869491577148 G Loss: 0.18969938158988953\n",
      "Epoch 32 - D Loss: 0.3312022089958191 G Loss: 0.28251752257347107\n",
      "Epoch 32 - D Loss: 0.502208948135376 G Loss: 0.2503660023212433\n",
      "Epoch 32 - D Loss: 0.2832309603691101 G Loss: 0.1694985330104828\n",
      "Epoch 32 - D Loss: 0.502397358417511 G Loss: 0.20064067840576172\n",
      "Epoch 32 - D Loss: 0.5939382314682007 G Loss: 0.25174692273139954\n",
      "Epoch 32 - D Loss: 0.285347044467926 G Loss: 0.1354036033153534\n",
      "Epoch 32 - D Loss: 0.29798001050949097 G Loss: 0.0773908942937851\n",
      "Epoch 32 - D Loss: 0.47473764419555664 G Loss: 0.0945519208908081\n",
      "Epoch 32 - D Loss: 0.32156407833099365 G Loss: 0.16199736297130585\n",
      "Epoch 33 - D Loss: 0.3856664299964905 G Loss: 0.24550789594650269\n",
      "Epoch 33 - D Loss: 0.5008256435394287 G Loss: 0.22181901335716248\n",
      "Epoch 33 - D Loss: 0.31814032793045044 G Loss: 0.1995069533586502\n",
      "Epoch 33 - D Loss: 0.27997130155563354 G Loss: 0.22989735007286072\n",
      "Epoch 33 - D Loss: 0.4714759886264801 G Loss: 0.2541964054107666\n",
      "Epoch 33 - D Loss: 0.4468521177768707 G Loss: 0.1663292497396469\n",
      "Epoch 33 - D Loss: 0.21187470853328705 G Loss: 0.2296648919582367\n",
      "Epoch 33 - D Loss: 0.2678084969520569 G Loss: 0.29284948110580444\n",
      "Epoch 33 - D Loss: 0.4996587634086609 G Loss: 0.09788019955158234\n",
      "Epoch 33 - D Loss: 0.38603687286376953 G Loss: 0.25969210267066956\n",
      "Epoch 33 - D Loss: 0.43149226903915405 G Loss: 0.20089709758758545\n",
      "Epoch 33 - D Loss: 0.4190494120121002 G Loss: 0.04784826189279556\n",
      "Epoch 33 - D Loss: 0.5204402208328247 G Loss: 0.2416519820690155\n",
      "Epoch 33 - D Loss: 0.37253275513648987 G Loss: 0.1852031797170639\n",
      "Epoch 33 - D Loss: 0.4563465118408203 G Loss: 0.16224901378154755\n",
      "Epoch 33 - D Loss: 0.5005007982254028 G Loss: 0.17825037240982056\n",
      "Epoch 33 - D Loss: 0.364042729139328 G Loss: 0.12117219716310501\n",
      "Epoch 33 - D Loss: 0.27712205052375793 G Loss: 0.09328290820121765\n",
      "Epoch 33 - D Loss: 0.45374804735183716 G Loss: 0.25359854102134705\n",
      "Epoch 33 - D Loss: 0.5113216042518616 G Loss: 0.1577146053314209\n",
      "Epoch 33 - D Loss: 0.3127971291542053 G Loss: 0.3120708763599396\n",
      "Epoch 33 - D Loss: 0.34570974111557007 G Loss: 0.2514330744743347\n",
      "Epoch 33 - D Loss: 0.38319340348243713 G Loss: 0.07149898260831833\n",
      "Epoch 33 - D Loss: 0.35205990076065063 G Loss: 0.25309500098228455\n",
      "Epoch 33 - D Loss: 0.5213989019393921 G Loss: 0.1120111346244812\n",
      "Epoch 34 - D Loss: 0.4566694498062134 G Loss: 0.13762909173965454\n",
      "Epoch 34 - D Loss: 0.41785645484924316 G Loss: 0.20291364192962646\n",
      "Epoch 34 - D Loss: 0.4531540870666504 G Loss: 0.2854379713535309\n",
      "Epoch 34 - D Loss: 0.5191765427589417 G Loss: 0.21732303500175476\n",
      "Epoch 34 - D Loss: 0.22187048196792603 G Loss: 0.06733262538909912\n",
      "Epoch 34 - D Loss: 0.3113998770713806 G Loss: 0.22234311699867249\n",
      "Epoch 34 - D Loss: 0.45652326941490173 G Loss: 0.20113830268383026\n",
      "Epoch 34 - D Loss: 0.45781636238098145 G Loss: 0.23433756828308105\n",
      "Epoch 34 - D Loss: 0.4727290868759155 G Loss: 0.07922567427158356\n",
      "Epoch 34 - D Loss: 0.3321220874786377 G Loss: 0.14763274788856506\n",
      "Epoch 34 - D Loss: 0.48305821418762207 G Loss: 0.2519211769104004\n",
      "Epoch 34 - D Loss: 0.35246461629867554 G Loss: 0.1488688886165619\n",
      "Epoch 34 - D Loss: 0.3239995241165161 G Loss: 0.2441842257976532\n",
      "Epoch 34 - D Loss: 0.26214319467544556 G Loss: 0.2112179696559906\n",
      "Epoch 34 - D Loss: 0.3744134306907654 G Loss: 0.1744864135980606\n",
      "Epoch 34 - D Loss: 0.4489316940307617 G Loss: 0.053625501692295074\n",
      "Epoch 34 - D Loss: 0.25360578298568726 G Loss: 0.17937737703323364\n",
      "Epoch 34 - D Loss: 0.17711251974105835 G Loss: 0.1664784848690033\n",
      "Epoch 34 - D Loss: 0.35192421078681946 G Loss: 0.15848839282989502\n",
      "Epoch 34 - D Loss: 0.4207709729671478 G Loss: 0.3121512830257416\n",
      "Epoch 34 - D Loss: 0.41803815960884094 G Loss: 0.21128585934638977\n",
      "Epoch 34 - D Loss: 0.3713686168193817 G Loss: 0.22070933878421783\n",
      "Epoch 34 - D Loss: 0.5231384038925171 G Loss: 0.20059064030647278\n",
      "Epoch 34 - D Loss: 0.37767529487609863 G Loss: 0.1453661471605301\n",
      "Epoch 34 - D Loss: 0.3506481945514679 G Loss: 0.2123318910598755\n",
      "Epoch 35 - D Loss: 0.2586997449398041 G Loss: 0.2558754086494446\n",
      "Epoch 35 - D Loss: 0.3941427171230316 G Loss: 0.2226947695016861\n",
      "Epoch 35 - D Loss: 0.4896908104419708 G Loss: 0.20152142643928528\n",
      "Epoch 35 - D Loss: 0.2984437942504883 G Loss: 0.21395185589790344\n",
      "Epoch 35 - D Loss: 0.37494003772735596 G Loss: 0.16598540544509888\n",
      "Epoch 35 - D Loss: 0.3742534816265106 G Loss: 0.2542649805545807\n",
      "Epoch 35 - D Loss: 0.7008907794952393 G Loss: 0.2583373188972473\n",
      "Epoch 35 - D Loss: 0.3229803144931793 G Loss: 0.3077920079231262\n",
      "Epoch 35 - D Loss: 0.3846847414970398 G Loss: 0.20081064105033875\n",
      "Epoch 35 - D Loss: 0.2431621104478836 G Loss: 0.21489860117435455\n",
      "Epoch 35 - D Loss: 0.4195547103881836 G Loss: 0.2278108149766922\n",
      "Epoch 35 - D Loss: 0.28690725564956665 G Loss: 0.17851874232292175\n",
      "Epoch 35 - D Loss: 0.4086969494819641 G Loss: 0.19690409302711487\n",
      "Epoch 35 - D Loss: 0.3137059807777405 G Loss: 0.17617028951644897\n",
      "Epoch 35 - D Loss: 0.475063681602478 G Loss: 0.11595577001571655\n",
      "Epoch 35 - D Loss: 0.34096965193748474 G Loss: 0.14479634165763855\n",
      "Epoch 35 - D Loss: 0.49302154779434204 G Loss: 0.2787160575389862\n",
      "Epoch 35 - D Loss: 0.34656697511672974 G Loss: 0.1533089131116867\n",
      "Epoch 35 - D Loss: 0.4056451916694641 G Loss: 0.20689643919467926\n",
      "Epoch 35 - D Loss: 0.5392272472381592 G Loss: 0.08901901543140411\n",
      "Epoch 35 - D Loss: 0.39095205068588257 G Loss: 0.1517283320426941\n",
      "Epoch 35 - D Loss: 0.36455637216567993 G Loss: 0.16200724244117737\n",
      "Epoch 35 - D Loss: 0.4031302332878113 G Loss: 0.1761796474456787\n",
      "Epoch 35 - D Loss: 0.250005304813385 G Loss: 0.1361202448606491\n",
      "Epoch 35 - D Loss: 0.47755444049835205 G Loss: 0.17762288451194763\n",
      "Epoch 36 - D Loss: 0.5238221883773804 G Loss: 0.1461251825094223\n",
      "Epoch 36 - D Loss: 0.2730153501033783 G Loss: 0.2503930628299713\n",
      "Epoch 36 - D Loss: 0.39239391684532166 G Loss: 0.19253723323345184\n",
      "Epoch 36 - D Loss: 0.4757964611053467 G Loss: 0.15754514932632446\n",
      "Epoch 36 - D Loss: 0.5299718379974365 G Loss: 0.1075403094291687\n",
      "Epoch 36 - D Loss: 0.360167920589447 G Loss: 0.3118750751018524\n",
      "Epoch 36 - D Loss: 0.40889984369277954 G Loss: 0.2838553190231323\n",
      "Epoch 36 - D Loss: 0.2669481933116913 G Loss: 0.14200548827648163\n",
      "Epoch 36 - D Loss: 0.5050921440124512 G Loss: 0.29403114318847656\n",
      "Epoch 36 - D Loss: 0.4403238296508789 G Loss: 0.28341346979141235\n",
      "Epoch 36 - D Loss: 0.4462611675262451 G Loss: 0.21576519310474396\n",
      "Epoch 36 - D Loss: 0.4306718111038208 G Loss: 0.13883329927921295\n",
      "Epoch 36 - D Loss: 0.4689585566520691 G Loss: 0.1683247685432434\n",
      "Epoch 36 - D Loss: 0.4517495930194855 G Loss: 0.19716840982437134\n",
      "Epoch 36 - D Loss: 0.6513923406600952 G Loss: 0.21773643791675568\n",
      "Epoch 36 - D Loss: 0.462523877620697 G Loss: 0.26415711641311646\n",
      "Epoch 36 - D Loss: 0.5675333738327026 G Loss: 0.1422273963689804\n",
      "Epoch 36 - D Loss: 0.4213831424713135 G Loss: 0.2168123871088028\n",
      "Epoch 36 - D Loss: 0.4844249486923218 G Loss: 0.2683178782463074\n",
      "Epoch 36 - D Loss: 0.32670947909355164 G Loss: 0.14517270028591156\n",
      "Epoch 36 - D Loss: 0.4962184727191925 G Loss: 0.2030602991580963\n",
      "Epoch 36 - D Loss: 0.3929360508918762 G Loss: 0.29304856061935425\n",
      "Epoch 36 - D Loss: 0.44236576557159424 G Loss: 0.2951078712940216\n",
      "Epoch 36 - D Loss: 0.33270037174224854 G Loss: 0.22228121757507324\n",
      "Epoch 36 - D Loss: 0.3212212324142456 G Loss: 0.24509799480438232\n",
      "Epoch 37 - D Loss: 0.44063258171081543 G Loss: 0.21538665890693665\n",
      "Epoch 37 - D Loss: 0.24621814489364624 G Loss: 0.15533773601055145\n",
      "Epoch 37 - D Loss: 0.47104257345199585 G Loss: 0.10488839447498322\n",
      "Epoch 37 - D Loss: 0.43339458107948303 G Loss: 0.15784910321235657\n",
      "Epoch 37 - D Loss: 0.44383278489112854 G Loss: 0.19583085179328918\n",
      "Epoch 37 - D Loss: 0.48627349734306335 G Loss: 0.2030220925807953\n",
      "Epoch 37 - D Loss: 0.4638995826244354 G Loss: 0.19233399629592896\n",
      "Epoch 37 - D Loss: 0.41742509603500366 G Loss: 0.2724840044975281\n",
      "Epoch 37 - D Loss: 0.4120821952819824 G Loss: 0.2367883026599884\n",
      "Epoch 37 - D Loss: 0.32964855432510376 G Loss: 0.1892642080783844\n",
      "Epoch 37 - D Loss: 0.33234912157058716 G Loss: 0.15982882678508759\n",
      "Epoch 37 - D Loss: 0.33205002546310425 G Loss: 0.2729623317718506\n",
      "Epoch 37 - D Loss: 0.5433791279792786 G Loss: 0.22515127062797546\n",
      "Epoch 37 - D Loss: 0.6207269430160522 G Loss: 0.16831591725349426\n",
      "Epoch 37 - D Loss: 0.37036627531051636 G Loss: 0.19919291138648987\n",
      "Epoch 37 - D Loss: 0.48680436611175537 G Loss: 0.17831972241401672\n",
      "Epoch 37 - D Loss: 0.346881240606308 G Loss: 0.2501611113548279\n",
      "Epoch 37 - D Loss: 0.4217761158943176 G Loss: 0.1617637574672699\n",
      "Epoch 37 - D Loss: 0.34408095479011536 G Loss: 0.23736786842346191\n",
      "Epoch 37 - D Loss: 0.4742935001850128 G Loss: 0.29106491804122925\n",
      "Epoch 37 - D Loss: 0.2909984290599823 G Loss: 0.17340783774852753\n",
      "Epoch 37 - D Loss: 0.43826189637184143 G Loss: 0.1935153752565384\n",
      "Epoch 37 - D Loss: 0.4909985065460205 G Loss: 0.1844945102930069\n",
      "Epoch 37 - D Loss: 0.5099750757217407 G Loss: 0.2035234421491623\n",
      "Epoch 37 - D Loss: 0.5815617442131042 G Loss: 0.2045971155166626\n",
      "Epoch 38 - D Loss: 0.5194234848022461 G Loss: 0.141149640083313\n",
      "Epoch 38 - D Loss: 0.3302896022796631 G Loss: 0.18077455461025238\n",
      "Epoch 38 - D Loss: 0.3179513216018677 G Loss: 0.22336158156394958\n",
      "Epoch 38 - D Loss: 0.37518924474716187 G Loss: 0.24064546823501587\n",
      "Epoch 38 - D Loss: 0.4276059865951538 G Loss: 0.2089240550994873\n",
      "Epoch 38 - D Loss: 0.44487810134887695 G Loss: 0.1525934934616089\n",
      "Epoch 38 - D Loss: 0.44564053416252136 G Loss: 0.23113089799880981\n",
      "Epoch 38 - D Loss: 0.4762279987335205 G Loss: 0.1932830512523651\n",
      "Epoch 38 - D Loss: 0.2957420349121094 G Loss: 0.2362915426492691\n",
      "Epoch 38 - D Loss: 0.36680343747138977 G Loss: 0.2200067937374115\n",
      "Epoch 38 - D Loss: 0.40642353892326355 G Loss: 0.2579476535320282\n",
      "Epoch 38 - D Loss: 0.5434418320655823 G Loss: 0.1698455810546875\n",
      "Epoch 38 - D Loss: 0.3673279881477356 G Loss: 0.18953029811382294\n",
      "Epoch 38 - D Loss: 0.45055896043777466 G Loss: 0.12341385334730148\n",
      "Epoch 38 - D Loss: 0.4531727433204651 G Loss: 0.19670549035072327\n",
      "Epoch 38 - D Loss: 0.4810086488723755 G Loss: 0.1740805208683014\n",
      "Epoch 38 - D Loss: 0.3649253845214844 G Loss: 0.25868064165115356\n",
      "Epoch 38 - D Loss: 0.4774100184440613 G Loss: 0.10639242827892303\n",
      "Epoch 38 - D Loss: 0.4729093909263611 G Loss: 0.18161316215991974\n",
      "Epoch 38 - D Loss: 0.41842120885849 G Loss: 0.1797385811805725\n",
      "Epoch 38 - D Loss: 0.48252424597740173 G Loss: 0.11230164766311646\n",
      "Epoch 38 - D Loss: 0.4814666211605072 G Loss: 0.22346848249435425\n",
      "Epoch 38 - D Loss: 0.4050052762031555 G Loss: 0.280570387840271\n",
      "Epoch 38 - D Loss: 0.3945994973182678 G Loss: 0.2587538957595825\n",
      "Epoch 38 - D Loss: 0.3311433792114258 G Loss: 0.22492675483226776\n",
      "Epoch 39 - D Loss: 0.3313114643096924 G Loss: 0.09089793264865875\n",
      "Epoch 39 - D Loss: 0.29538124799728394 G Loss: 0.2632700204849243\n",
      "Epoch 39 - D Loss: 0.4872540831565857 G Loss: 0.2196732461452484\n",
      "Epoch 39 - D Loss: 0.5322897434234619 G Loss: 0.22115588188171387\n",
      "Epoch 39 - D Loss: 0.4715557396411896 G Loss: 0.12616099417209625\n",
      "Epoch 39 - D Loss: 0.406004399061203 G Loss: 0.26650485396385193\n",
      "Epoch 39 - D Loss: 0.47315946221351624 G Loss: 0.2181057631969452\n",
      "Epoch 39 - D Loss: 0.3535981774330139 G Loss: 0.19405800104141235\n",
      "Epoch 39 - D Loss: 0.43159019947052 G Loss: 0.2800302505493164\n",
      "Epoch 39 - D Loss: 0.5108393430709839 G Loss: 0.10585323721170425\n",
      "Epoch 39 - D Loss: 0.4182921350002289 G Loss: 0.25119656324386597\n",
      "Epoch 39 - D Loss: 0.41820764541625977 G Loss: 0.3151985704898834\n",
      "Epoch 39 - D Loss: 0.4217027425765991 G Loss: 0.2916001081466675\n",
      "Epoch 39 - D Loss: 0.35473957657814026 G Loss: 0.17042076587677002\n",
      "Epoch 39 - D Loss: 0.24260936677455902 G Loss: 0.1265006959438324\n",
      "Epoch 39 - D Loss: 0.4775572121143341 G Loss: 0.18482373654842377\n",
      "Epoch 39 - D Loss: 0.3655945658683777 G Loss: 0.2369251251220703\n",
      "Epoch 39 - D Loss: 0.47320741415023804 G Loss: 0.25581467151641846\n",
      "Epoch 39 - D Loss: 0.2910903990268707 G Loss: 0.13698001205921173\n",
      "Epoch 39 - D Loss: 0.5009217262268066 G Loss: 0.16372191905975342\n",
      "Epoch 39 - D Loss: 0.5310646295547485 G Loss: 0.18235009908676147\n",
      "Epoch 39 - D Loss: 0.4923344850540161 G Loss: 0.10561539232730865\n",
      "Epoch 39 - D Loss: 0.36106961965560913 G Loss: 0.25202858448028564\n",
      "Epoch 39 - D Loss: 0.4063766300678253 G Loss: 0.15175564587116241\n",
      "Epoch 39 - D Loss: 0.3935043513774872 G Loss: 0.14410452544689178\n",
      "Epoch 40 - D Loss: 0.48377057909965515 G Loss: 0.1557425707578659\n",
      "Epoch 40 - D Loss: 0.45195358991622925 G Loss: 0.19090241193771362\n",
      "Epoch 40 - D Loss: 0.4929630756378174 G Loss: 0.27240005135536194\n",
      "Epoch 40 - D Loss: 0.39962661266326904 G Loss: 0.23596662282943726\n",
      "Epoch 40 - D Loss: 0.3291305899620056 G Loss: 0.07433389127254486\n",
      "Epoch 40 - D Loss: 0.546126127243042 G Loss: 0.1646691858768463\n",
      "Epoch 40 - D Loss: 0.3732331395149231 G Loss: 0.27384164929389954\n",
      "Epoch 40 - D Loss: 0.4914598762989044 G Loss: 0.20381063222885132\n",
      "Epoch 40 - D Loss: 0.4120904207229614 G Loss: 0.20679086446762085\n",
      "Epoch 40 - D Loss: 0.4121851921081543 G Loss: 0.1377708464860916\n",
      "Epoch 40 - D Loss: 0.4293537139892578 G Loss: 0.16398701071739197\n",
      "Epoch 40 - D Loss: 0.39760708808898926 G Loss: 0.15679654479026794\n",
      "Epoch 40 - D Loss: 0.44476813077926636 G Loss: 0.24473407864570618\n",
      "Epoch 40 - D Loss: 0.22964820265769958 G Loss: 0.1939750611782074\n",
      "Epoch 40 - D Loss: 0.4427843987941742 G Loss: 0.14673066139221191\n",
      "Epoch 40 - D Loss: 0.298277348279953 G Loss: 0.17880897223949432\n",
      "Epoch 40 - D Loss: 0.37066811323165894 G Loss: 0.21827124059200287\n",
      "Epoch 40 - D Loss: 0.47433775663375854 G Loss: 0.12285638600587845\n",
      "Epoch 40 - D Loss: 0.4967108368873596 G Loss: 0.11435352265834808\n",
      "Epoch 40 - D Loss: 0.5489349961280823 G Loss: 0.16908152401447296\n",
      "Epoch 40 - D Loss: 0.5129536390304565 G Loss: 0.19035711884498596\n",
      "Epoch 40 - D Loss: 0.28924456238746643 G Loss: 0.2872468829154968\n",
      "Epoch 40 - D Loss: 0.393374502658844 G Loss: 0.2655795216560364\n",
      "Epoch 40 - D Loss: 0.37534356117248535 G Loss: 0.25820574164390564\n",
      "Epoch 40 - D Loss: 0.4067710041999817 G Loss: 0.25600147247314453\n",
      "Epoch 41 - D Loss: 0.31203046441078186 G Loss: 0.11310859024524689\n",
      "Epoch 41 - D Loss: 0.4162525534629822 G Loss: 0.168751060962677\n",
      "Epoch 41 - D Loss: 0.4526052474975586 G Loss: 0.27358585596084595\n",
      "Epoch 41 - D Loss: 0.5065715312957764 G Loss: 0.16021820902824402\n",
      "Epoch 41 - D Loss: 0.42831116914749146 G Loss: 0.11264601349830627\n",
      "Epoch 41 - D Loss: 0.46881648898124695 G Loss: 0.20995758473873138\n",
      "Epoch 41 - D Loss: 0.48076003789901733 G Loss: 0.2931373119354248\n",
      "Epoch 41 - D Loss: 0.41244062781333923 G Loss: 0.12344791740179062\n",
      "Epoch 41 - D Loss: 0.50248122215271 G Loss: 0.15178878605365753\n",
      "Epoch 41 - D Loss: 0.3566436767578125 G Loss: 0.1580963134765625\n",
      "Epoch 41 - D Loss: 0.33932605385780334 G Loss: 0.16010890901088715\n",
      "Epoch 41 - D Loss: 0.5847395658493042 G Loss: 0.10568509995937347\n",
      "Epoch 41 - D Loss: 0.5189893841743469 G Loss: 0.25657933950424194\n",
      "Epoch 41 - D Loss: 0.5914881229400635 G Loss: 0.2858724594116211\n",
      "Epoch 41 - D Loss: 0.39908838272094727 G Loss: 0.2570725977420807\n",
      "Epoch 41 - D Loss: 0.2658941149711609 G Loss: 0.11619659513235092\n",
      "Epoch 41 - D Loss: 0.4580320715904236 G Loss: 0.11295409500598907\n",
      "Epoch 41 - D Loss: 0.4556402564048767 G Loss: 0.22884497046470642\n",
      "Epoch 41 - D Loss: 0.3551560044288635 G Loss: 0.20990484952926636\n",
      "Epoch 41 - D Loss: 0.37604251503944397 G Loss: 0.1662966012954712\n",
      "Epoch 41 - D Loss: 0.36958402395248413 G Loss: 0.29973167181015015\n",
      "Epoch 41 - D Loss: 0.49102044105529785 G Loss: 0.29775696992874146\n",
      "Epoch 41 - D Loss: 0.3777204751968384 G Loss: 0.1969309002161026\n",
      "Epoch 41 - D Loss: 0.40047720074653625 G Loss: 0.1154940277338028\n",
      "Epoch 41 - D Loss: 0.24952229857444763 G Loss: 0.1155320256948471\n",
      "Epoch 42 - D Loss: 0.31828033924102783 G Loss: 0.17828711867332458\n",
      "Epoch 42 - D Loss: 0.42098066210746765 G Loss: 0.2263801544904709\n",
      "Epoch 42 - D Loss: 0.37225061655044556 G Loss: 0.19457927346229553\n",
      "Epoch 42 - D Loss: 0.3742425739765167 G Loss: 0.22041946649551392\n",
      "Epoch 42 - D Loss: 0.22628922760486603 G Loss: 0.07088477164506912\n",
      "Epoch 42 - D Loss: 0.2955358922481537 G Loss: 0.25733333826065063\n",
      "Epoch 42 - D Loss: 0.39914464950561523 G Loss: 0.21681594848632812\n",
      "Epoch 42 - D Loss: 0.4663178324699402 G Loss: 0.23477131128311157\n",
      "Epoch 42 - D Loss: 0.44231563806533813 G Loss: 0.20548014342784882\n",
      "Epoch 42 - D Loss: 0.503922700881958 G Loss: 0.1690705120563507\n",
      "Epoch 42 - D Loss: 0.19537881016731262 G Loss: 0.23902654647827148\n",
      "Epoch 42 - D Loss: 0.42091602087020874 G Loss: 0.22699978947639465\n",
      "Epoch 42 - D Loss: 0.5361496210098267 G Loss: 0.1956283450126648\n",
      "Epoch 42 - D Loss: 0.42957186698913574 G Loss: 0.13169710338115692\n",
      "Epoch 42 - D Loss: 0.4389750361442566 G Loss: 0.130741149187088\n",
      "Epoch 42 - D Loss: 0.39426612854003906 G Loss: 0.3092939257621765\n",
      "Epoch 42 - D Loss: 0.42285627126693726 G Loss: 0.15374010801315308\n",
      "Epoch 42 - D Loss: 0.36581212282180786 G Loss: 0.2089725136756897\n",
      "Epoch 42 - D Loss: 0.4405989348888397 G Loss: 0.1965220868587494\n",
      "Epoch 42 - D Loss: 0.4341771602630615 G Loss: 0.22695417702198029\n",
      "Epoch 42 - D Loss: 0.4068605899810791 G Loss: 0.18094024062156677\n",
      "Epoch 42 - D Loss: 0.36672210693359375 G Loss: 0.31138163805007935\n",
      "Epoch 42 - D Loss: 0.4747951626777649 G Loss: 0.20568692684173584\n",
      "Epoch 42 - D Loss: 0.32375192642211914 G Loss: 0.2592320442199707\n",
      "Epoch 42 - D Loss: 0.4261249303817749 G Loss: 0.11854677647352219\n",
      "Epoch 43 - D Loss: 0.36343300342559814 G Loss: 0.2432735562324524\n",
      "Epoch 43 - D Loss: 0.28075897693634033 G Loss: 0.0796433836221695\n",
      "Epoch 43 - D Loss: 0.5009733438491821 G Loss: 0.18897755444049835\n",
      "Epoch 43 - D Loss: 0.4258745312690735 G Loss: 0.26582807302474976\n",
      "Epoch 43 - D Loss: 0.43086761236190796 G Loss: 0.1413533091545105\n",
      "Epoch 43 - D Loss: 0.5776962041854858 G Loss: 0.25294408202171326\n",
      "Epoch 43 - D Loss: 0.320972740650177 G Loss: 0.09047553688287735\n",
      "Epoch 43 - D Loss: 0.19680237770080566 G Loss: 0.25168150663375854\n",
      "Epoch 43 - D Loss: 0.317429780960083 G Loss: 0.16236348450183868\n",
      "Epoch 43 - D Loss: 0.384878933429718 G Loss: 0.19429874420166016\n",
      "Epoch 43 - D Loss: 0.5163577198982239 G Loss: 0.3312727212905884\n",
      "Epoch 43 - D Loss: 0.25016823410987854 G Loss: 0.27685314416885376\n",
      "Epoch 43 - D Loss: 0.33888906240463257 G Loss: 0.1952989101409912\n",
      "Epoch 43 - D Loss: 0.3191584348678589 G Loss: 0.21193276345729828\n",
      "Epoch 43 - D Loss: 0.463569313287735 G Loss: 0.312856525182724\n",
      "Epoch 43 - D Loss: 0.24911779165267944 G Loss: 0.17119920253753662\n",
      "Epoch 43 - D Loss: 0.42249903082847595 G Loss: 0.11447885632514954\n",
      "Epoch 43 - D Loss: 0.3641265034675598 G Loss: 0.19753500819206238\n",
      "Epoch 43 - D Loss: 0.24658484756946564 G Loss: 0.27651098370552063\n",
      "Epoch 43 - D Loss: 0.2819657623767853 G Loss: 0.17825481295585632\n",
      "Epoch 43 - D Loss: 0.3863649368286133 G Loss: 0.12001048773527145\n",
      "Epoch 43 - D Loss: 0.4836270809173584 G Loss: 0.1692761778831482\n",
      "Epoch 43 - D Loss: 0.5108974575996399 G Loss: 0.1257169246673584\n",
      "Epoch 43 - D Loss: 0.4350064992904663 G Loss: 0.22022543847560883\n",
      "Epoch 43 - D Loss: 0.46497029066085815 G Loss: 0.18966835737228394\n",
      "Epoch 44 - D Loss: 0.4822099804878235 G Loss: 0.16522368788719177\n",
      "Epoch 44 - D Loss: 0.49137356877326965 G Loss: 0.22355909645557404\n",
      "Epoch 44 - D Loss: 0.3304245173931122 G Loss: 0.19149944186210632\n",
      "Epoch 44 - D Loss: 0.5507385730743408 G Loss: 0.14968228340148926\n",
      "Epoch 44 - D Loss: 0.36653393507003784 G Loss: 0.13448111712932587\n",
      "Epoch 44 - D Loss: 0.3258218765258789 G Loss: 0.2150588482618332\n",
      "Epoch 44 - D Loss: 0.3613364100456238 G Loss: 0.16332292556762695\n",
      "Epoch 44 - D Loss: 0.446139395236969 G Loss: 0.2154834270477295\n",
      "Epoch 44 - D Loss: 0.4121081233024597 G Loss: 0.1806529015302658\n",
      "Epoch 44 - D Loss: 0.5085617303848267 G Loss: 0.15014508366584778\n",
      "Epoch 44 - D Loss: 0.17565083503723145 G Loss: 0.1675119251012802\n",
      "Epoch 44 - D Loss: 0.5502004623413086 G Loss: 0.14798098802566528\n",
      "Epoch 44 - D Loss: 0.4023773670196533 G Loss: 0.15400055050849915\n",
      "Epoch 44 - D Loss: 0.5113214254379272 G Loss: 0.11336284875869751\n",
      "Epoch 44 - D Loss: 0.3325382471084595 G Loss: 0.15174974501132965\n",
      "Epoch 44 - D Loss: 0.32613956928253174 G Loss: 0.135208398103714\n",
      "Epoch 44 - D Loss: 0.4639889597892761 G Loss: 0.19898906350135803\n",
      "Epoch 44 - D Loss: 0.5024691820144653 G Loss: 0.19873735308647156\n",
      "Epoch 44 - D Loss: 0.3687322735786438 G Loss: 0.2612496614456177\n",
      "Epoch 44 - D Loss: 0.22170569002628326 G Loss: 0.16182458400726318\n",
      "Epoch 44 - D Loss: 0.5059239864349365 G Loss: 0.21926453709602356\n",
      "Epoch 44 - D Loss: 0.38307857513427734 G Loss: 0.18326856195926666\n",
      "Epoch 44 - D Loss: 0.4901946485042572 G Loss: 0.11719895154237747\n",
      "Epoch 44 - D Loss: 0.29591265320777893 G Loss: 0.2052401304244995\n",
      "Epoch 44 - D Loss: 0.31493449211120605 G Loss: 0.14638900756835938\n",
      "Epoch 45 - D Loss: 0.40010666847229004 G Loss: 0.26791155338287354\n",
      "Epoch 45 - D Loss: 0.34725987911224365 G Loss: 0.20024040341377258\n",
      "Epoch 45 - D Loss: 0.43581530451774597 G Loss: 0.18680672347545624\n",
      "Epoch 45 - D Loss: 0.38153132796287537 G Loss: 0.18069294095039368\n",
      "Epoch 45 - D Loss: 0.37816643714904785 G Loss: 0.18419447541236877\n",
      "Epoch 45 - D Loss: 0.35071486234664917 G Loss: 0.15457157790660858\n",
      "Epoch 45 - D Loss: 0.5383942723274231 G Loss: 0.17823821306228638\n",
      "Epoch 45 - D Loss: 0.4001944065093994 G Loss: 0.14378255605697632\n",
      "Epoch 45 - D Loss: 0.5030168294906616 G Loss: 0.22986584901809692\n",
      "Epoch 45 - D Loss: 0.4162454903125763 G Loss: 0.2512062191963196\n",
      "Epoch 45 - D Loss: 0.2995370030403137 G Loss: 0.12492111325263977\n",
      "Epoch 45 - D Loss: 0.4615509510040283 G Loss: 0.20703068375587463\n",
      "Epoch 45 - D Loss: 0.43605902791023254 G Loss: 0.2901720702648163\n",
      "Epoch 45 - D Loss: 0.3187646269798279 G Loss: 0.1937045454978943\n",
      "Epoch 45 - D Loss: 0.4940149486064911 G Loss: 0.2486184984445572\n",
      "Epoch 45 - D Loss: 0.2888406217098236 G Loss: 0.13393381237983704\n",
      "Epoch 45 - D Loss: 0.4440273642539978 G Loss: 0.2637583613395691\n",
      "Epoch 45 - D Loss: 0.3082635998725891 G Loss: 0.249224454164505\n",
      "Epoch 45 - D Loss: 0.1497504711151123 G Loss: 0.29326939582824707\n",
      "Epoch 45 - D Loss: 0.5233497023582458 G Loss: 0.16006602346897125\n",
      "Epoch 45 - D Loss: 0.2564811706542969 G Loss: 0.25985661149024963\n",
      "Epoch 45 - D Loss: 0.3733888864517212 G Loss: 0.18605361878871918\n",
      "Epoch 45 - D Loss: 0.3837563395500183 G Loss: 0.17719565331935883\n",
      "Epoch 45 - D Loss: 0.4081793427467346 G Loss: 0.25675755739212036\n",
      "Epoch 45 - D Loss: 0.46169474720954895 G Loss: 0.18155163526535034\n",
      "Epoch 46 - D Loss: 0.2677343487739563 G Loss: 0.21607492864131927\n",
      "Epoch 46 - D Loss: 0.6003613471984863 G Loss: 0.155404195189476\n",
      "Epoch 46 - D Loss: 0.40364211797714233 G Loss: 0.23391053080558777\n",
      "Epoch 46 - D Loss: 0.3435189723968506 G Loss: 0.13364019989967346\n",
      "Epoch 46 - D Loss: 0.3877599239349365 G Loss: 0.16471077501773834\n",
      "Epoch 46 - D Loss: 0.4245891869068146 G Loss: 0.19821180403232574\n",
      "Epoch 46 - D Loss: 0.5471913814544678 G Loss: 0.22588607668876648\n",
      "Epoch 46 - D Loss: 0.4179931879043579 G Loss: 0.2612566351890564\n",
      "Epoch 46 - D Loss: 0.4914557635784149 G Loss: 0.1657322347164154\n",
      "Epoch 46 - D Loss: 0.37728697061538696 G Loss: 0.2394219934940338\n",
      "Epoch 46 - D Loss: 0.4597836434841156 G Loss: 0.2356215864419937\n",
      "Epoch 46 - D Loss: 0.33869558572769165 G Loss: 0.2050212025642395\n",
      "Epoch 46 - D Loss: 0.31531137228012085 G Loss: 0.180985689163208\n",
      "Epoch 46 - D Loss: 0.4063621163368225 G Loss: 0.15956543385982513\n",
      "Epoch 46 - D Loss: 0.3218861222267151 G Loss: 0.17242085933685303\n",
      "Epoch 46 - D Loss: 0.15819349884986877 G Loss: 0.07863616198301315\n",
      "Epoch 46 - D Loss: 0.42749935388565063 G Loss: 0.14848852157592773\n",
      "Epoch 46 - D Loss: 0.4557816684246063 G Loss: 0.3070129156112671\n",
      "Epoch 46 - D Loss: 0.4683370590209961 G Loss: 0.2700657248497009\n",
      "Epoch 46 - D Loss: 0.3738846182823181 G Loss: 0.2907230854034424\n",
      "Epoch 46 - D Loss: 0.38597720861434937 G Loss: 0.2655755579471588\n",
      "Epoch 46 - D Loss: 0.41216206550598145 G Loss: 0.3203064203262329\n",
      "Epoch 46 - D Loss: 0.5690693855285645 G Loss: 0.08947423100471497\n",
      "Epoch 46 - D Loss: 0.4618817865848541 G Loss: 0.1791675090789795\n",
      "Epoch 46 - D Loss: 0.387489914894104 G Loss: 0.18575792014598846\n",
      "Epoch 47 - D Loss: 0.4064978361129761 G Loss: 0.18059806525707245\n",
      "Epoch 47 - D Loss: 0.5486573576927185 G Loss: 0.21232640743255615\n",
      "Epoch 47 - D Loss: 0.5287548303604126 G Loss: 0.1861996352672577\n",
      "Epoch 47 - D Loss: 0.43419456481933594 G Loss: 0.33029666543006897\n",
      "Epoch 47 - D Loss: 0.41440385580062866 G Loss: 0.16843855381011963\n",
      "Epoch 47 - D Loss: 0.3472854197025299 G Loss: 0.22603140771389008\n",
      "Epoch 47 - D Loss: 0.45063188672065735 G Loss: 0.1462390273809433\n",
      "Epoch 47 - D Loss: 0.3066321015357971 G Loss: 0.1237972229719162\n",
      "Epoch 47 - D Loss: 0.326465904712677 G Loss: 0.3087829053401947\n",
      "Epoch 47 - D Loss: 0.5074834823608398 G Loss: 0.14224563539028168\n",
      "Epoch 47 - D Loss: 0.3973371386528015 G Loss: 0.3150210380554199\n",
      "Epoch 47 - D Loss: 0.2521513104438782 G Loss: 0.1934814751148224\n",
      "Epoch 47 - D Loss: 0.32109272480010986 G Loss: 0.17500841617584229\n",
      "Epoch 47 - D Loss: 0.554309070110321 G Loss: 0.1540437489748001\n",
      "Epoch 47 - D Loss: 0.32375943660736084 G Loss: 0.06499423831701279\n",
      "Epoch 47 - D Loss: 0.34914278984069824 G Loss: 0.16190385818481445\n",
      "Epoch 47 - D Loss: 0.36074063181877136 G Loss: 0.18825551867485046\n",
      "Epoch 47 - D Loss: 0.272166907787323 G Loss: 0.22088205814361572\n",
      "Epoch 47 - D Loss: 0.43012791872024536 G Loss: 0.144558385014534\n",
      "Epoch 47 - D Loss: 0.5894703269004822 G Loss: 0.31578850746154785\n",
      "Epoch 47 - D Loss: 0.33354106545448303 G Loss: 0.25592008233070374\n",
      "Epoch 47 - D Loss: 0.5554990768432617 G Loss: 0.13463900983333588\n",
      "Epoch 47 - D Loss: 0.3083288371562958 G Loss: 0.2400939166545868\n",
      "Epoch 47 - D Loss: 0.3647382855415344 G Loss: 0.2062646895647049\n",
      "Epoch 47 - D Loss: 0.3337993621826172 G Loss: 0.17403465509414673\n",
      "Epoch 48 - D Loss: 0.32293280959129333 G Loss: 0.23237155377864838\n",
      "Epoch 48 - D Loss: 0.2992844581604004 G Loss: 0.1850401610136032\n",
      "Epoch 48 - D Loss: 0.3443681597709656 G Loss: 0.2572041153907776\n",
      "Epoch 48 - D Loss: 0.3511599898338318 G Loss: 0.22082380950450897\n",
      "Epoch 48 - D Loss: 0.6254391074180603 G Loss: 0.22238793969154358\n",
      "Epoch 48 - D Loss: 0.596179723739624 G Loss: 0.24301208555698395\n",
      "Epoch 48 - D Loss: 0.3444080948829651 G Loss: 0.2051318734884262\n",
      "Epoch 48 - D Loss: 0.3801060914993286 G Loss: 0.22244322299957275\n",
      "Epoch 48 - D Loss: 0.4936121106147766 G Loss: 0.09801080077886581\n",
      "Epoch 48 - D Loss: 0.3689047694206238 G Loss: 0.18428155779838562\n",
      "Epoch 48 - D Loss: 0.4365708827972412 G Loss: 0.10853968560695648\n",
      "Epoch 48 - D Loss: 0.2749289870262146 G Loss: 0.19564321637153625\n",
      "Epoch 48 - D Loss: 0.2354203164577484 G Loss: 0.13743793964385986\n",
      "Epoch 48 - D Loss: 0.35929495096206665 G Loss: 0.20825925469398499\n",
      "Epoch 48 - D Loss: 0.4690312445163727 G Loss: 0.3045206665992737\n",
      "Epoch 48 - D Loss: 0.21376794576644897 G Loss: 0.13943560421466827\n",
      "Epoch 48 - D Loss: 0.2796150743961334 G Loss: 0.2756422460079193\n",
      "Epoch 48 - D Loss: 0.4437583088874817 G Loss: 0.21176683902740479\n",
      "Epoch 48 - D Loss: 0.5370581150054932 G Loss: 0.16216272115707397\n",
      "Epoch 48 - D Loss: 0.46463271975517273 G Loss: 0.14806276559829712\n",
      "Epoch 48 - D Loss: 0.34429931640625 G Loss: 0.14819452166557312\n",
      "Epoch 48 - D Loss: 0.5147771239280701 G Loss: 0.15184547007083893\n",
      "Epoch 48 - D Loss: 0.3850668668746948 G Loss: 0.199532151222229\n",
      "Epoch 48 - D Loss: 0.23323491215705872 G Loss: 0.20140227675437927\n",
      "Epoch 48 - D Loss: 0.4868448078632355 G Loss: 0.20934388041496277\n",
      "Epoch 49 - D Loss: 0.4859656095504761 G Loss: 0.16115352511405945\n",
      "Epoch 49 - D Loss: 0.4211602509021759 G Loss: 0.22088640928268433\n",
      "Epoch 49 - D Loss: 0.3677242398262024 G Loss: 0.1594829261302948\n",
      "Epoch 49 - D Loss: 0.33811578154563904 G Loss: 0.2466714084148407\n",
      "Epoch 49 - D Loss: 0.5790913701057434 G Loss: 0.2978430986404419\n",
      "Epoch 49 - D Loss: 0.4008098244667053 G Loss: 0.21883311867713928\n",
      "Epoch 49 - D Loss: 0.25821855664253235 G Loss: 0.15902312099933624\n",
      "Epoch 49 - D Loss: 0.43399035930633545 G Loss: 0.24392913281917572\n",
      "Epoch 49 - D Loss: 0.43043607473373413 G Loss: 0.1065368577837944\n",
      "Epoch 49 - D Loss: 0.27880769968032837 G Loss: 0.237985759973526\n",
      "Epoch 49 - D Loss: 0.416612446308136 G Loss: 0.2033703327178955\n",
      "Epoch 49 - D Loss: 0.38358885049819946 G Loss: 0.21651697158813477\n",
      "Epoch 49 - D Loss: 0.4974605143070221 G Loss: 0.15645194053649902\n",
      "Epoch 49 - D Loss: 0.41701221466064453 G Loss: 0.15077289938926697\n",
      "Epoch 49 - D Loss: 0.263984739780426 G Loss: 0.11874061822891235\n",
      "Epoch 49 - D Loss: 0.36465030908584595 G Loss: 0.23735707998275757\n",
      "Epoch 49 - D Loss: 0.478645920753479 G Loss: 0.22092920541763306\n",
      "Epoch 49 - D Loss: 0.5193992257118225 G Loss: 0.22106841206550598\n",
      "Epoch 49 - D Loss: 0.5062061548233032 G Loss: 0.2851470410823822\n",
      "Epoch 49 - D Loss: 0.4244515895843506 G Loss: 0.28435230255126953\n",
      "Epoch 49 - D Loss: 0.456650972366333 G Loss: 0.15405023097991943\n",
      "Epoch 49 - D Loss: 0.32567423582077026 G Loss: 0.24140216410160065\n",
      "Epoch 49 - D Loss: 0.45837387442588806 G Loss: 0.21910855174064636\n",
      "Epoch 49 - D Loss: 0.46295514702796936 G Loss: 0.16743972897529602\n",
      "Epoch 49 - D Loss: 0.34937113523483276 G Loss: 0.20744062960147858\n",
      "Epoch 50 - D Loss: 0.3606797456741333 G Loss: 0.13378840684890747\n",
      "Epoch 50 - D Loss: 0.3283279538154602 G Loss: 0.10852105915546417\n",
      "Epoch 50 - D Loss: 0.33809810876846313 G Loss: 0.23960232734680176\n",
      "Epoch 50 - D Loss: 0.49705690145492554 G Loss: 0.2286587953567505\n",
      "Epoch 50 - D Loss: 0.5632238388061523 G Loss: 0.13360178470611572\n",
      "Epoch 50 - D Loss: 0.3609869182109833 G Loss: 0.10974615812301636\n",
      "Epoch 50 - D Loss: 0.34455710649490356 G Loss: 0.20733940601348877\n",
      "Epoch 50 - D Loss: 0.3785024881362915 G Loss: 0.15193718671798706\n",
      "Epoch 50 - D Loss: 0.44448772072792053 G Loss: 0.18148329854011536\n",
      "Epoch 50 - D Loss: 0.44721394777297974 G Loss: 0.08881834894418716\n",
      "Epoch 50 - D Loss: 0.46256059408187866 G Loss: 0.09589220583438873\n",
      "Epoch 50 - D Loss: 0.5820385217666626 G Loss: 0.3077149987220764\n",
      "Epoch 50 - D Loss: 0.3950763940811157 G Loss: 0.23251402378082275\n",
      "Epoch 50 - D Loss: 0.47505468130111694 G Loss: 0.14313043653964996\n",
      "Epoch 50 - D Loss: 0.3133658766746521 G Loss: 0.2424190640449524\n",
      "Epoch 50 - D Loss: 0.4322150647640228 G Loss: 0.141383096575737\n",
      "Epoch 50 - D Loss: 0.48856160044670105 G Loss: 0.1674690544605255\n",
      "Epoch 50 - D Loss: 0.3960838317871094 G Loss: 0.1280198097229004\n",
      "Epoch 50 - D Loss: 0.5148669481277466 G Loss: 0.11811674386262894\n",
      "Epoch 50 - D Loss: 0.364632248878479 G Loss: 0.13745398819446564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m      3\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m truth \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m900\u001b[39m,\u001b[38;5;241m1000\u001b[39m,(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      9\u001b[0m loss_real \u001b[38;5;241m=\u001b[39m loss_fn(prob,truth)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss_real\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m x_gen \u001b[38;5;241m=\u001b[39m generator(z)\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nucleaise/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# To see tensorboard results runt he following:\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nucleaise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
