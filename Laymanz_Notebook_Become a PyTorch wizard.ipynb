{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":12722,"status":"ok","timestamp":1714698084624,"user":{"displayName":"Ambrose Ling","userId":"02772582281435222926"},"user_tz":240},"id":"ZX1CyCYO4A7p"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Laymanz Notebooks: Training your first deep learning model\n","Author: Ali Ahmad & Amrbose Ling\n","\n","**Our goal is to get rid of abstractions and black boxes when learning about ML**\n","\n","**What is this notebook about?**\n","\n","In this notebook, we will go over some of the fundamental ideas behind deep learning, neural networks, model training, inference, backpropagation, loss functions, optimizers.\n","\n","**What do I need to set up my environment?**\n","\n","All of our notebooks will only use numpy, pytorch, matplotlib for visualizations. If you are very eager to learn about what PyTorch is and how it works, check out this super detailed notebook on PyTorch! If you are running this on colab you can just import the packages, if you are running this notebook locally , just remember to `pip install numpy torch matplotlib`. Check [here](https://pytorch.org/get-started/locally/) to see which torch version depending on the hardware you have.\n","\n","**How is this notebook structured?**\n","\n","Each notebook will have\n","\n","[**How to use matplotlib for plotting**](https://colab.research.google.com/github/amanchadha/aman-ai/blob/master/matplotlib.ipynb#scrollTo=1-AcMM6NSmP-)\n","\n","\n","## Breakdown\n","*   What is deep learning?\n","*   What is a neuron? What is an activation?\n","*   What is a neural network?\n","*   What is a multi-layer perceptron?\n","*   What is a computation graph?\n","*   What is autograd?\n","*   What is a forward pass?\n","*   What is a loss function?\n","*   What is an optimizer?\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader,Dataset"]},{"cell_type":"markdown","metadata":{},"source":["# What is deep learning?\n","\n","You can find many definitions online, but here is my definition:\n","Deep learning involves extracting meaningful insights from data using deep neural networks.\n","Deep learning is a branch of Machine Learning that specializes in the use of neural networks to make predictions.\n","\n","\n","# What can deep learning be used for?\n","* Classification\n","    - Binary classification\n","    - Multiclass classification\n","    - Multilabel classficiation\n","* Regression\n","    - Linear Regression\n","    - Logistic Regression\n","    - Polynomial Regression\n","\n","Ambrose's interpretation:\n","Deep learning is about feeding a neural network a bunch of data, the neural network produces some \n","output, we see how wrong they were compared to what we want, we tell it \"hey you were wrong you gotta change how you think so you get it right next time!\" then we change it, we repeat this until we iterate through all of our data.\n","\n","The usual receipe for a deep learning task:\n","Step 1: Find a bunch of data for the thing you want to train your model on\n","Step 2: Preprocess the data to turn it into the right form\n","Step 3: Train the model by feeding it the data\n","Step 4: Evaluate it, find ways to make it better or train it again "]},{"cell_type":"markdown","metadata":{},"source":["# What is a neuron?\n","A neuron are nerve cells in our brains, responsible for transmitting electrical signals from one part of the brain to another. \n","\n","## Properties of neurons:\n","- They receive their signals via their dendrites\n","- They have snynapses that module the electrical signals it receives (between dendrites and axons)\n","- They fire an output signal only when the total strength of the input signal exceed a certain threshold"]},{"cell_type":"markdown","metadata":{},"source":["# What is a perceptron?\n","\n","A perceptron is a mathematical model of a biological neuron. Hence we use mathematical operations to model the properties of a neuron.\n","\n","## Properties of a perecptron?\n","- electrical signals are represented by numerical values (some vector)\n","- modulation is modelled by multiplying some weight value to the input signal / values\n","- we model the total strength of a signal by performing a weighted sum of the inputs \n","- we apply an activation function or a step function to model the firing of the signal upon some threshold\n","- it is believed that neurons remain inactive until the net input to the cell body reaches a certain threshold"]},{"cell_type":"markdown","metadata":{},"source":["<center><img src=\"https://miro.medium.com/v2/resize:fit:2902/format:webp/1*hkYlTODpjJgo32DoCOWN5w.png\" height=230 width=500/></center>\n","<center> On the left you have the biological neuron, on the right you have the artificial neuron </center>"]},{"cell_type":"markdown","metadata":{},"source":["### How do we represent a perceptron mathematically?\n","\n","$$\n","y = \\sigma ( \\Sigma w_i x_i + b )\n","$$\n","\n","where : \n","* y: represents the output from the neuron\n","* w represents the weight associated with this neuron\n","* x: represents the input to the neuron\n","* b: represets the bias added to each \n","\n","Ambrose's intuition:\n","Think of each neuron having its own behaviour or its own state, it behaves differently from other neurosn which is why each\n","has a different weight and bias associated with it.\n"]},{"cell_type":"markdown","metadata":{},"source":["### How do we represent all these quantities mathematically?\n","Think of different ways you may want to represent inputs, and I can list some examples for some of the most common machine learning applications, all these are different modalities. Since we mentioned that the key to \n","  \n","\n","Lets try to develop an intuition for how you would quantitatively represent data?\n","\n","1. Usecase: Predicting house prices \n","Scenario: Lets say i want to predict the price of houses in Toronto given some information of a house(expensive af)\n","What does data look like:  For this scenario, I want to probably find some way to represent that information of a house **quantitatively**\n","What does input look like: Some collection of numbers that represent some characteristics of the home\n","What does output look like: A number (float/double) representating the price of the home\n","A house can be represented by:\n","    - int: how many bedrooms are in this house\n","    - int: how many square feet is this house\n","    - int: how many bathrooms it has\n","    - String: where is this house (the location)\n","    - String: the population of the city it is in\n","\n","2.  Usecase: Predicting postiive and negative sentiment from text\n","Scenario: Lets say i want to predict if a tweet contains harmful or positive intent right, you know those goddamn politicans\n","What does data look like? In this scenario, the data would probably be the tweets themselves, which is a series of strings.\n","\n","\n","3. Usecase: Predicting the price of Bitcoin \n","Scenario: Lets say i want to predict if the price of bitcoin. \n","What does input look like: A collection of numbers representing past bitcon pries\n","The price of bitcoin can be represented by:\n","    - list of ints: \n","\n","\n","4. Usecase: "]},{"cell_type":"markdown","metadata":{},"source":["### Model Width VS Model Depth\n","\n","**Larger width** (vertically, more neurons): usually means that the neural network has the capacity to remember more feature or encode more features in the weight connecitons or weight matrix. \n","* With more neurons in each layer, you can capture more delicate details with more neurons. \n","* Think of the more neurons you have in each layer, you have more processing units and internal states corresponding to each input unit. \n","* Think of if you have 1 x 10 input, 15 neurons VS 1 x 10 input, 100 neurons, you have much more overall weight connections to the input, so you can capture each intricate value in the input more precisely with more weights.\n","* Think of F1 cars right when they get to the pit stop, if you have only 5 people changing the tires, wiping the windows, pumping gas VS when you have 20 people , you would be be able to be more precise about what you do to the car, 5 people would probably capture less of the tasks they need to do on the car. People (neurons), the car (input).\n","\n","**Larger depth** (more layers), usually means that the neural network can remember or encode more complex,high dimensional features from the training data.\n","\n","* The way i think could help also understand is that think of width as having more functions to model your data, but more functions do not necessarily mean that the functions are more complex. \n","* Increasing the width is similar to going from 1 function: y = mx+b to 20  functions, 20 y = wx+b's. You may be able to capture specific changes in the input data better with more lines. But they are still linear functions and it would always be linear\n","* But with greater depth, you can chain non-linearities together as you apply activation functions and additional weights. So now think of 20 y = (wa(wa(wx+b)+b)+b), this entire function gets more and more complex and you introduce more non-linearity at each layer.\n","* So going from 20 y=wx+b's to 20 y = (wa(wa(wx+b)+b)+b) is what enables you to capture a much more complicated function that fits your training data."]},{"cell_type":"markdown","metadata":{},"source":["### Rank\n","Rank refers to the number of dimensions a tensor has\n","\n","- Rank 0:  referred as a **scalar**, 1 single value\n","- Rank 1:  referred as a **vector**, 1 list of values\n","- Rank 2:  referred as a **matrix**, 1 2d array of values\n","- Rank 3:  referred as a **3D tensor**, you can think of as a cube of numbers or a stack of a number of 2D matrices\n","\n","\n","### Shape \n","Shape is an array of numbers representing the length of each dimension. \n","You can understand this as the number of elements along each dimension\n","\n","```python\n","x = np.array([[[1],[2]],[[3],[4]]])\n","```\n","\n","Ambrose's intuition:\n","The way I like thinking about shapes is in terms box boxes.\n","Lets say the shape of tensor `x` is `(2,4,5)`. This means that there are 2 big boxes. Within those 2 big boxes, each of the 2 big boxes has 4 boxes in it\n","Then for each of the 4 boxes you then have 5 boxes in each one.\n","\n","One very typical example tensor is with shape `(B,C,H,W)`, for instance `(8,3,100,100)`\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Hn3xTzct-b84"},"outputs":[],"source":["\n","# 1D: Array\n","x = np.array([0.12,0.23,2.34])\n","\n","# 2D: Matrix\n","x = np.array([[0.1232,0.3445,0.345532],[0.1232,0.3445,0.345532]])\n","\n","# Why do we use matrices?\n","# There are many highly efficient linear algebra libraries (NumPY, PyTorch) that are optimized to perfrom matrix multiplications extremely fast\n","# Using matrices allows us to perform operations in PARALLEL much faster to doing sequential operations\n","\n","\n","# Matrix\n","x = np.array([[0,1,2,3,4],[2,3,5,67,5]])\n","\n","# Tensor\n","x = np.array([[0,1,2,3,4],[2,3,5,67,5],[10,20,40,20,2]])\n","\n","# A tensor is nothing but a bigger matrix, it is an array that carries numerical data or think of a tensor as a multi-dimensional array. \n","# Usually matrices are  m x n, tensors can be m x n x c x p ...\n"]},{"cell_type":"markdown","metadata":{},"source":["### Why does neural network architecture matter?\n","When we talk about the neural network architecture, we are referring to the specific arrangement of neurons, the connections of neurons,layers. The specific architecture dictates the capabilities of the model, the specific data it is best suited for, the features it would excel at capturing, the specific tasks it is suited for.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### What is PyTorch?\n","\n","PyTorch is a neural network library that lets you build and trian neural networks with their very comprehensive collection of APIs.\n","\n","1. Tensor Operations: Basic operations for creating, manipulating, and transforming tensors.\n","2. Mathematical Operations: Functions for performing arithmetic, linear algebra, and complex mathematical computations.\n","3. Neural Network Operations: Layers, activation functions, loss functions, and other building blocks for constructing neural networks.\n","4. Data Manipulation: Functions for data loading, preprocessing, and augmentation.\n","5. Optimization: Algorithms for updating model parameters, such as SGD, Adam, etc.\n","6. Autograd: Operators supporting automatic differentiation.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["###  How do you train a neural network\n","\n","* Step 1: Find a bunch of data for the thing you want to train your model on\n","* Step 2: Preprocess the data to turn it into the right form\n","* Step 3: Train the model by feeding it the data\n","* Step 4: Evaluate it \n","\n","In this notebook we will put more emphasis on step 3\n","The most common training loop you will ever see:\n","```python \n","for batch in training_data:\n","    output = model(batch.x) #S\n","    loss = loss_fn(batch.y,output)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","```"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing your data\n","* In PyTorch, a `Dataset` object is a class that allows you to access samples of your data. This class also allows you to perform certain data transformations to process the raw samples you grab from your dataset\n","* In case you dont know, a **sample** just means 1 pair of input and output. So for a image classification task, a sample may be (x: image,y: label), in text sentiment analysis, sample may be (x: string of text,y: float probability).\n","\n","* Lets say you have a dataset called `MyDataset`,it inherits the `Dataset` class. It represents a collection of data samples. \n","    ```python\n","    # \n","    class MyDataset(Dataset):\n","        ...\n","        def __getitem__(self,index):\n","            return\n","\n","    ```\n","    - What this class lets you do is that you can specify the logic for accessing 1 sample\n","    - So when you do `dataset[0]` calls `__getitem__()` and a sample of data will be returned\n","    - you can also apply transformations that would be applied to all the samples in your dataset when you fetch them\n","        - inside `__getitem__()`, you can apply transformations to the input and output by doing `x = self.transform(x)` and `y = self.target_transform(y)`.\n","        - **When will I do this?**, some common usecases\n","            - Transforms:\n","                - Images: cropping, resizing, normalization\n","                - Text: tokenization (turning words into numbers), padding and truncation\n","                - Audio: resampling, normalization, augmentation\n","            - Target Transform:\n","                - one hot encoding\n","                - encoding labels\n","                - normalization (coordinates)\n","                - format conversions\n","        - NOTE: the most common application of transformations is on image modalities, for text we have designated components that specifically do preprocessing (tokenizer) \n","* You can also define specific ways you want to grab samples from the dataset through the use of a sampler or `Sampler` class, which intuitively dictates how you sample data from your dataset.\n","    ```python\n","    class MySampler(Sampler):\n","        ...\n","        def __iter__(self):\n","            for i in range(self.N):\n","                yield i \n","    ```\n","    - it is used to specify the dataloading order, and specifies the indexes\n","    - more specifically it is an iterable dataset\n","* As for a `DataLoader`, this object is the intermediate process that prepares the data we take from the data into batches.\n","    ```python\n","    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","    ```\n","    - the DataLoader receives the `dataset` object as input and returns the dataloader \n","### How they all work together:\n","<center>\n","<a href=\"https://ibb.co/ZMgcwc4\"><img src=\"https://i.ibb.co/W2zBdB8/Screenshot-2024-05-30-023025.png\" alt=\"Screenshot-2024-05-30-023025\" border=\"0\"></a></center>\n","\n","* 1. the dataset class allows you to index into different samples in your dataset\n","* 2. your CPU has different workers that grab the samples responsible for constructing a batch\n","* 3. the CPU workers load the queried samples into a queue. \n","* 4. the sampler class also provides which indices do we need for \n","* 4. the dataloader performs the collating procedure, which draws samples in the queue and puts them into a batch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Here is an example of a collating function\n","\n","def collate_fn(samples):\n","    return batch\n","\n","# As input, the collate function takes the raw batch_size number of samples\n","# It then performs some processing or you can define some custom logic\n","# to turn these samples into tensors"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tiny_ling/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n","  warnings.warn(\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader,Dataset\n","import torch.nn.functional as F\n","from torchvision.transforms import v2\n","from torchvision.transforms import functional as TF \n","\n","# Load the MNIST dataset\n","dataset = MNIST('',train=True,download=True,transform=v2.Compose([v2.ToTensor()]),target_transform=v2.Compose([\n","                                lambda x:torch.LongTensor([x]), # or just torch.tensor\n","                                lambda x:F.one_hot(x,10)]))\n","\n","\n","# Create a dataloader for the dataset\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","# What is happening here?\n","# "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.1647, 0.4627, 0.8588, 0.6510, 0.4627,\n","          0.4627, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.4039, 0.9490, 0.9961, 0.9961, 0.9961, 0.9961,\n","          0.9961, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0706, 0.9098, 0.9961, 0.9961, 0.9961, 0.9961,\n","          0.9961, 0.9333, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.4078, 0.9569, 0.9961, 0.8784, 0.9961,\n","          0.9961, 0.9961, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.8118, 0.9961, 0.8235, 0.9961,\n","          0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.8078, 0.9961, 0.9961,\n","          0.9961, 0.9961, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.8196, 0.9961,\n","          0.9961, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.3569, 0.5373, 0.9922, 0.9961,\n","          0.9961, 0.9961, 0.4392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.1569, 0.8392, 0.9804, 0.9961, 0.9961, 0.9961,\n","          0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.3176, 0.9686, 0.9961, 0.9961, 0.9961, 0.9961,\n","          0.9961, 0.9961, 0.5725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.4314, 0.9647, 0.9961, 0.9961, 0.9961,\n","          0.9961, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 0.3490, 0.3490, 0.3647,\n","          0.9412, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n","          0.5020, 0.9961, 0.8588, 0.1216, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n","          0.9961, 0.9961, 0.8392, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5412,\n","          0.9961, 0.9961, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6941,\n","          0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.9412,\n","          0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6431, 0.9961,\n","          0.8431, 0.2471, 0.1412, 0.0000, 0.2000, 0.3490, 0.8078, 0.9961,\n","          0.9961, 0.5451, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.7725,\n","          0.9961, 0.9961, 0.8706, 0.7059, 0.9451, 0.9961, 0.9961, 0.9922,\n","          0.8353, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5490,\n","          0.4118, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9255,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0275, 0.4588, 0.4588, 0.6471, 0.9961, 0.9961, 0.9373, 0.1961,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000]]])\n","tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])\n"]}],"source":["# Lets see the dataset sample\n","# Each sample is a tuple (remeber it is a pair of x and y)\n","# THis is the x (image x)\n","print(dataset[10][0])\n","\n","# This is the label for the corresponding \n","print(dataset[10][1])\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 28, 28])\n","torch.Size([1, 10])\n"]}],"source":["# Lets see what is the shape of the sample\n","print(dataset[10][0].shape)\n","# The shape is 1,28,28, which tells us that this image has 1 channel, since it is a grayscale image, it also has a height and width of 28 pixels\n","print(dataset[10][1].shape)\n","# The shape tells us that it is a binary vector of size 10.\n","# THis is what we call a one-hot encoded vector, where a 1 is present to indicate the presence of that class."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Creating our very own neural network engine\n","\n","class Value():\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPy/ZaTDRYGonjxONkNzLGo","gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
